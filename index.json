
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"我是一个在阿姆斯特丹大学读书的计算机博士生，主要研究方向是自然语言处理和深度学习/人工智能。 英文版主页分享所有我学术相关的内容，中文版主要用于分享一些课外阅读相关的笔记、兴趣爱好、碎碎念等短博客。\n","date":1593520676,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1593520676,"objectID":"d19cb99606c79129dc5815cfb5145ebb","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"我是一个在阿姆斯特丹大学读书的计算机博士生，主要研究方向是自然语言处理和深度学习/人工智能。 英文版主页分享所有我学术相关的内容，中文版主要用于分享一些课外阅读相关的笔记、兴趣爱好、碎碎念等短博客。","tags":null,"title":"江少杰","type":"authors"},{"authors":null,"categories":null,"content":"I am a Senior Machine Learning Scientist at Huawei R\u0026amp;D Center Amsterdam. I work on training Huawei’s ChatGPT models, and am responsible for the Reinforcement Learning algorithm development and model training. More broadly, my work involves chatbots, information retrieval and conversational QA systems. In parallel, I am completing my PhD at the University of Amsterdam, under the supervision of Prof. Maarten de Rijke. In the past, I did two internships at Replika.ai and Amazon.com, respectively.\nDownload my CV in PDF\n","date":1589640007,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1589640007,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Senior Machine Learning Scientist at Huawei R\u0026D Center Amsterdam. I work on training Huawei’s ChatGPT models, and am responsible for the Reinforcement Learning algorithm development and model training.","tags":null,"title":"Shaojie Jiang 江少杰","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://shaojiejiang.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Shaojie Jiang","Svitlana Vakulenko","Maarten De Rijke"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"895e4f83fa6a3ea3215e981e618944b8","permalink":"https://shaojiejiang.github.io/publication/jiang-2023-weakly/","publishdate":"2023-01-29T23:01:57.847161Z","relpermalink":"/publication/jiang-2023-weakly/","section":"publication","summary":"","tags":null,"title":"Weakly Supervised Turn-level Engagingness Evaluator for Dialogues","type":"publication"},{"authors":["Shaojie Jiang","Ruqing Zhang","Svitlana Vakulenko","Maarten de Rijke"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"b0dbb9b1cefeff88eec7c2da4542b791","permalink":"https://shaojiejiang.github.io/publication/jiang-2022-simple/","publishdate":"2023-01-29T22:58:29.976238Z","relpermalink":"/publication/jiang-2022-simple/","section":"publication","summary":"","tags":null,"title":"A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration","type":"publication"},{"authors":["江少杰"],"categories":["情商课"],"content":"这是非常短的一本书。作者主要分享了有求于人时的7个突破口，分别是：\n投其所好 儆其所恶 选择的自由 被认可欲 非你不可 团队化 感谢 结合之前读的《The 7 Habits of Highly Effective People》发现，这些技巧与高效能人士的办事原则并不冲突，反而是相互补充的：原则是大方向的导航，技巧是走好每一步的保障。而且经得起时间检验的技巧也都是以原则为指导的；只顾眼下利益的技巧，更应该叫“套路”，而这用不了几次就会被人识破。\n以上7个技巧分别对应了哪项做事原则呢？\n投其所好、儆其所恶，是“双赢原则”的实践 提供选择、被认可欲、非你不可，是“要想被理解，先要理解对方”原则的实践 团队化，是理解、认可对方的一种方式，更是“协同原则”的直接实践 衷心感谢，也属于对别人付出的理解和认可 另外书中提到一点“要想掌握‘措辞菜谱’，输出是最便捷的途径”，正印证了本人学习的三要素 博客里关于“输出”对于学习的重要性。\n最后吐槽下该书的不足：作者同样1为广告文案员，因此第二章（最后一章）基本都是关于文案写作的语言张力的，与情商没什么关联，有点被欺骗的感觉。作者可能犯了文案员“标题党”的职业病了吧。\n相对于上一本书《好文案一句话就够了》的作者 ↩︎\n","date":1593520676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593520676,"objectID":"116a11027a095563c4658fe6c3e92ca7","permalink":"https://shaojiejiang.github.io/post/zh/eq-speaking/","publishdate":"2020-06-30T14:37:56+02:00","relpermalink":"/post/zh/eq-speaking/","section":"post","summary":"看书前没查黄历，接连两本都是文案员写的，而且这本书有大量内容都是在讲语言表达张力的，属于比较偏文案的内容。书中讲情商的并不多，有标题党的感觉（典型的文案员套路）。","tags":["读书笔记"],"title":"《所谓情商高，就是会说话》读书笔记","type":"post"},{"authors":["江少杰"],"categories":["写作技巧"],"content":"本人读《好文案一句话就够了》这本书（以下统称“该书”），主要是为了学习一些普适的写作技巧，毕竟虽然本人不需要写广告文案，但还是需要写推文、博客、文章等等，如果能练出更打动人心、有张力、吸引目光的写作风格，又何乐不为呢？\n该书总结的广告文案的三大基本原则，我认为很是精炼：\n让受众觉得信息与自己有关 语言表达要有张力 勾起读者的疑惑等阅读兴趣 随后该书还将三大原则展开，并通过实例分享了共77条文案写作技巧。现将这些技巧的核心内容，根据本人的喜好以及理解进行总结。\n让受众觉得信息与自己有关 首先应该明确受众，与其石沉人海、波澜不惊，不如缩小范围、把话说给懂的人听。为达到这样的目的，通常可以加上人称代词（如果原先没有），或者加上限定词对范围进行明确界定。\n语言表达要有张力 该书关于语言张力的技巧有很多，主要可以概括为：\n简洁凝炼、具体形象 感情真挚、说出心声、平易近人 纵览全局（过程概览、所需时间、结果展望等） 表达技巧（强调语气、利用格式节奏、制造冲突、引用名言） 勾起读者的疑惑等阅读兴趣 留白或者设问 创造吸睛新词 有故事性 总结 其实写到这里，通过对全书内容进行回顾，我发现该书并没有给我太多的新信息。我依然认为要写好各种形式的文字，只需要做到四点就可以了：\n明确自己想要表达什么 依据经验选择最有力的表达方式（这一点需要大量积累） 站在读者角度思考信息能否流畅传达 以上步骤反复迭代优化 该书的主要作用，也许就是像作者呼吁的那样“放在公司的桌上，当成字典来使用”吧。 顺带提一下本人对该书不太认同的一点，就是某些技巧过于强调“吸睛”了，以至于作者本人都多次提醒读者“作为文案员可以如此写，而作为消费者要保持头脑清醒，不要轻易被广告文案诱惑”。 这也从侧面说明了以上“技巧”中，最普适也最不像“技巧”1的一点，就是说话要发自内心，靠套路得来的人心只能是暂时的，而且还有被看穿的危险。\n个人理解所谓技巧，是指可以简单地掌握，并且应用起来见效很快。然而真挚的讲话，对于说惯了客套话的人是需要一定练习的，并且见效可能没那么快。用科维的观点来说，“感情真挚”更应该算是一项原则而非技巧。 ↩︎\n","date":1593447796,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593447796,"objectID":"8d5a1b3376e864941fb158689df91f73","permalink":"https://shaojiejiang.github.io/post/zh/copywriting/","publishdate":"2020-06-29T18:23:16+02:00","relpermalink":"/post/zh/copywriting/","section":"post","summary":"本人并不从事广告方案行业，读这本书也不是以锻炼广告文案力为目的，只是为了从中学习一些普适的写作技巧。现将本人认为该书中有价值的信息以及一些感想总结为本篇博客，供日后参考查阅。","tags":["读书笔记"],"title":"《好文案一句话就够了》读后感","type":"post"},{"authors":["Shaojie Jiang 江少杰"],"categories":["paper reading notes","Deep Learning","NLP"],"content":"In this paper1, transformer is trained to perform both translation and alignment tasks.\nApplication scenarios of word alignments in NMT Generating bilingual lexica from parallel corpora External dictionary assisted translation to improve translation of low frequency words Trust, explanation, error analysis Preserving style on webpages Model design The attention mechanism has long been motivated by word alignments in statistical machine translation, but ensure the alignment quality, additional supervision is needed.\nThere is a tendency that the attention probabilities from the penultimate layer of a normally trained transformer MT model corresponds to word alignments. Therefore, one attention head (clever!) in the penultimate layer is trained as the alignment head. The motivation of selecting only one attention head for alignment is to give the freedom to the model of choosing whether to rely more on the alignment or other attention heads.\nHow two train the alignment head There are two approaches existing in the literature:\nLabel alignments beforehand and train the attention weights through KL-divergence. Use the attentional vector to also predict either the target word or the properties such as POS tags of the target tokens. In this work, an unsupervised training approach is used to train the alignment head. An alignment model is first trained on translation, then the penultimate layer attention weights are averaged and used as weak alignment supervision for a translation (and alignment) model. The alignment model is trained in both directions.\nPrevious work reported performance gain by introducing alignment supervision. In this paper, however, alignment performances are good, but translation results are moderate.\nJointly Learning to Align and Translate with Transformer Models ↩︎\n","date":1589640007,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589640007,"objectID":"38341c3884e597a2d29c38e88a913fd2","permalink":"https://shaojiejiang.github.io/post/en/transformer-align-model/","publishdate":"2020-05-16T16:40:07+02:00","relpermalink":"/post/en/transformer-align-model/","section":"post","summary":"Jointly Learning to Align and Translate with Transformer Models","tags":[],"title":"Transformer Align Model","type":"post"},{"authors":["Shaojie Jiang 江少杰"],"categories":["paper reading notes","Deep Learning","NLP"],"content":"Built on top of Transformer-XL, Compressive Transformer1 condenses old memories (hidden states) and stores them in the compressed memory buffer, before completely discarding them. This model is suitable for long-range sequence learning but may cause too much computational burden for tasks that only have short sequences. Compressive Transformers can also be used as memory components in conjunction with other models.\nBackground In the beginning, the authors draw the connection between their work and human brains by mentioning that humans memorize things via lossy compression.\nWe aggressively select, filter, or integrate input stimuli based on factors of surprise, perceived danger, or repetition – amongst other signals.\nIt’s often, if not always, good to see such insights of how AI works are inspired by humans. It’s also good to see that they relate their work to previous works, i.e. RNNs, transformers and sparse attention.\nAn RNN compresses previous memories into a fixed size hidden vector, which is space-efficient, but also results in its temporal nature and hence difficult to parallelize. Transformers, on the other hand, store all the past memories uncompressed, which can be beneficial for achieving better performances such as precision, BLEU, perplexity, etc, but it costs more and more computation and memory space with the sequence length growing. Sparse attention can be used to reduce computation, while the spatial cost remains the same.\nModel design and training The proposed Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, trained to query both its short-term granular memory and longer-term coarse memory.\nIf trained using original task-relevant loss only, it requires backpropagating-through-time (BPTT) over long unrolls for very old memories. A better solution is to use local auxiliary losses by stopping gradients and reconstructing either the original memory vectors (lossless objective) or attention vectors (lossy objective; reportedly to work better). The second choice for the auxiliary loss, in other words, means that we don’t care whether the original memory can be reconstructed or not, as long as the attention vector can be reconstructed, given the same query (brilliant!).\nSome practical concerns The auxiliary loss is only used to train the compression module, as it harms the learning when the gradients flow back to the main network. This might also explain why I couldn’t reproduce ACT! Batch accumulation (4x bigger batch size) is used for better performance. It is observed in some works that bigger batch sizes lead to better generalization, but some other works found the opposite to be true (discussed in the papers and talks mentioned in my other post). Model optimization is very sensitive to gradient scales, so the gradient norms are clipped to 0.1 for stable results. This is typical for transformer variants. Convolution works best for memory compression. Further thoughs/questions: Compressive Transformer improves the modeling of rare words. But why? In the discussion section, the authors pointed out that future directions could include the investigation of adaptive compression rates by layer, the use of long-range shallow memory layers together with deep short-range memory, and even the use of RNNs as compressors. Compressive Transformers for Long-Range Sequence Modelling ↩︎\n","date":1589286584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589286584,"objectID":"e761cf5f7cf905ca6934d95b4f307ae4","permalink":"https://shaojiejiang.github.io/post/en/compressive-transformers/","publishdate":"2020-05-12T14:29:44+02:00","relpermalink":"/post/en/compressive-transformers/","section":"post","summary":"Built on top of Transformer-XL, Compressive Transformer1 condenses old memories (hidden states) and stores them in the compressed memory buffer, before completely discarding them. This model is suitable for long-range sequence learning but may cause too much computational burden for tasks that only have short sequences.","tags":[],"title":"Compressive Transformers","type":"post"},{"authors":["Shaojie Jiang 江少杰"],"categories":["paper reading notes","Deep Learning"],"content":"Here are some notes take while reading the NeurlIPS 2018 paper Visualizing the Loss Landscape of Neural Nets. This work helps explain why some models are easier to train/generalize than others. The above image is a good illustration: with a much smoother loss landscape, DenseNet with 121 layers is much easier to train than a ResNet-110 without skip connections, and generalizes better in the mean time.\nThe traditional way of visualizing loss functions of neural models in 2D contour plots is by choosing a center point $\\theta^*$ (normally the converged model parameters), two random direction vectors $\\delta$ and $\\eta$, then plot the function: $$f(\\alpha, \\beta) = L(\\theta^* + \\alpha \\delta + \\beta \\eta)$$ Batch norm parameters are unchanged.\nThe above method fails to capture the intrinsic geometry of loss surfaces, and cannot be used to compare the geometry of two different minimizers or two different networks. This is because of the scale invariance in network weights (this statement only applies to rectified networks as per the paper). To tackle this, the authors normalize each filter in a direction vector $d$ ($\\delta$ or $\\eta$) to have the same norm of the corresponding filter in $\\theta$: $$d_{i, j} \\leftarrow \\frac{d_{i, j}}{||d_{i, j}||} ||\\theta_{i, j} ||.$$ $i$ is the layer number and $j$ the filter number. With the proposed filter-wise normalized direction vectors, the authors found that the sharpness of local minima correlates well with generalization error, even better than layer-wise normalization (for direction vectors).\nWhy flat minima: In a recent talk1, Tom Goldstein (the last author) pointed out that flat minima correspond to large margin classifiers, which is more tolerant to domain shifts of data, thus having better generalization ability.\nKnown influential factors: Small-batch training results in flat minima, while large-batch training results in sharp minima. Increased width prevents chaotic behavior, and skip connections dramatically widen minimizers (see figure in the beginning).\nInterpreting with precaution: The loss surface is viewed under a dramatic dimensionality reduction. According to the authors’ analysis, if non-convexity is present in the dimensionality reduced plot, then non-convexity must be present in the full-dimensional surface as well. However, apparent convexity in the low-dimensional surface does not mean the high-dimensional function is truly convex. Rather it means that the positive curvatures are dominant.\nIn a nutshell: It’s a great work trying to visualize the mystery of what’s going well/bad when training a neural model. Although claiming the study to be empirical, I personally found their experiments and results very convincing. Appendix B about visualizing optimization paths is also very insightful, and the authors probably also thought so, so they decided to move it as a main section in their latest Arxiv version 😄!\nFurther thoughts/questions:\nHas it been done for visualizing NLP models? Is it more appropriate to visualize loss for NLG or other measures? This might depend on how to define “labels” in NLG tasks. How big a convolution filter normally is? What’s similar between RNN and skip connections? This work can be used together with automatic neural architecture search, but is there any other more efficient way of getting better models? Generalization in neural nets: a perspective from science (not math) Starting at 1:54:00 in the video. ↩︎\n","date":1588752823,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588752823,"objectID":"2bc7b91284bd51e705484a592cafaeec","permalink":"https://shaojiejiang.github.io/post/en/visualizing-loss/","publishdate":"2020-05-06T10:13:43+02:00","relpermalink":"/post/en/visualizing-loss/","section":"post","summary":"What characterizes a easier to train, easier to generalize neural model?","tags":[],"title":"Visualizing the Loss Landscape of Neural Nets","type":"post"},{"authors":["江少杰"],"categories":["交叉"],"content":"前注：如无特殊说明，本文的摘录均取自\u0026#34;摄影的艺术 (世界顶级摄影大师)\u0026#34; by Bruce Barnbaum, 樊智毅\n“自我审问要有一个合理的极限。在因为过于自省而感到焦虑之前，你应该通过拍摄一些照片来对外交流。”\n在读到这段话的时候，我想起了机器学习对我个人学习方式的一些启发，同时也是我博士导师Maarten对我的教导：表述（比如演讲、报告、交流和写作等）是学习的一个重要环节。 孔子说：\n“学而不思则罔，思而不学则贻。” ——论语·为政\n但我认为，如果没有一个表述的过程，“学”和“思”很有可能会变成空中楼阁。 只有通过将自己的观点表达出来、放到现实中让它们去经受时间的考验、经受别人的批评，才能真正地将自己的所学所思落到实地。 因此我认为，学习的三要素可以归纳为：学、思和述。\n向“机器学习”学习 近来随着对“机器学习”思考的深入，我逐渐从中得到一些启发，比如我开始重视阅读就是因为认识到了大数据量对训练机器学习算法的重要性：如果算法需要输入大量的数据以及反复地学习才能得到理想的性能，那么我是不是也应该这么做呢？ 阅读就是人类学习的一个重要“输入”方式。 它让前人总结出的思想精华，穿越了时空的限制不断地输入到读者的思想中。 当然，其他更加实时实地的输入方式也是不可或缺的，比如参加别人的演讲和报告。 我常常自嘲：机器学习领域的大师们把工作和生活中总结出来的哲理应用到机器学习算法上，而我这样的无名之辈从他们的工作中都能学到受之不尽的哲理。 封神之路任重而道远啊，哈哈！\n最近对机器学习有了更加深入的理解，认识到“输入”、“处理”和“输出检验”这三个环节，缺一不可。 而反思下我自己，输入（阅读）和处理（思考）正在稳步进行，但输出（表述）却还做得远远不够啊。 这也是我最近决定培养记笔记习惯的一个重要原因。 那么在我业余爱好的摄影领域，“输出”就不是记笔记或跟别人语言交流那么简单了，而更多地应该是通过拍摄照片来表述自己：\n“成功地表达你的信息，是创意摄影的根本。”\n关于表述 “单纯地向别人汇报你看到的场景，那是逃避责任；把场景演绎出来，才是接受挑战。虽然场景可能不是你创造出来的，但照片却一定是！因此，不要止步于你的所见，加入你的评论、感受和建议，把它们都放到照片中吧，表达你的观点，阐明你的立场，让读者信服你的结论。”\n这段话是在讲艺术摄影的表达方式：从自己看到的场景（输入）中提炼出自己的观点（处理），并把这些观点表现在自己的作品中（输出）。 同时这段话也阐述了表达方式的三个层次：不加思索拍出的快照是最底层的、简单的记录，融入了自己观点或视角的作品是中层的，能够让观众明白你传达出的观点才是最高层的。 那么写作又何尝不是如此呢？ 简单的记叙是最底层的，就算多用些华丽的词藻（对应到摄影上就是套用构图的范式、），也还是一篇没有思想的文字；表达了自己思想的文字是勇敢的，至于观点能不能被读者接受，是一个需要不断反思的过程：是自己的表达方式不够有说服力，还是想法太超前而不为世人接受？\n“了解自己要说什么！ 了解自己要怎么说！ 然后毫不妥协地说出来”\n有意思的是，这也正是我自己最近关于写作的心得，尤其是在指导研究生毕业论文的时候，我对他们说出了几乎同样话，只不过我第三部分的观点要更批判性一些：站在读者的客观角度去审视自己有没有很好地传达观点。\n以上一直在说“机器学习”和“阅读”对我学习、摄影创作的影响以及他们之前的共性。 反思机器学习，其实上面提到的表达三层次对机器学习的研究也是具有指导意义的：简单的记述（autoencoder）是最低层次；具有创作性的表达方式（对话、摘要、翻译等等）是中层的；而如何让机器的表达更为人类所接受（是否合理、连贯、有趣），正是现在领域内的研究难点。 这也许对机器学习的多任务训练有些价值？\n关于摄影的“真实” “我认为大部分艺术家所主要追寻的不是真实，而是一种恰当的方式，以表达他们所理解的真实。”\n人都是有思想的，有思想就一定会有主观，那么一个作者基于现实事物的创作，无论是摄影还是文学，都一定是主观的。 就算你能够保证自己作品的绝对客观和真实，你也没有办法保证读者带着主观情绪来审视你的作品，那么在他们眼里，你还是主观的。 是不是很有些相对论的意思，哈哈。\n真正的客观和真实是不存在的，也不应该作为终极追求。 正如我上面所说的对表达方式的反思，应该在自己的表达和别人的接受度上取一个合理的平衡，这才是创作的精髓吧。\n","date":1588230727,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588230727,"objectID":"79e2855329990f9fe3732456da3f1220","permalink":"https://shaojiejiang.github.io/post/zh/three-key-elements-of-learning/","publishdate":"2020-04-30T09:12:07+02:00","relpermalink":"/post/zh/three-key-elements-of-learning/","section":"post","summary":"通过我对机器学习的长期研究和思考，以及在阅读中看到的他人观点，我认为可以将学习的过程归纳为三要素：学、思、述。本文主要以“摄影的艺术”一书作为依据。","tags":["读书笔记","机器学习","摄影"],"title":"学习的三要素","type":"post"},{"authors":["江少杰"],"categories":["摄影"],"content":"前注：如无特殊说明，本文的摘录均取自\u0026#34;摄影的艺术 (世界顶级摄影大师)\u0026#34; by Bruce Barnbaum, 樊智毅\n“你的兴趣是什么？只有你自己可以回答。但这个回答是非常重要的，因为如果你要创作有意义的摄影作品，就必须专注于那些你最感兴趣的领域。不仅如此，你还必须专注于那些你具有强烈个人想法的领域。”\n这是一个被大师们多次强调的心得：兴趣是第一导师。 而如果没有兴趣会怎么样？ 不得不说，下面这段话里描述的感受似曾相识。。\n“在日常对话中，你是否尝试过在你不感兴趣或没什么见解的主题上，说一些具有意义的话？这是不可能的！你无话可说，因为你不感兴趣。不过，这一般不会妨碍你继续讨论。正如人们谈论没有兴趣的话题一样，他们也可以拍摄其不感兴趣的事物，而结果是一样的：枯燥乏味。”\n这个例子也很生动：\n“以一个伟大的演说家（例如丘吉尔或者马丁·路德·金）为例，如果我们让他们对缝被子这个主题作一次激情洋溢的演讲，他们是没法做到的！他们无话可说，因为这不是其话题所在、其激情所在。他们需要在自己的主题上展示伟大的演说才华和说服技巧。”\n大师们的作品一般都是风格非常一致的，因为他们很专注。 而正是专注，才让他们成为大师。 世界上的沃土那么多，但如果仅仅局限于走马观花似地去游览这些土地，而不是选择一块进行深耕，也是不会有任何收获的，这同时也是最近自己对科研的一些感悟。 现在慢慢认识到，所谓的“鄙视链”，一般都是处在起跑线上犹豫的选手对于前途望而生畏的感叹。 真正在赛道上奔驰的专业运动员，是不会有时间和心情去思考这些的，因为对于他们来说，他们的领域跟“上游”和“下游”领域之间的区别仅仅是赛场的不同，而每个赛场都有足够激烈的比赛来供他们忙碌、有足够大的荣誉来供他们争取。 用Stephen Covey的话来说，这叫富足思维（abundance mentality）。\n“而伟大的摄影家则知道什么是他们感兴趣的、什么是他们觉得乏味的，也能认识到自己的强项和弱项，并专于自己的兴趣和强项。他们会定期地在其他领域进行一些尝试，来扩大自己的兴趣范围并改进他们的弱项（你也应该这样），但他们不会把尝试性的拍摄和严肃深刻的表达混淆。 韦斯顿不会拍摄瞬间发生的事情，纽曼不会拍摄风景照，尤斯曼不会拍摄不幸的社会成员，阿勃丝不会印制出超现实效果的多重影像。他们的每一位都专注于自己兴趣最大、本领最强的领域。他们或许可以在其他领域创作出不错的作品，但这些作品的永恒性和冲击力会大打折扣。他们，还有其他伟大的摄影家，都睿智地决定在他们最擅长的领域内创作。”\n回到正题摄影上。 我从一年半前认真对待摄影以来，还没有真正对什么主题感兴趣过，或者说没有坚持下来。 刚开始是带着18-200mm套机镜头，在大街小巷里漫无目的地穿梭，不知道自己想拍什么。 就算偶尔看到感觉有意思的东西，也不知道该怎么把它们记录下来。 几乎所有的题材都是一时兴起、浅尝辄止，尤其是在买了新设备之后拍一些适合新镜头的主题。 比如当时买了85mm f/1.8镜头和三脚架，也在拍了一两次自拍照之后便再没有尝试过，虽然后来有过一两次的冲动；买了70-300mm长焦镜头也就拍了一两次鸟；买105mm微距只拍了几次蘑菇；买10-20mm广角镜头也只拍了几次建筑。 说来实在惭愧，微距和广角镜头让自己满意的两张照片，还都分别是在刚买这两支镜头那会拍下的： 蘑菇，105mm微距 “箭头”，10-20mm广角 其实很多摄影题材都有吸引我的地方。 拍摄旅游题材可以收获各地的美景照，但是前期需要对目的地的人文、历史甚至天气和交通等都要有充足的了解；拍摄微距可以把渺小的事物以震撼的角度展示出来，前提是我有足够的耐心去做细致入微的观察；长焦摄影可以让我把美妙的野生动物拉近到眼前并展现给观众，但是难度不亚于狙击一个不确定的目标，而且前期对动物习性的了解也是少不了的；风景摄影可以展示人类建筑或是大自然美景的震撼，但是创意角度的选取、怎么避开人群或者让他们很好地融入画面、在什么样的天气拍摄等等也是让我很头大；而人像摄影不仅需要自己的审美观点，还需要建立好跟模特的关系或交流——如果我有模特的话。\n“在更深的层次上，除非摄影师和主角之间有着友好的关系，否则任何尝试都不可以为你的肖像摄影增色（即使没有友好的关系，也至少有实质性的沟通或强烈的第一印象）。摄影师应该认识主角，对他感兴趣，对他有一定的看法，并努力把主角的个性以最强烈的方式表达出来。有时，摄影师必须强烈依赖于第一印象，因为他往往很难花上足够的时间来完全了解主角。”\n总而言之，任何题材下好的作品似乎都离不开前期做大量的功课。 我明显是摄影态度还不够端正：想要好的照片又总嫌弃过程太辛苦，能静下心来看些摄影教材也是从前不久才开始的。\n没有什么好的照片是靠运气得来的；就算有，也需要提前把基本功练好才能抓住稍纵即逝的机会。 我无法忘记刚买D7200的时候，现实给我的一次教训。 当时拿着刚到手的新相机，兴致冲冲地就上街取景了，然后拍下了这张后来很快意识到问题很多的照片： 网红墙下睡觉的猫，未调修 那天我很幸运，因为偶遇了这面网红墙。当时还不知道这面墙小有名气，只是觉得这句话很醒目、很有个性，最关键的是下面长凳上有只睡觉的猫，就像是有人专门安排的一样！ 然而那天我也是很不幸的，因为首先我当时拍照不会构图，把画面重点的猫放到了左下角非常边缘的位置；其次没有足够了解相机的设置，所以没有记录下来无损的照片，给后期调整带来了很大的限制。 所以当我把这张照片分享到社交媒体上之后反响平平，甚至很多人都没注意到那只在阴影里的猫，反而被占据画面大部分的、高对比的墙吸引了。 其实即便我现在有这样的机会，也还是很难拍好这个场面的：墙上的涂鸦太大、太醒目；猫的位置不合适，很难作为主体被突出。 当然，要想拍的比我这张好，还是很容易的，毕竟这张照片已经不能再烂了，哈哈！ 我当天拍了好多张，其中也有些角度好点的，但无奈当时自己太傻，把那些照片以“锐度不够”删掉了。。。 都是新手学费啊！ 这样的事情当然是少有的可惜。 但如果我不练好基本功、不找到自己的摄影风格，我还会继续浪费快门数。\n后来我又多次拜访过这面墙，可惜再也没见到有猫在下面睡觉了。。\n后记：在后续对本书的学习中，看到了作者对如下作品的介绍，让我重新燃对猫那张照片的一点信心。\n乞女 by Bruce Barnbaum “虽然乞丐是视觉中心，但你不会立刻就能发现她。她太小了，无法马上就看得到。但只要你发现了，影像的性质就完全改变了。”\n从Barnbaum的这幅作品里，我认识到有时候对作品的一些打破常规的解释是有必要的：既然大师可以把主体放在不显眼的位置，那么我为什么不可以呢？ 只要我给观者足够的引导和解释，比如这里标题只提猫，等观者发现猫的时候，依然可以体会到我当是看到这个场景时的感受。\n睡觉的猫，抢救版 这里对照片进行了裁剪和黑白处理以过滤掉颜色干扰，并做了split toning以使猫和墙的色彩略有分离。 当然，这里叫它“抢救版”，是因为它离一幅好的作品依然还很远。 如果我再有这样的机会，我会把机位左移，以尽量把猫放在更显眼的位置。\n","date":1588162657,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588162657,"objectID":"00466de38428cc762616bc8f3ebe84e8","permalink":"https://shaojiejiang.github.io/post/zh/photography-subject/","publishdate":"2020-04-29T14:17:37+02:00","relpermalink":"/post/zh/photography-subject/","section":"post","summary":"兴趣是最好的老师。在我找到自己真正的摄影兴趣点之前，我还会不断地浪费快门以及时间。","tags":["读书笔记","摄影"],"title":"我需要明确自己的摄影主题","type":"post"},{"authors":["Shaojie Jiang 江少杰"],"categories":["paper reading notes","Deep Learning"],"content":"My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks1.\nAdditive vs multiplicative halting probability Multiplicative: In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time. It is acknowledged by the authors that using the logits $h_n^t$ as the halting probability at step $n$ might be more straightforward. Therefore, the overall halting probability is calculated as $$p_t^n = h_t^n \\prod_{u=1}^{n-1} (1 - h_t^u).$$ We use $(1 - h_t^u)$ for previous update steps to indicate that the updating is not stopped until $n$.\nAs each $p_t^n \\in (0, 1)$ is relatively independent with each other and $\\sum p_t^n$ is not bound to 1, this approach does not restrict the update depth to grow arbitrarily. The model can be of course trained to lower the expected ponder time $\\rho_t = \\sum n p_t^n$, but it is observed in the experiments that the resulting model is not preferable in two ways:\n$h_t^1$ is usually just below threshold, intermediate $h_t^n = 0$, and final $h_t^N$ is high enough to halt the update. as the expectation is low, $p_t^N \\ll p_t^1$, but the network learns to have a much higher magnitude of output states at step $N$, so that the final output is still dominated by the final state. Additive: In contrast, the additive approach have an constraint of $\\sum p_t^n = 1$, so that the probability is decreased monotonically with the number of updates growing larger. Though being non-differentiable, the total ponder time (total updates at all positions) is penalized to avoid consuming unnecessary computation. There is still one drawback of this approach, however. The performance is sensitive to the penalty factor $\\tau$, which is not intuitive to choose as a hyperparameter.\nAdaptive Computation Time for Recurrent Neural Networks ↩︎\n","date":1588063604,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588063604,"objectID":"79012c930a33c8676646b76d9275465a","permalink":"https://shaojiejiang.github.io/post/en/adaptive-computation-time/","publishdate":"2020-04-28T10:46:44+02:00","relpermalink":"/post/en/adaptive-computation-time/","section":"post","summary":"My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks1.\nAdditive vs multiplicative halting probability Multiplicative: In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time.","tags":[],"title":"Adaptive Computation Time","type":"post"},{"authors":["江少杰"],"categories":["经济学"],"content":" “收入等于人们的需求消费的总量。”\n“投资——购买厂房、设备等——等于将支出“注入”经济中。”\n“在这里，水龙头指利息，即借贷的价格。当利息降低——打开水龙头——借贷变得便宜，更多的人会进行贷款。”\n“世界充满了不确定，人们并不一定要将自己的储蓄与厂房和工厂挂钩。或许你可能只想把钱放在床垫下面以备不时之需。在凯恩斯看来，利息并不能有助于将多余的储蓄转为投资。事实上，储蓄和投资之间并没有关联。”\n“凯恩斯认为，当流出量大于流入量时便会发生经济衰退。” (from “经济学通识课（耶鲁大学出品！耶鲁大学经济学入门课，普通人也能读懂的经济学！理论到现实，搭起用经济学改善现实生活的桥梁 ) (博集经管商务必读系列)” by 尼尔·基什特尼, 张缘, 刘婧)\n传统经济学认为，一个国家的收入等于经济产能，但凯恩斯认为收入等于人们的消费总量。消费等于为经济注入活力，而没有得到利用的储蓄意味着活力的流失。如果无法阻止人们把钱都存起来，并且无法有效利用人们的储蓄，那么经济衰退就会发生。\n也许中国的经济学家早就认可了这个理论，所以他们制定了一个由基础建设为核心的经济发展策略。中国的人民变得喜欢存钱，他们存起来的钱一部分被用来投资公共基础设施：公路、铁路、电力、水利、互联网等等，而且这些设施有利于中国全面的尤其是内陆地区的经济发展。此外，由全面发展带的的经济进步，促使人们拥有了更多的储蓄。他们的储蓄并不会永久增加，因为当下的社会风气迫使他们把几乎所有的储蓄都用在了买房、买车、养老、医疗、旅游以及后代教育上。人们挣得越来越多，而且中国的经济学家们总有办法让人们把手里的钱花出去，于是中国的经济才会持续不断地焕发新活力。\n","date":1588018704,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588018704,"objectID":"598c28728a627b1ac27550dba41a6ac9","permalink":"https://shaojiejiang.github.io/post/zh/keynesianism/","publishdate":"2020-04-27T22:18:24+02:00","relpermalink":"/post/zh/keynesianism/","section":"post","summary":"一个国家的收入等于人们的消费总和。那么所有生产力创作出多少价值其实是次要的，让人们或者替他们把这些价值消费出去，才是经济永葆青春的奥义。","tags":["读书笔记","经济学"],"title":"凯恩斯主义——中国以基建为核心的经济策略理论背景","type":"post"},{"authors":["江少杰"],"categories":["经济学"],"content":" “企业家获得成功的同时也得到了财富。他们的新商品在经济体中传播，人们发现自己想要一个留声机或电视，便出门购买。亨利·福特和安德鲁·卡内基分别靠着生产适用于大众的廉价汽车和在钢铁制造中引入新方法而发财致富。 很快，仿效者们开始仿效最初的企业家，生产出了同样的汽车、熔炉或染料。新商品和技术传播地更远了，这引起了整个行业的变革，并扩大了经济体量。最终，一些企业倒闭，经济开始萎缩，直至新一轮创新出现。资本主义的荣衰与浮沉都源自层出不穷的创新浪潮以及创业和模仿的消长。” (from “经济学通识课（耶鲁大学出品！耶鲁大学经济学入门课，普通人也能读懂的经济学！理论到现实，搭起用经济学改善现实生活的桥梁 ) (博集经管商务必读系列)” by 尼尔·基什特尼, 张缘, 刘婧)\n诚然，拼多多也许在技术上、营销上并没有为同行们带来什么创新，而且还破坏了很多工薪阶层对网购平台的印象，然而也许这些都只是负面作用。整体上拼多多使得方便的网购果实惠及到偏远的乡村家庭里，可以说最大化了当前网购产业的利用率，甚至还可能会带动乡村经济的发展。\n就像引文里提到的福特。也许他刚开始也因为生产廉价的汽车而为贵族所不齿，也许他的汽车也伤过一时贪图便宜的贵族的心，但不可否认的是，正是他的这种努力才使得汽车成为平民大众的交通工具而不是贵族用来炫耀的奢侈品。那么拼多多也是承担了这样一个角色。\n作为生在新时代的我们，亦或是生活在大城市、坐在舒适办公桌前的我们，可能无法理解在网购领域已经有淘宝天猫京东等平台覆盖了从廉价到品质各个价格区间，为什么还会有拼多多来进一步拉低品质与价格的下限。我们之所以会有这样的想法，是因为我们在与网购一起成长，我们接受了网购同时也成就了网购。习惯成自然的我们忽视了有一个群体一直都不在我们与网购形成的共生体中——那些生活在乡村的贫苦农民们。\n他们不能像我们一样很快适应新科技、新潮流的发展，更不能像我们一样在价格与品质之间随性选择。记得之前在一篇时事点评里（『每日人物』关于拼多多的点评《他们，在拼多多上拼运气》。惊讶得发现文章已经是2018年8月的了，我明明记得是去年看过的啊。。）看到过，我们眼里的假冒伪劣产品，可能是那些穷人眼里的一次消费升级。他们不会用淘宝天猫京东，也没有动力去学习如何使用——毕竟他们需求低，就算有需求，这些平台的商品也可能会超出他们的（心理）承受范围。若不是拼多多依靠低价、亲友帮忙砍价等等策略让这个群体接受这种新的消费方式，网络购物何时才能渗透到社会的神经末梢里去？拼多多是一个模仿者，也是一个推广者、一股深化技术对社会变革不可缺少的力量。\n","date":1588017790,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017790,"objectID":"187345ecebe6a7c7bbd9f50af4cba873","permalink":"https://shaojiejiang.github.io/post/zh/pinduoduo/","publishdate":"2020-04-27T22:03:10+02:00","relpermalink":"/post/zh/pinduoduo/","section":"post","summary":"为人们所不齿的拼多多不断发展壮大并在美国上市，这其中是不是反映了它存在的价值以及历史的必然性呢？","tags":["读书笔记","经济学"],"title":"拼多多或许造福了中国的贫下中农以及经济与科技","type":"post"},{"authors":["Shaojie Jiang 江少杰"],"categories":["Deep Learning"],"content":"This is a growing list of pointers to useful blog posts and papers related to transformers.\nTransformers explained Blog: The Illustrated Transformer has many intuitive animations of how transformer models work Blog: Universal Transformers introduces the idea of recurrence among layers Blog: Transformer vs RNN and CNN for Translation Task GNNs: similarities and differences Blog: Transformers are Graph Neural Networks bridges transformer models and Graph Neural Networks Transformer improvements Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems Nural Turing Machine + transformer? ","date":1583155619,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583155619,"objectID":"0d4a4f10dd74af99cd5d991e25459aa5","permalink":"https://shaojiejiang.github.io/post/en/transformer-blog-paper-hub/","publishdate":"2020-03-02T14:26:59+01:00","relpermalink":"/post/en/transformer-blog-paper-hub/","section":"post","summary":"This is a growing list of pointers to useful blog posts and papers related to transformers.\nTransformers explained Blog: The Illustrated Transformer has many intuitive animations of how transformer models work Blog: Universal Transformers introduces the idea of recurrence among layers Blog: Transformer vs RNN and CNN for Translation Task GNNs: similarities and differences Blog: Transformers are Graph Neural Networks bridges transformer models and Graph Neural Networks Transformer improvements Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems Nural Turing Machine + transformer?","tags":[],"title":"A Hub for Transformer Blogs and Papers","type":"post"},{"authors":["Shaojie Jiang","Thomas Wolf","Christof Monz","Maarten de Rijke"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"1c1fe13c5c7acd80fdbd6dd25ebfff8a","permalink":"https://shaojiejiang.github.io/publication/jiang-2020-tldr/","publishdate":"2020-04-08T08:07:10.989311Z","relpermalink":"/publication/jiang-2020-tldr/","section":"publication","summary":"","tags":null,"title":"TLDR: Token Loss Dynamic Reweighting for Reducing Repetitive Utterance Generation","type":"publication"},{"authors":["Shaojie Jiang 江少杰"],"categories":["NLP","Deep Learning"],"content":"R.I.P BERT BERT got a head shot yesterday, by another guy called XLNet. It is reported that XLNet defeated BERT on 20 NLP tasks, and achieved 18 new state-of-the-art results. Isn’t it impressive? So, farewell, BERT. R.I.P BERT Is BERT really dead? Since I love BERT, I decided to read the paper to find out what killed him. While reading, I was thinking wait a minute, is BERT really dead? After finished the paper, I was so glad to know that BERT is still well alive! He is just wearing another coat named Two-Stream Self-Attention (TSSA), with some other gadgets! Because:\nXLNet = BERT + TSSA + bidirectional data input\nBert you’re so tough, buddy!\nLet’s take a closer look at what were trying to kill BERT.\nTwo-stream self-attention (TSSA) Why TSSA is needed to kill BERT? Well, let’s first see some weaknesses BERT has.\nBERT is using a masked language model (MLM) training objective, which is essentially why it achieves bidirectional representation. Image source In this example, both words “store” and “gallon” are intended to be predicted by BERT, and their input word embeddings are replaced by the embedding of a special token [MASK]. Usually this isn’t a problem, but what if the prediction of “store” requires knowing the word “gallon”? That is exactly where BERT falls short.\nTSSA is what you can use to overcome that downside of MLM: Query stream, source In this illustration, query stream gives you the query vector needed for attention calculation, and this stream is designed in such a way that it doesn’t leak the info of the word it’s going to predict, but guarantees all information from other positions. Take $x_1$ for example: $x_1$’s embedding (and hidden state) is not used at all, but embeddings and hidden states from other positions are used in each layer.\nContent stream, source Content stream, on the other hand, gives you the key and value vectors needed for context vector calculation. This stream uses a strategy similar to that in a standard Transformer decoder by masking future positions. The only difference is that in content stream, the order of tokens is randomly permuted. For example $x_2$ is right after $x_3$, and therefore $h_2^{(1)}$ can only see the embedding of itself and that of $x_3$ (and $mem^{(0)}$), but not that of $x_1$ or $x_4$.\nMask a span Another difference from BERT is masking a span of consecutive words. The reason I guess, is that this guarantees the dependence of masked words (as claimed to be what BERT can’t model). This is not a fresh-new idea, though. Recently there are two ERNIE papers (BERT based) that propose masking named entities (often of multiple words, paper link) and/or phrases (paper link).\nBidirectional data input Another notably different thing in XLNet is the usage of bidirectional data input. The idea (I guess) is to decide the factorization direction (either forward or backward), so that the idea of “masking future positions” used in a standard Transformer decoder can also be easily used together with XLNet.\nMasking a span makes XLNet look like a denoising autoencoder; but by using bidirectional data input (or masking future positions), XLNet performs more like a autoregressive language model in the masked region.\nClosing remarks So now you probably can see the similarities and differences between XLNet and BERT. If not, here is a quick summary:\nInstead of masking random words, mask a span of words Use bidirectional data input to decide which direction you treat as “future”, and then apply the idea of masking future positions To avoid leaking the information of the position to be predicted, use Two-Stream Self-Attention (TSSA) Other minor things like segment recurrence, relative positional encoding, etc. However, it doesn’t seem to be enough changes to make all those improvements. What if BERT is also trained using the additional data (Giga5, ClueWeb, Common Crawl), will XLNet still be able to defeat BERT?\nEDIT:\nAnother model named MASS employs a very similar idea. According to Jacob Devlin (author of BERT), relative positional embedding might be of great importance. ","date":1560988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562158067,"objectID":"3e123752d170cde6c79a5c3e36823981","permalink":"https://shaojiejiang.github.io/post/en/xlnet/","publishdate":"2019-06-20T00:00:00Z","relpermalink":"/post/en/xlnet/","section":"post","summary":"In this post, I will try to understand what makes XLNet better than BERT.","tags":["BERT","Transformer","XLNet"],"title":"What's New in XLNet?","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://shaojiejiang.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Shaojie Jiang","Pengjie Ren","Christof Monz","Maarten de Rijke"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"b3a4e5ab9d577302864b0f9fed964dea","permalink":"https://shaojiejiang.github.io/publication/jiang-2019-improving/","publishdate":"2019-07-03T14:11:43.236784Z","relpermalink":"/publication/jiang-2019-improving/","section":"publication","summary":"","tags":["Chatbot","Dialogue system","Sequence-to-sequence model"],"title":"Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss","type":"publication"},{"authors":["Shaojie Jiang","Maarten de Rijke"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"efb8b2e27e980d0e5fc82c2aeb85ada0","permalink":"https://shaojiejiang.github.io/publication/jiang-2018-sequence/","publishdate":"2019-07-03T14:11:43.236107Z","relpermalink":"/publication/jiang-2018-sequence/","section":"publication","summary":"","tags":null,"title":"Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots","type":"publication"},{"authors":["Shaojie Jiang","Jifeng Ning","Cheng Cai","Yunsong Li"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"78279d46cb00b734ed5b0a0e62a5ece1","permalink":"https://shaojiejiang.github.io/publication/jiang-2017-robust/","publishdate":"2019-07-03T14:11:43.237507Z","relpermalink":"/publication/jiang-2017-robust/","section":"publication","summary":"","tags":null,"title":"Robust Struck tracker via color Haar-like feature and selective updating","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://shaojiejiang.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://shaojiejiang.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Jifeng Ning","Jimei Yang","Shaojie Jiang","Lei Zhang","Ming-Hsuan Yang"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"9c2efa257f5582fd9c1a59c4dbed5c4d","permalink":"https://shaojiejiang.github.io/publication/ning-2016-object/","publishdate":"2019-07-03T14:11:43.235175Z","relpermalink":"/publication/ning-2016-object/","section":"publication","summary":"","tags":null,"title":"Object tracking via dual linear structured SVM and explicit feature map","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ef616ea5fc1a401bdb2aee1473e427c9","permalink":"https://shaojiejiang.github.io/home-zh/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home-zh/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":" Tips: Please change to Light theme 提示：请换成明亮主题\nNote: If the integrated search function fails to return content, try Google search. 说明：如果集成搜索功能失效，请使用谷歌搜索。\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4e4f919455da0987a141b4ef412aeb48","permalink":"https://shaojiejiang.github.io/search/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/search/","section":"","summary":"Tips: Please change to Light theme 提示：请换成明亮主题\nNote: If the integrated search function fails to return content, try Google search. 说明：如果集成搜索功能失效，请使用谷歌搜索。","tags":null,"title":"Google Search this site 全站谷歌搜索","type":"page"}]
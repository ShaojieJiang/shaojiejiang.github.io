[{"authors":["admin"],"categories":null,"content":"I am a 3rd year PhD student under the supervision of Maarten de Rijke and Christof Monz at ILPS, University of Amsterdam. I am interested in ML and NLP, especially open-domain dialogue systems (chatbots ðŸ¤–). I also take photos as a hobby ðŸ“¸.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1588017132,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shaojiejiang.github.io/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a 3rd year PhD student under the supervision of Maarten de Rijke and Christof Monz at ILPS, University of Amsterdam. I am interested in ML and NLP, especially open-domain dialogue systems (chatbots ðŸ¤–).","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":"My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks1.\nAdditive vs multiplicative halting probability Multiplicative: In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time. It is acknowledged by the authors that using the logits $h_n^t$ as the halting probability at step $n$ might be more straightforward. Therefore, the overall halting probability is calculated as $$p_t^n = h_t^n \\prod_{u=1}^{n-1} (1 - h_t^u)$$. We use $(1 - h_t^u)$ for previous update steps to indicate that the updating is *not* stopped until $n$.\nAs each $p_t^n \\in (0, 1)$ is relatively independent with each other and $\\sum p_t^n$ is not bound to 1, this approach does not restrict the update depth to grow arbitrarily. The model can be of course trained to lower the expected ponder time $\\rho_t = \\sum n p_t^n$, but it is observed in the experiments that the resulting model is not preferable in two ways:\n $h_t^1$ is usually just below threshold, intermediate $h_t^n = 0$, and final $h_t^N$ is high enough to halt the update. as the expectation is low, $p_t^N \\ll p_t^1$, but the network learns to have a much higher magnitude of output states at step $N$, so that the final output is still dominated by the final state.  Additive: In contrast, the additive approach have an constraint of $\\sum p_t^n = 1$, so that the probability is decreased monotonically with the number of updates growing larger. Though being non-differentiable, the total ponder time (total updates at all positions) is penalized to avoid consuming unnecessary computation. There is still one drawback of this approach, however. The performance is sensitive to the penalty factor $\\tau$, which is not intuitive to choose as a hyperparameter.\n   Adaptive Computation Time for Recurrent Neural Networks \u0026#x21a9;\u0026#xfe0e;\n   ","date":1588063604,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588144286,"objectID":"05de8094936b345ef11c707b13c99c0a","permalink":"https://shaojiejiang.github.io/post/adaptive-computation-time/","publishdate":"2020-04-28T10:46:44+02:00","relpermalink":"/post/adaptive-computation-time/","section":"post","summary":"My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks1.\nAdditive vs multiplicative halting probability Multiplicative: In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time.","tags":[],"title":"Adaptive Computation Time","type":"post"},{"authors":[],"categories":[],"content":"This is a growing list of pointers to useful blog posts and papers related to transformers.\nTransformers explained   Blog: The Illustrated Transformer has many intuitive animations of how transformer models work  Blog: Universal Transformers introduces the idea of recurrence among layers  Blog: Transformer vs RNN and CNN for Translation Task  GNNs: similarities and differences   Blog: Transformers are Graph Neural Networks bridges transformer models and Graph Neural Networks  Transformer improvements   Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems Nural Turing Machine + transformer?  ","date":1583155619,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"009e43aa18fc8b1dcb47093e6fa52979","permalink":"https://shaojiejiang.github.io/post/transformer-blog-paper-hub/","publishdate":"2020-03-02T14:26:59+01:00","relpermalink":"/post/transformer-blog-paper-hub/","section":"post","summary":"This is a growing list of pointers to useful blog posts and papers related to transformers.\nTransformers explained   Blog: The Illustrated Transformer has many intuitive animations of how transformer models work  Blog: Universal Transformers introduces the idea of recurrence among layers  Blog: Transformer vs RNN and CNN for Translation Task  GNNs: similarities and differences   Blog: Transformers are Graph Neural Networks bridges transformer models and Graph Neural Networks  Transformer improvements   Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems Nural Turing Machine + transformer?","tags":[],"title":"A Hub for Transformer Blogs and Papers","type":"post"},{"authors":["Shaojie Jiang","Thomas Wolf","Christof Monz","Maarten de Rijke"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"1c1fe13c5c7acd80fdbd6dd25ebfff8a","permalink":"https://shaojiejiang.github.io/publication/jiang-2020-tldr/","publishdate":"2020-04-08T08:07:10.989311Z","relpermalink":"/publication/jiang-2020-tldr/","section":"publication","summary":"","tags":null,"title":"TLDR: Token Loss Dynamic Reweighting for Reducing Repetitive Utterance Generation","type":"publication"},{"authors":[""],"categories":["NLP","Deep Learning"],"content":"R.I.P BERT  BERT got a head shot yesterday, by another guy called XLNet. It is reported that XLNet defeated BERT on 20 NLP tasks, and achieved 18 new state-of-the-art results. Isn\u0026rsquo;t it impressive? So, farewell, BERT.   R.I.P BERT   Is BERT really dead? Since I love BERT, I decided to read the paper to find out what killed him. While reading, I was thinking wait a minute, is BERT really dead? After finished the paper, I was so glad to know that BERT is still well alive! He is just wearing another coat named Two-Stream Self-Attention (TSSA), with some other gadgets! Because:\nXLNet = BERT + TSSA + bidirectional data input\nBert you\u0026rsquo;re so tough, buddy!\nLet\u0026rsquo;s take a closer look at what were trying to kill BERT.\nTwo-stream self-attention (TSSA) Why TSSA is needed to kill BERT? Well, let\u0026rsquo;s first see some weaknesses BERT has.\nBERT is using a masked language model (MLM) training objective, which is essentially why it achieves bidirectional representation.   Image source   In this example, both words \u0026ldquo;store\u0026rdquo; and \u0026ldquo;gallon\u0026rdquo; are intended to be predicted by BERT, and their input word embeddings are replaced by the embedding of a special token [MASK]. Usually this isn\u0026rsquo;t a problem, but what if the prediction of \u0026ldquo;store\u0026rdquo; requires knowing the word \u0026ldquo;gallon\u0026rdquo;? That is exactly where BERT falls short.\nTSSA is what you can use to overcome that downside of MLM:   Query stream, source   In this illustration, query stream gives you the query vector needed for attention calculation, and this stream is designed in such a way that it doesn\u0026rsquo;t leak the info of the word it\u0026rsquo;s going to predict, but guarantees all information from other positions. Take $x_1$ for example: $x_1$'s embedding (and hidden state) is not used at all, but embeddings and hidden states from other positions are used in each layer.\n  Content stream, source   Content stream, on the other hand, gives you the key and value vectors needed for context vector calculation. This stream uses a strategy similar to that in a standard Transformer decoder by masking future positions. The only difference is that in content stream, the order of tokens is randomly permuted. For example $x_2$ is right after $x_3$, and therefore $h_2^{(1)}$ can only see the embedding of itself and that of $x_3$ (and $mem^{(0)}$), but not that of $x_1$ or $x_4$.\nMask a span Another difference from BERT is masking a span of consecutive words. The reason I guess, is that this guarantees the dependence of masked words (as claimed to be what BERT can\u0026rsquo;t model). This is not a fresh-new idea, though. Recently there are two ERNIE papers (BERT based) that propose masking named entities (often of multiple words, paper link) and/or phrases ( paper link).\nBidirectional data input Another notably different thing in XLNet is the usage of bidirectional data input. The idea (I guess) is to decide the factorization direction (either forward or backward), so that the idea of \u0026ldquo;masking future positions\u0026rdquo; used in a standard Transformer decoder can also be easily used together with XLNet.\nMasking a span makes XLNet look like a denoising autoencoder; but by using bidirectional data input (or masking future positions), XLNet performs more like a autoregressive language model in the masked region.\nClosing remarks So now you probably can see the similarities and differences between XLNet and BERT. If not, here is a quick summary:\n Instead of masking random words, mask a span of words Use bidirectional data input to decide which direction you treat as \u0026ldquo;future\u0026rdquo;, and then apply the idea of masking future positions To avoid leaking the information of the position to be predicted, use Two-Stream Self-Attention (TSSA) Other minor things like segment recurrence, relative positional encoding, etc.  However, it doesn\u0026rsquo;t seem to be enough changes to make all those improvements. What if BERT is also trained using the additional data (Giga5, ClueWeb, Common Crawl), will XLNet still be able to defeat BERT?\nEDIT:\n Another model named MASS employs a very similar idea. According to Jacob Devlin (author of BERT), relative positional embedding might be of great importance.  ","date":1560988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"062f3b0d22289906f49ec859d89f3c3c","permalink":"https://shaojiejiang.github.io/post/xlnet/","publishdate":"2019-06-20T00:00:00Z","relpermalink":"/post/xlnet/","section":"post","summary":"In this post, I will try to understand what makes XLNet better than BERT.","tags":["BERT","Transformer","XLNet"],"title":"What's New in XLNet?","type":"post"},{"authors":["Shaojie Jiang","Pengjie Ren","Christof Monz","Maarten de Rijke"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"b3a4e5ab9d577302864b0f9fed964dea","permalink":"https://shaojiejiang.github.io/publication/jiang-2019-improving/","publishdate":"2019-07-03T14:11:43.236784Z","relpermalink":"/publication/jiang-2019-improving/","section":"publication","summary":"","tags":["Chatbot","Dialogue system","Sequence-to-sequence model"],"title":"Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss","type":"publication"},{"authors":["Shaojie Jiang","Maarten de Rijke"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"efb8b2e27e980d0e5fc82c2aeb85ada0","permalink":"https://shaojiejiang.github.io/publication/jiang-2018-sequence/","publishdate":"2019-07-03T14:11:43.236107Z","relpermalink":"/publication/jiang-2018-sequence/","section":"publication","summary":"","tags":null,"title":"Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots","type":"publication"},{"authors":["Shaojie Jiang","Jifeng Ning","Cheng Cai","Yunsong Li"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"78279d46cb00b734ed5b0a0e62a5ece1","permalink":"https://shaojiejiang.github.io/publication/jiang-2017-robust/","publishdate":"2019-07-03T14:11:43.237507Z","relpermalink":"/publication/jiang-2017-robust/","section":"publication","summary":"","tags":null,"title":"Robust Struck tracker via color Haar-like feature and selective updating","type":"publication"},{"authors":["Jifeng Ning","Jimei Yang","Shaojie Jiang","Lei Zhang","Ming-Hsuan Yang"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"9c2efa257f5582fd9c1a59c4dbed5c4d","permalink":"https://shaojiejiang.github.io/publication/ning-2016-object/","publishdate":"2019-07-03T14:11:43.235175Z","relpermalink":"/publication/ning-2016-object/","section":"publication","summary":"","tags":null,"title":"Object tracking via dual linear structured SVM and explicit feature map","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588017132,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://shaojiejiang.github.io/cv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/cv/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shaojie Jiang&#39;s Homepage</title>
    <link>https://shaojiejiang.github.io/</link>
      <atom:link href="https://shaojiejiang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Shaojie Jiang&#39;s Homepage</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://shaojiejiang.github.io/media/icon_huf1850796dc0c27e76df1b37fe2f35b33_25680_512x512_fill_lanczos_center_3.png</url>
      <title>Shaojie Jiang&#39;s Homepage</title>
      <link>https://shaojiejiang.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://shaojiejiang.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Has the era of AI gaming already come?</title>
      <link>https://shaojiejiang.github.io/post/en/ai-gaming-era/</link>
      <pubDate>Sun, 13 Aug 2023 17:05:13 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/ai-gaming-era/</guid>
      <description>&lt;p&gt;With the big noises made by ChatGPT, many different industries have noticed the value of LLM technologies.
Unsurprisingly, the gaming industry is one of them.
In this blog, I introduce several cool demos/WIPs that I&amp;rsquo;ve recently found, and share my opinions on why they might have profound influences on the future of gaming industry.
I also try to explain the current difficulties, and possible directions for solving them.
In the end, I also share some dreams of future games.
I believe, the era of AI gaming has come!&lt;/p&gt;
&lt;h2 id=&#34;the-matrix-ai-powered-npcs-demo-by-the-replica-studios&#34;&gt;The Matrix AI-Powered NPCs demo by the Replica Studios&lt;/h2&gt;
&lt;p&gt;Players are used to have chats with the NPCs, but most of these conversations are scripted.
The current best conversational experience you can have with NPCs is to select from several possible responses, so you have some freedom of steering dialogues.&lt;/p&gt;


















&lt;figure  id=&#34;figure-dialogue-selection-in-witcher-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Dialogue selection in Witcher 3&#34; srcset=&#34;
               /post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_527b0774b1c015883502fc1666882b7e.webp 400w,
               /post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_2c0e283be5301be66b25147cae746fe8.webp 760w,
               /post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_527b0774b1c015883502fc1666882b7e.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Dialogue selection in Witcher 3
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;If you are a game lover, have you ever dreamt about talking to NPCs like they&amp;rsquo;re other human players?
Well, this is definitely possible now, and the Replica Studios already made a demo about it&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
Instead of looping over pre-scripted lines, the Replica Studios attached LMs (probably OpenAI ChatGPT) to the NPCs, allowing them to all speak characteristically.
You can even chat with NPCs using your voices directly, and they will speak back.
Take a look at this YouTube video&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; of the demo.&lt;/p&gt;
&lt;p&gt;In many games, the plot is driven (or better put, reflected) by chatting with NPCs.
But since LLM chatbots can have randomness in their responses, maybe in the future, the game progression can be take to anywhere, so that every player can have a unique experience in the same game.
This is already partly made true in the AI Dungeon text game&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The Matrix demo may look sleek in the video, but in reality it can take around 10 seconds to get a response from NPCs.
This lag is probably due to many users are calling the LLM API at the same time, and slow processing of several different modules, such as ASR and TTS.
Besides, current general-purpose LLMs like ChatGPT are very large in terms of number of parameters, and this means long processing time.
Potential solutions can be training bespoke, smaller-sized chatbot models, and maybe even audio-to-audio model so that the processing is simplified.&lt;/p&gt;
&lt;h2 id=&#34;herika-by-dwemer-dynamics&#34;&gt;Herika by Dwemer Dynamics&lt;/h2&gt;
&lt;p&gt;The experience that every player being able to conduct unique conversations with each NPC can already be fascinating.
Isn&amp;rsquo;t it more interesting to have a computer-controlled companion, one that can not only chat with you, but can also follow your voice commands?
Then you definitely want to check out Herika, a mod&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; for &lt;em&gt;The Elder Scrolls V: Skyrim&lt;/em&gt;.
Herika is a ChatGPT-powered AI companion that can understand the player&amp;rsquo;s audio and textual inputs.
She is capable of chit-chatting with the player, commenting on the game scenes and events, following the player&amp;rsquo;s various commands, and more.&lt;/p&gt;


















&lt;figure  id=&#34;figure-system-design-of-herika-image-credit-dwemer-dynamics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;System design of Herika. Image credit: Dwemer Dynamics&#34; srcset=&#34;
               /post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_8ef29e5e73f8099642bfabb22652b4d5.webp 400w,
               /post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_76bd25ffc50f49e25b364c14c33177b2.webp 760w,
               /post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_8ef29e5e73f8099642bfabb22652b4d5.webp&#34;
               width=&#34;760&#34;
               height=&#34;418&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      System design of Herika. Image credit: Dwemer Dynamics
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Above is an illustration of Herika&amp;rsquo;s system design.
Here is a brief overview of its main components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Audio inputs and outputs are processed to and from texts by ASR and TTS modules&lt;/li&gt;
&lt;li&gt;Game objects, scenes, locations, etc., are extracted from the game as texts&lt;/li&gt;
&lt;li&gt;The chatting and commenting are all achieved by querying the OpenAI API, in the form of role-playing chats&lt;/li&gt;
&lt;li&gt;Given player&amp;rsquo;s command in natural language, the command-following ability is achieved by asking GPT to generate formatted commands that are used by the game engine to control Herika&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Check out this YouTube video&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; to get the feeling of how Herika works.
Although it seems to work astonishingly well in the video, currently Herika has the same problem of long response time like the Matrix demo.
Of course, another issue is that playing with such a companion can burn money quickly, and this is because most of Herika&amp;rsquo;s functionality is achieved by calling paid APIs.
Still a lot of work to do before this kind of gameplay can get popular, but this mod definitely cracks open another line of bright future!&lt;/p&gt;
&lt;h2 id=&#34;ai-playing-tomb-raider&#34;&gt;AI playing Tomb Raider&lt;/h2&gt;
&lt;p&gt;OK, we&amp;rsquo;ve already seen AI controlling our companion in the game, then what&amp;rsquo;s next?
Controlling the player directly, of course!
Here is a video of AI playing Tomb Raider&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.
In this demo, similar techniques to Herika like LLM and TTS are also used.
What&amp;rsquo;s more, it seems that the author has employed several other AI modules, too, such as object detection.
It&amp;rsquo;s not yet clear how the game character is controlled at the time of writing this blog (08/13/2023).&lt;/p&gt;
&lt;h2 id=&#34;more-work-of-ai-playing-games-in-academia&#34;&gt;More work of AI playing games, in academia&lt;/h2&gt;
&lt;p&gt;It worths noting that using modern AI&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; to play games is not new.
Many previous endeavours have already been made, such as the OpenAI Five&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; playing Dota 2.
Many scientific experiments in the RL field were actually conducted on game environments like OpenAI Gym&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt; and Unity ML-Agents&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;.
However, the research characteristic of this line of work makes it far from revolutionizing the gaming industry, and indeed, this was usually not the indention of researchers.&lt;/p&gt;


















&lt;figure  id=&#34;figure-an-example-of-openai-gym&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An example of OpenAI Gym.&#34;
           src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/openai-gym.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An example of OpenAI Gym.
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-an-example-of-ml-agents&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An example of ML-Agents.&#34; srcset=&#34;
               /post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_5b753ca6463c4352b20c3c6d4160653f.webp 400w,
               /post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_192e35e39d2b75df501a70ef62b5bd4f.webp 760w,
               /post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_5b753ca6463c4352b20c3c6d4160653f.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An example of ML-Agents.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In the recent months, several other research outcomes related to gaming have attracted people&amp;rsquo;s attention, e.g., Generative Agents&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt; by Stanford University, and CALM&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt; by Nvidia.
While Generative Agents might have put more focus on studying human behaviour instead of game playing, CALM presents an algorithms of controlling game characters using textual commands (hence easily with voice through ASR).
What&amp;rsquo;s more interesting about CALM is that the model size it uses to control the game character is as small as several hundreds of parameters, making it easily runnable locally.
Of course, attaching LMs for more flexible natural language understanding can increase the parameter size many times, but still possible to find a good middle ground between performance and latency.&lt;/p&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;It seems that the technologies for applying modern AI in games are already maturing.
Although current AI models, especially those generative ones, are often criticised for problems like hallucination, repetition, and unsafe responses etc., I would argue that such problems will be much less destructive in the game world than in real life.
It would be very interesting to see more and more games with AI companions that chat with you, and give you a hand when asked.
To make it more exciting, how about train your AI companions by yourselves, while you&amp;rsquo;re playing the game?
You already generate a lot of (labelled) data when you play games, and using it to train your AI companion is theoretically possible.
However, popular game engines like Unity and Unreal don&amp;rsquo;t directly support AI training yet, so we still need some time to make it happen.
But games with custom engines is much more flexible, and Human-Like&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt; is such a game that uses your data generated in the game and trains an AI opponent online.&lt;/p&gt;
&lt;p&gt;If tools aren&amp;rsquo;t a problem, what about model sizes?
In the last couple years, we saw best-performing models getting larger and larger, most of which definitely can&amp;rsquo;t run on consumer machines.
I personally believe increasing the model size isn&amp;rsquo;t the ultimate answer.
Luckily, there is another stream of research studying the grokking&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt; phenomenon of models as small as an MLP, with merely several hundreds of parameters.
Probably LMs won&amp;rsquo;t have any sensible performance at this size level, but it&amp;rsquo;s possible to largely decrease the model sizes once we have enough understanding on their mechanisms.&lt;/p&gt;
&lt;p&gt;Taking a step back, not too long ago deep learning and gaming were still almost two extremes of the spectrum: the former is often associated with hard-working researchers, while the latter often reminded us of people killing time.
Gamers are usually those young and smart people, who devoted large amount of time and energy in the game they love.
Since finally the &amp;ldquo;two extremes&amp;rdquo; are coming together, maybe something more profound can happen?
Just some personal thoughts, probably unrealistic, but for instance using LLMs as portals that make the player more interested in real world, and even learn about practical skills that they can use in real life?
It might be possible, who knows?&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.replicastudios.com/blog/smart-npc-plugin-release&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Replica Smart NPCs&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SbzBTp_kBIk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI-Powered NPCs: A Game-Changing FREE Demo&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://aidungeon.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Dungeon: A text-based adventure-story game you direct (and star in) while the AI brings it to life.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nexusmods.com/skyrimspecialedition/mods/89931&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Herika - The ChatGPT Companion&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0svu8WBzeQM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The AI Takes Control of the adventure in Skyrim!&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0wTf_bbkW2U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Creating a Self-Aware Lara Croft that Plays Tomb Raider&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;As opposed to traditional AI used in games, which are usually implemented with sets of rules, here by modern AI I mean those powered by DL and/or RL algorithms&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/OpenAI_Five&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Five&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/getting-started-with-openai-gym/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Started With OpenAI Gym: The Basic Building Blocks&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://unity.com/products/machine-learning-agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unity Machine Learning Agents&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/joonspk-research/generative_agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/par/calm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CALM: Conditional Adversarial Latent Models for Directable Virtual Characters&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://store.steampowered.com/app/1400190/HumanLike/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Human-Like game on Steam&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.02177&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://pair.withgoogle.com/explorables/grokking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Machine Learning Models Memorize or Generalize?&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>One source of LLM hallucination is exposure bias</title>
      <link>https://shaojiejiang.github.io/post/en/llm-hallucination/</link>
      <pubDate>Wed, 09 Aug 2023 22:16:30 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/llm-hallucination/</guid>
      <description>&lt;p&gt;With the release of closed-source ChatGPT, GPT-4, and open-source LLaMa models, the LLM development has seen tremendous improvements in recent months.
While we are hyped with the fact that these LLMs are capable of many tasks, we have also noticed again and again that these LLMs hallucinate content.
Today I came accross this inspiring paper, &lt;a href=&#34;https://arxiv.org/abs/2305.14552&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sources of Hallucination by Large Language Models on Inference Tasks&lt;/a&gt; by McKenna et al., in which the authors have identified two main sources of hallucination:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge that was memorised by the model during pre-training&lt;/li&gt;
&lt;li&gt;Corpus-based heuristics such as term frequency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In my opinion, I would put these two reasons into one category: the exposure bias.
This is becuase either the memorised knowledge, or frequent terms, were exposed to the LLM at pre-training state.
The observation made in this paper is very enlightning, and reminded me of an ealier paper of mine, where we also concluded that the low-diversity issue of generative chatbots are caused by frequent terms in the training corpora&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Although LLMs are becoming larger, trained with more sophisticated techniques like RLHF, they have a deep root in the field of statistical models.
Losses are calculated based on terms, which are used to update the model weights, so it&amp;rsquo;s not surprising at all if the trained LLMs respond differently to terms with different frequencies.
And in fact, it would be surprising if these LLMs only learn &lt;strong&gt;perfect&lt;/strong&gt; grammar and semantics and totally shake off the frequency part.
There is nothing wrong for LLMs being statistical.
We human often make decisions based on experience, and isn&amp;rsquo;t that a kind of statistical model?
To make matters even worse, natural languages have a statistical nature too &amp;ndash; most of them, if not all, evolve over time, not neccessarily changing the meaning of words, but definitely changing the frequency speakers use them.&lt;/p&gt;
&lt;p&gt;As pointed out by Konstantine Arkoudas&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, GPT-4 can&amp;rsquo;t reason.
I agree with this statement.
I think LLMs are sophisticated statistical models, and the generation process is more like information retrieval but using the neural network weights and in the granularity of tokens.
Also as mentioned by Arkoudas, the lack of reasoning in LLMs has a connection with the hallucination problem.
I agree with him and many other researchers, retrieval-augmentation could serve as the &amp;ldquo;guardrail&amp;rdquo; of LLM generations, but unlikely to be the silver bullet for eliminating the hallucination problem.&lt;/p&gt;
&lt;p&gt;However, &amp;ldquo;can&amp;rsquo;t be solved&amp;rdquo; is different from &amp;ldquo;can&amp;rsquo;t be improved&amp;rdquo;.
Given that more and more studies have shown the vulnerability of LLMs to the statistical nature of their training data, maybe more effort is needed in thinking of a different way of training the model.&lt;/p&gt;
&lt;p&gt;Lastly, it&amp;rsquo;s worth noting that the McKenna et al. work was studied under NLI.
Although the hallucination problem is more prominent in NLG, it&amp;rsquo;s not straightforwad how to do a similar analysis in the NLG scenario.
But if it can be done, it would be more attention catching.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3308558.3313415&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.preprints.org/manuscript/202308.0148/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-4 Can&amp;rsquo;t Reason&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Weakly Supervised Turn-level Engagingness Evaluator for Dialogues</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2023-weakly/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2023-weakly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2022-simple/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2022-simple/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ã€Šæ‰€è°“æƒ…å•†é«˜ï¼Œå°±æ˜¯ä¼šè¯´è¯ã€‹è¯»ä¹¦ç¬”è®°</title>
      <link>https://shaojiejiang.github.io/post/zh/eq-speaking/</link>
      <pubDate>Tue, 30 Jun 2020 14:37:56 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/eq-speaking/</guid>
      <description>&lt;p&gt;è¿™æ˜¯éå¸¸çŸ­çš„ä¸€æœ¬ä¹¦ã€‚ä½œè€…ä¸»è¦åˆ†äº«äº†æœ‰æ±‚äºäººæ—¶çš„7ä¸ªçªç ´å£ï¼Œåˆ†åˆ«æ˜¯ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æŠ•å…¶æ‰€å¥½&lt;/li&gt;
&lt;li&gt;å„†å…¶æ‰€æ¶&lt;/li&gt;
&lt;li&gt;é€‰æ‹©çš„è‡ªç”±&lt;/li&gt;
&lt;li&gt;è¢«è®¤å¯æ¬²&lt;/li&gt;
&lt;li&gt;éä½ ä¸å¯&lt;/li&gt;
&lt;li&gt;å›¢é˜ŸåŒ–&lt;/li&gt;
&lt;li&gt;æ„Ÿè°¢&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;ç»“åˆä¹‹å‰è¯»çš„ã€ŠThe 7 Habits of Highly Effective Peopleã€‹å‘ç°ï¼Œè¿™äº›æŠ€å·§ä¸é«˜æ•ˆèƒ½äººå£«çš„åŠäº‹åŸåˆ™å¹¶ä¸å†²çªï¼Œåè€Œæ˜¯ç›¸äº’è¡¥å……çš„ï¼šåŸåˆ™æ˜¯å¤§æ–¹å‘çš„å¯¼èˆªï¼ŒæŠ€å·§æ˜¯èµ°å¥½æ¯ä¸€æ­¥çš„ä¿éšœã€‚è€Œä¸”ç»å¾—èµ·æ—¶é—´æ£€éªŒçš„æŠ€å·§ä¹Ÿéƒ½æ˜¯ä»¥åŸåˆ™ä¸ºæŒ‡å¯¼çš„ï¼›åªé¡¾çœ¼ä¸‹åˆ©ç›Šçš„æŠ€å·§ï¼Œæ›´åº”è¯¥å«â€œå¥—è·¯â€ï¼Œè€Œè¿™ç”¨ä¸äº†å‡ æ¬¡å°±ä¼šè¢«äººè¯†ç ´ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ä¸Š7ä¸ªæŠ€å·§åˆ†åˆ«å¯¹åº”äº†å“ªé¡¹åšäº‹åŸåˆ™å‘¢ï¼Ÿ&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æŠ•å…¶æ‰€å¥½ã€å„†å…¶æ‰€æ¶ï¼Œæ˜¯â€œåŒèµ¢åŸåˆ™â€çš„å®è·µ&lt;/li&gt;
&lt;li&gt;æä¾›é€‰æ‹©ã€è¢«è®¤å¯æ¬²ã€éä½ ä¸å¯ï¼Œæ˜¯â€œè¦æƒ³è¢«ç†è§£ï¼Œå…ˆè¦ç†è§£å¯¹æ–¹â€åŸåˆ™çš„å®è·µ&lt;/li&gt;
&lt;li&gt;å›¢é˜ŸåŒ–ï¼Œæ˜¯ç†è§£ã€è®¤å¯å¯¹æ–¹çš„ä¸€ç§æ–¹å¼ï¼Œæ›´æ˜¯â€œååŒåŸåˆ™â€çš„ç›´æ¥å®è·µ&lt;/li&gt;
&lt;li&gt;è¡·å¿ƒæ„Ÿè°¢ï¼Œä¹Ÿå±äºå¯¹åˆ«äººä»˜å‡ºçš„ç†è§£å’Œè®¤å¯&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å¦å¤–ä¹¦ä¸­æåˆ°ä¸€ç‚¹â€œè¦æƒ³æŒæ¡â€˜æªè¾èœè°±â€™ï¼Œè¾“å‡ºæ˜¯æœ€ä¾¿æ·çš„é€”å¾„â€ï¼Œæ­£å°è¯äº†æœ¬äºº&lt;a href=&#34;../three-key-elements-of-learning&#34;&gt;å­¦ä¹ çš„ä¸‰è¦ç´ &lt;/a&gt; åšå®¢é‡Œå…³äºâ€œè¾“å‡ºâ€å¯¹äºå­¦ä¹ çš„é‡è¦æ€§ã€‚&lt;/p&gt;
&lt;p&gt;æœ€ååæ§½ä¸‹è¯¥ä¹¦çš„ä¸è¶³ï¼šä½œè€…åŒæ ·&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;ä¸ºå¹¿å‘Šæ–‡æ¡ˆå‘˜ï¼Œå› æ­¤ç¬¬äºŒç« ï¼ˆæœ€åä¸€ç« ï¼‰åŸºæœ¬éƒ½æ˜¯å…³äºæ–‡æ¡ˆå†™ä½œçš„è¯­è¨€å¼ åŠ›çš„ï¼Œä¸æƒ…å•†æ²¡ä»€ä¹ˆå…³è”ï¼Œæœ‰ç‚¹è¢«æ¬ºéª—çš„æ„Ÿè§‰ã€‚ä½œè€…å¯èƒ½çŠ¯äº†æ–‡æ¡ˆå‘˜â€œæ ‡é¢˜å…šâ€çš„èŒä¸šç—…äº†å§ã€‚&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;ç›¸å¯¹äºä¸Šä¸€æœ¬ä¹¦ã€Šå¥½æ–‡æ¡ˆä¸€å¥è¯å°±å¤Ÿäº†ã€‹çš„ä½œè€…&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ã€Šå¥½æ–‡æ¡ˆä¸€å¥è¯å°±å¤Ÿäº†ã€‹è¯»åæ„Ÿ</title>
      <link>https://shaojiejiang.github.io/post/zh/copywriting/</link>
      <pubDate>Mon, 29 Jun 2020 18:23:16 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/copywriting/</guid>
      <description>&lt;p&gt;æœ¬äººè¯»ã€Šå¥½æ–‡æ¡ˆä¸€å¥è¯å°±å¤Ÿäº†ã€‹è¿™æœ¬ä¹¦ï¼ˆä»¥ä¸‹ç»Ÿç§°â€œè¯¥ä¹¦â€ï¼‰ï¼Œä¸»è¦æ˜¯ä¸ºäº†å­¦ä¹ ä¸€äº›æ™®é€‚çš„å†™ä½œæŠ€å·§ï¼Œæ¯•ç«Ÿè™½ç„¶æœ¬äººä¸éœ€è¦å†™å¹¿å‘Šæ–‡æ¡ˆï¼Œä½†è¿˜æ˜¯éœ€è¦å†™æ¨æ–‡ã€åšå®¢ã€æ–‡ç« ç­‰ç­‰ï¼Œå¦‚æœèƒ½ç»ƒå‡ºæ›´æ‰“åŠ¨äººå¿ƒã€æœ‰å¼ åŠ›ã€å¸å¼•ç›®å…‰çš„å†™ä½œé£æ ¼ï¼Œåˆä½•ä¹ä¸ä¸ºå‘¢ï¼Ÿ&lt;/p&gt;
&lt;p&gt;è¯¥ä¹¦æ€»ç»“çš„å¹¿å‘Šæ–‡æ¡ˆçš„ä¸‰å¤§åŸºæœ¬åŸåˆ™ï¼Œæˆ‘è®¤ä¸ºå¾ˆæ˜¯ç²¾ç‚¼ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;è®©å—ä¼—è§‰å¾—ä¿¡æ¯ä¸è‡ªå·±æœ‰å…³&lt;/li&gt;
&lt;li&gt;è¯­è¨€è¡¨è¾¾è¦æœ‰å¼ åŠ›&lt;/li&gt;
&lt;li&gt;å‹¾èµ·è¯»è€…çš„ç–‘æƒ‘ç­‰é˜…è¯»å…´è¶£&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;éšåè¯¥ä¹¦è¿˜å°†ä¸‰å¤§åŸåˆ™å±•å¼€ï¼Œå¹¶é€šè¿‡å®ä¾‹åˆ†äº«äº†å…±77æ¡æ–‡æ¡ˆå†™ä½œæŠ€å·§ã€‚ç°å°†è¿™äº›æŠ€å·§çš„æ ¸å¿ƒå†…å®¹ï¼Œæ ¹æ®&lt;strong&gt;æœ¬äººçš„å–œå¥½ä»¥åŠç†è§£&lt;/strong&gt;è¿›è¡Œæ€»ç»“ã€‚&lt;/p&gt;
&lt;h2 id=&#34;è®©å—ä¼—è§‰å¾—ä¿¡æ¯ä¸è‡ªå·±æœ‰å…³&#34;&gt;è®©å—ä¼—è§‰å¾—ä¿¡æ¯ä¸è‡ªå·±æœ‰å…³&lt;/h2&gt;
&lt;p&gt;é¦–å…ˆåº”è¯¥æ˜ç¡®å—ä¼—ï¼Œä¸å…¶çŸ³æ²‰äººæµ·ã€æ³¢æ¾œä¸æƒŠï¼Œä¸å¦‚ç¼©å°èŒƒå›´ã€æŠŠè¯è¯´ç»™æ‡‚çš„äººå¬ã€‚ä¸ºè¾¾åˆ°è¿™æ ·çš„ç›®çš„ï¼Œé€šå¸¸å¯ä»¥åŠ ä¸Šäººç§°ä»£è¯ï¼ˆå¦‚æœåŸå…ˆæ²¡æœ‰ï¼‰ï¼Œæˆ–è€…åŠ ä¸Šé™å®šè¯å¯¹èŒƒå›´è¿›è¡Œæ˜ç¡®ç•Œå®šã€‚&lt;/p&gt;
&lt;h2 id=&#34;è¯­è¨€è¡¨è¾¾è¦æœ‰å¼ åŠ›&#34;&gt;è¯­è¨€è¡¨è¾¾è¦æœ‰å¼ åŠ›&lt;/h2&gt;
&lt;p&gt;è¯¥ä¹¦å…³äºè¯­è¨€å¼ åŠ›çš„æŠ€å·§æœ‰å¾ˆå¤šï¼Œä¸»è¦å¯ä»¥æ¦‚æ‹¬ä¸ºï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ç®€æ´å‡ç‚¼ã€å…·ä½“å½¢è±¡&lt;/li&gt;
&lt;li&gt;æ„Ÿæƒ…çœŸæŒšã€è¯´å‡ºå¿ƒå£°ã€å¹³æ˜“è¿‘äºº&lt;/li&gt;
&lt;li&gt;çºµè§ˆå…¨å±€ï¼ˆè¿‡ç¨‹æ¦‚è§ˆã€æ‰€éœ€æ—¶é—´ã€ç»“æœå±•æœ›ç­‰ï¼‰&lt;/li&gt;
&lt;li&gt;è¡¨è¾¾æŠ€å·§ï¼ˆå¼ºè°ƒè¯­æ°”ã€åˆ©ç”¨æ ¼å¼èŠ‚å¥ã€åˆ¶é€ å†²çªã€å¼•ç”¨åè¨€ï¼‰&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;å‹¾èµ·è¯»è€…çš„ç–‘æƒ‘ç­‰é˜…è¯»å…´è¶£&#34;&gt;å‹¾èµ·è¯»è€…çš„ç–‘æƒ‘ç­‰é˜…è¯»å…´è¶£&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;ç•™ç™½æˆ–è€…è®¾é—®&lt;/li&gt;
&lt;li&gt;åˆ›é€ å¸ç›æ–°è¯&lt;/li&gt;
&lt;li&gt;æœ‰æ•…äº‹æ€§&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;æ€»ç»“&#34;&gt;æ€»ç»“&lt;/h2&gt;
&lt;p&gt;å…¶å®å†™åˆ°è¿™é‡Œï¼Œé€šè¿‡å¯¹å…¨ä¹¦å†…å®¹è¿›è¡Œå›é¡¾ï¼Œæˆ‘å‘ç°è¯¥ä¹¦å¹¶æ²¡æœ‰ç»™æˆ‘å¤ªå¤šçš„æ–°ä¿¡æ¯ã€‚æˆ‘ä¾ç„¶è®¤ä¸ºè¦å†™å¥½å„ç§å½¢å¼çš„æ–‡å­—ï¼Œåªéœ€è¦åšåˆ°å››ç‚¹å°±å¯ä»¥äº†ï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æ˜ç¡®è‡ªå·±æƒ³è¦è¡¨è¾¾ä»€ä¹ˆ&lt;/li&gt;
&lt;li&gt;ä¾æ®ç»éªŒé€‰æ‹©æœ€æœ‰åŠ›çš„è¡¨è¾¾æ–¹å¼ï¼ˆè¿™ä¸€ç‚¹éœ€è¦å¤§é‡ç§¯ç´¯ï¼‰&lt;/li&gt;
&lt;li&gt;ç«™åœ¨è¯»è€…è§’åº¦æ€è€ƒä¿¡æ¯èƒ½å¦æµç•…ä¼ è¾¾&lt;/li&gt;
&lt;li&gt;ä»¥ä¸Šæ­¥éª¤åå¤è¿­ä»£ä¼˜åŒ–&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è¯¥ä¹¦çš„ä¸»è¦ä½œç”¨ï¼Œä¹Ÿè®¸å°±æ˜¯åƒä½œè€…å‘¼åçš„é‚£æ ·â€œæ”¾åœ¨å…¬å¸çš„æ¡Œä¸Šï¼Œå½“æˆå­—å…¸æ¥ä½¿ç”¨â€å§ã€‚
é¡ºå¸¦æä¸€ä¸‹æœ¬äººå¯¹è¯¥ä¹¦ä¸å¤ªè®¤åŒçš„ä¸€ç‚¹ï¼Œå°±æ˜¯æŸäº›æŠ€å·§è¿‡äºå¼ºè°ƒâ€œå¸ç›â€äº†ï¼Œä»¥è‡³äºä½œè€…æœ¬äººéƒ½å¤šæ¬¡æé†’è¯»è€…â€œä½œä¸ºæ–‡æ¡ˆå‘˜å¯ä»¥å¦‚æ­¤å†™ï¼Œè€Œä½œä¸ºæ¶ˆè´¹è€…è¦ä¿æŒå¤´è„‘æ¸…é†’ï¼Œä¸è¦è½»æ˜“è¢«å¹¿å‘Šæ–‡æ¡ˆè¯±æƒ‘â€ã€‚
è¿™ä¹Ÿä»ä¾§é¢è¯´æ˜äº†ä»¥ä¸Šâ€œæŠ€å·§â€ä¸­ï¼Œæœ€æ™®é€‚ä¹Ÿæœ€ä¸åƒâ€œæŠ€å·§â€&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;çš„ä¸€ç‚¹ï¼Œå°±æ˜¯è¯´è¯è¦å‘è‡ªå†…å¿ƒï¼Œé å¥—è·¯å¾—æ¥çš„äººå¿ƒåªèƒ½æ˜¯æš‚æ—¶çš„ï¼Œè€Œä¸”è¿˜æœ‰è¢«çœ‹ç©¿çš„å±é™©ã€‚&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;ä¸ªäººç†è§£æ‰€è°“æŠ€å·§ï¼Œæ˜¯æŒ‡å¯ä»¥ç®€å•åœ°æŒæ¡ï¼Œå¹¶ä¸”åº”ç”¨èµ·æ¥è§æ•ˆå¾ˆå¿«ã€‚ç„¶è€ŒçœŸæŒšçš„è®²è¯ï¼Œå¯¹äºè¯´æƒ¯äº†å®¢å¥—è¯çš„äººæ˜¯éœ€è¦ä¸€å®šç»ƒä¹ çš„ï¼Œå¹¶ä¸”è§æ•ˆå¯èƒ½æ²¡é‚£ä¹ˆå¿«ã€‚ç”¨ç§‘ç»´çš„è§‚ç‚¹æ¥è¯´ï¼Œâ€œæ„Ÿæƒ…çœŸæŒšâ€æ›´åº”è¯¥ç®—æ˜¯ä¸€é¡¹åŸåˆ™è€ŒéæŠ€å·§ã€‚&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Transformer Align Model</title>
      <link>https://shaojiejiang.github.io/post/en/transformer-align-model/</link>
      <pubDate>Sat, 16 May 2020 16:40:07 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/transformer-align-model/</guid>
      <description>&lt;p&gt;In this paper&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, transformer is trained to perform both translation and alignment tasks.&lt;/p&gt;
&lt;h2 id=&#34;application-scenarios-of-word-alignments-in-nmt&#34;&gt;Application scenarios of word alignments in NMT&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generating bilingual lexica from parallel corpora&lt;/li&gt;
&lt;li&gt;External dictionary assisted translation to improve translation of low frequency words&lt;/li&gt;
&lt;li&gt;Trust, explanation, error analysis&lt;/li&gt;
&lt;li&gt;Preserving style on webpages&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-design&#34;&gt;Model design&lt;/h2&gt;
&lt;p&gt;The attention mechanism has long been motivated by word alignments in statistical machine translation, but ensure the alignment quality, additional supervision is needed.&lt;/p&gt;
&lt;p&gt;There is a tendency that the attention probabilities from the penultimate layer of a normally trained transformer MT model corresponds to word alignments.
Therefore, one attention head (clever!) in the penultimate layer is trained as the alignment head.
The motivation of selecting only one attention head for alignment is to give the freedom to the model of choosing whether to rely more on the alignment or other attention heads.&lt;/p&gt;
&lt;!-- While in Beamer alignment, the freedom is fully preserved in the attention layer, and the alignment is used for RNN hidden states. --&gt;
&lt;h2 id=&#34;how-two-train-the-alignment-head&#34;&gt;How two train the alignment head&lt;/h2&gt;
&lt;p&gt;There are two approaches existing in the literature:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Label alignments beforehand and train the attention weights through KL-divergence.&lt;/li&gt;
&lt;li&gt;Use the attentional vector to also predict either the target word or the properties such as POS tags of the target tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this work, an unsupervised training approach is used to train the alignment head.
An alignment model is first trained on translation, then the penultimate layer attention weights are averaged and used as weak alignment supervision for a translation (and alignment) model.
The alignment model is trained in both directions.&lt;/p&gt;
&lt;p&gt;Previous work reported performance gain by introducing alignment supervision.
In this paper, however, alignment performances are good, but translation results are moderate.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.02074&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jointly Learning to Align and Translate with Transformer Models&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Compressive Transformers</title>
      <link>https://shaojiejiang.github.io/post/en/compressive-transformers/</link>
      <pubDate>Tue, 12 May 2020 14:29:44 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/compressive-transformers/</guid>
      <description>&lt;p&gt;Built on top of Transformer-XL, Compressive Transformer&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; condenses old memories (hidden states) and stores them in the compressed memory buffer, before completely discarding them.
This model is suitable for long-range sequence learning but may cause too much computational burden for tasks that only have short sequences.
Compressive Transformers can also be used as memory components in conjunction with other models.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In the beginning, the authors draw the connection between their work and human brains by mentioning that humans memorize things via lossy compression.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We aggressively select, filter, or integrate input stimuli based on factors of surprise, perceived danger, or repetition &amp;ndash; amongst other signals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It&amp;rsquo;s often, if not always, good to see such insights of how AI works are inspired by humans.
It&amp;rsquo;s also good to see that they relate their work to previous works, i.e. RNNs, transformers and sparse attention.&lt;/p&gt;
&lt;p&gt;An RNN compresses previous memories into a fixed size hidden vector, which is space-efficient, but also results in its temporal nature and hence difficult to parallelize.
Transformers, on the other hand, store all the past memories uncompressed, which can be beneficial for achieving better performances such as precision, BLEU, perplexity, etc, but it costs more and more computation and memory space with the sequence length growing.
Sparse attention can be used to reduce computation, while the spatial cost remains the same.&lt;/p&gt;
&lt;h2 id=&#34;model-design-and-training&#34;&gt;Model design and training&lt;/h2&gt;
&lt;p&gt;The proposed Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, trained to query both its short-term granular memory and longer-term coarse memory.&lt;/p&gt;
&lt;p&gt;If trained using original task-relevant loss only, it requires backpropagating-through-time (BPTT) over long unrolls for very old memories.
A better solution is to use local auxiliary losses by stopping gradients and reconstructing either the original memory vectors (lossless objective) or attention vectors (lossy objective; reportedly to work better).
The second choice for the auxiliary loss, in other words, means that we don&amp;rsquo;t care whether the original memory can be reconstructed or not, as long as the attention vector can be reconstructed, given the same query (brilliant!).&lt;/p&gt;
&lt;h3 id=&#34;some-practical-concerns&#34;&gt;Some practical concerns&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The auxiliary loss is only used to train the compression module, as it harms the learning when the gradients flow back to the main network.
This might also explain why I couldn&amp;rsquo;t reproduce &lt;a href=&#34;../adaptive-computation-time&#34;&gt;ACT&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;Batch accumulation (4x bigger batch size) is used for better performance.
It is observed in some works that bigger batch sizes lead to better generalization, but some other works found the opposite to be true (discussed in the papers and talks mentioned &lt;a href=&#34;../visualizing-loss&#34;&gt;in my other post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Model optimization is very sensitive to gradient scales, so the gradient norms are clipped to 0.1 for stable results.
This is typical for transformer variants.&lt;/li&gt;
&lt;li&gt;Convolution works best for memory compression.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;further-thoughsquestions&#34;&gt;Further thoughs/questions:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Compressive Transformer improves the modeling of rare words.
But why?&lt;/li&gt;
&lt;li&gt;In the discussion section, the authors pointed out that future directions could include the investigation of adaptive compression rates by layer, the use of long-range shallow memory layers together with deep short-range memory, and even the use of RNNs as compressors.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compressive Transformers for Long-Range Sequence Modelling&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing the Loss Landscape of Neural Nets</title>
      <link>https://shaojiejiang.github.io/post/en/visualizing-loss/</link>
      <pubDate>Wed, 06 May 2020 10:13:43 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/visualizing-loss/</guid>
      <description>&lt;p&gt;Here are some notes take while reading the NeurlIPS 2018 paper &lt;a href=&#34;http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt;.
This work helps explain why some models are easier to train/generalize than others.
The above image is a good illustration: with a much smoother loss landscape, DenseNet with 121 layers is much easier to train than a ResNet-110 without skip connections, and generalizes better in the mean time.&lt;/p&gt;
&lt;p&gt;The traditional way of visualizing loss functions of neural models in 2D contour plots is by choosing a center point $\theta^*$ (normally the converged model parameters), two random direction vectors $\delta$ and $\eta$, then plot the function:
$$f(\alpha, \beta) = L(\theta^* + \alpha \delta + \beta \eta)$$
Batch norm parameters are unchanged.&lt;/p&gt;
&lt;p&gt;The above method fails to capture the intrinsic geometry of loss surfaces, and cannot be used to compare the geometry of two different minimizers or two different networks.
This is because of the &lt;em&gt;scale invariance&lt;/em&gt; in network weights (this statement only applies to rectified networks as per the paper).
To tackle this, the authors normalize each filter in a direction vector $d$ ($\delta$ or $\eta$) to have the same norm of the corresponding filter in $\theta$:
$$d_{i, j} \leftarrow \frac{d_{i, j}}{||d_{i, j}||} ||\theta_{i, j} ||.$$
$i$ is the layer number and $j$ the filter number.
With the proposed filter-wise normalized direction vectors, the authors found that the sharpness of local minima correlates well with generalization error, even better than layer-wise normalization (for direction vectors).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why flat minima:&lt;/strong&gt; In a recent talk&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, Tom Goldstein (the last author) pointed out that flat minima correspond to large margin classifiers, which is more tolerant to domain shifts of data, thus having better generalization ability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Known influential factors:&lt;/strong&gt;
Small-batch training results in flat minima, while large-batch training results in sharp minima.
Increased width prevents chaotic behavior, and skip connections dramatically widen minimizers (see figure in the beginning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting with precaution:&lt;/strong&gt;
The loss surface is viewed under a dramatic dimensionality reduction.
According to the authors&amp;rsquo; analysis, if non-convexity is present in the dimensionality reduced plot, then non-convexity must be present in the full-dimensional surface as well.
However, apparent convexity in the low-dimensional surface does not mean the high-dimensional function is truly convex. Rather it means that the positive curvatures are dominant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In a nutshell:&lt;/strong&gt; It&amp;rsquo;s a great work trying to visualize the mystery of what&amp;rsquo;s going well/bad when training a neural model.
Although claiming the study to be empirical, I personally found their experiments and results very convincing.
Appendix B about visualizing optimization paths is also very insightful, and the authors probably also thought so, so they decided to move it as a main section in their latest &lt;a href=&#34;https://arxiv.org/pdf/1712.09913.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arxiv version&lt;/a&gt; ğŸ˜„!&lt;/p&gt;
&lt;p&gt;Further thoughts/questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Has it been done for visualizing NLP models?&lt;/li&gt;
&lt;li&gt;Is it more appropriate to visualize loss for NLG or other measures?
This might depend on how to define &amp;ldquo;labels&amp;rdquo; in NLG tasks.&lt;/li&gt;
&lt;li&gt;How big a convolution filter normally is?&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s similar between RNN and skip connections?&lt;/li&gt;
&lt;li&gt;This work can be used together with automatic neural architecture search, but is there any other more efficient way of getting better models?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://iclr2020deepdiffeq.rice.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalization in neural nets:  a perspective from science (not math)&lt;/a&gt; Starting at 1:54:00 in the video.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>å­¦ä¹ çš„ä¸‰è¦ç´ </title>
      <link>https://shaojiejiang.github.io/post/zh/three-key-elements-of-learning/</link>
      <pubDate>Thu, 30 Apr 2020 09:12:07 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/three-key-elements-of-learning/</guid>
      <description>&lt;p&gt;å‰æ³¨ï¼šå¦‚æ— ç‰¹æ®Šè¯´æ˜ï¼Œæœ¬æ–‡çš„æ‘˜å½•å‡å–è‡ª&amp;quot;æ‘„å½±çš„è‰ºæœ¯ (ä¸–ç•Œé¡¶çº§æ‘„å½±å¤§å¸ˆ)&amp;quot; by Bruce Barnbaum, æ¨Šæ™ºæ¯…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;è‡ªæˆ‘å®¡é—®è¦æœ‰ä¸€ä¸ªåˆç†çš„æé™ã€‚åœ¨å› ä¸ºè¿‡äºè‡ªçœè€Œæ„Ÿåˆ°ç„¦è™‘ä¹‹å‰ï¼Œä½ åº”è¯¥é€šè¿‡æ‹æ‘„ä¸€äº›ç…§ç‰‡æ¥å¯¹å¤–äº¤æµã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;åœ¨è¯»åˆ°è¿™æ®µè¯çš„æ—¶å€™ï¼Œæˆ‘æƒ³èµ·äº†æœºå™¨å­¦ä¹ å¯¹æˆ‘ä¸ªäººå­¦ä¹ æ–¹å¼çš„ä¸€äº›å¯å‘ï¼ŒåŒæ—¶ä¹Ÿæ˜¯æˆ‘åšå£«å¯¼å¸ˆMaartenå¯¹æˆ‘çš„æ•™å¯¼ï¼šè¡¨è¿°ï¼ˆæ¯”å¦‚æ¼”è®²ã€æŠ¥å‘Šã€äº¤æµå’Œå†™ä½œç­‰ï¼‰æ˜¯å­¦ä¹ çš„ä¸€ä¸ªé‡è¦ç¯èŠ‚ã€‚
å­”å­è¯´ï¼š&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å­¦è€Œä¸æ€åˆ™ç½”ï¼Œæ€è€Œä¸å­¦åˆ™è´»ã€‚&amp;rdquo; â€”â€”è®ºè¯­Â·ä¸ºæ”¿&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ä½†æˆ‘è®¤ä¸ºï¼Œå¦‚æœæ²¡æœ‰ä¸€ä¸ªè¡¨è¿°çš„è¿‡ç¨‹ï¼Œâ€œå­¦â€å’Œâ€œæ€â€å¾ˆæœ‰å¯èƒ½ä¼šå˜æˆç©ºä¸­æ¥¼é˜ã€‚
åªæœ‰é€šè¿‡å°†è‡ªå·±çš„è§‚ç‚¹è¡¨è¾¾å‡ºæ¥ã€æ”¾åˆ°ç°å®ä¸­è®©å®ƒä»¬å»ç»å—æ—¶é—´çš„è€ƒéªŒã€ç»å—åˆ«äººçš„æ‰¹è¯„ï¼Œæ‰èƒ½çœŸæ­£åœ°å°†è‡ªå·±çš„æ‰€å­¦æ‰€æ€è½åˆ°å®åœ°ã€‚
å› æ­¤æˆ‘è®¤ä¸ºï¼Œå­¦ä¹ çš„ä¸‰è¦ç´ å¯ä»¥å½’çº³ä¸ºï¼šå­¦ã€æ€å’Œè¿°ã€‚&lt;/p&gt;
&lt;h2 id=&#34;å‘æœºå™¨å­¦ä¹ å­¦ä¹ &#34;&gt;å‘â€œæœºå™¨å­¦ä¹ â€å­¦ä¹ &lt;/h2&gt;
&lt;p&gt;è¿‘æ¥éšç€å¯¹â€œæœºå™¨å­¦ä¹ â€æ€è€ƒçš„æ·±å…¥ï¼Œæˆ‘é€æ¸ä»ä¸­å¾—åˆ°ä¸€äº›å¯å‘ï¼Œæ¯”å¦‚æˆ‘å¼€å§‹é‡è§†é˜…è¯»å°±æ˜¯å› ä¸ºè®¤è¯†åˆ°äº†å¤§æ•°æ®é‡å¯¹è®­ç»ƒæœºå™¨å­¦ä¹ ç®—æ³•çš„é‡è¦æ€§ï¼šå¦‚æœç®—æ³•éœ€è¦è¾“å…¥å¤§é‡çš„æ•°æ®ä»¥åŠåå¤åœ°å­¦ä¹ æ‰èƒ½å¾—åˆ°ç†æƒ³çš„æ€§èƒ½ï¼Œé‚£ä¹ˆæˆ‘æ˜¯ä¸æ˜¯ä¹Ÿåº”è¯¥è¿™ä¹ˆåšå‘¢ï¼Ÿ
é˜…è¯»å°±æ˜¯äººç±»å­¦ä¹ çš„ä¸€ä¸ªé‡è¦â€œè¾“å…¥â€æ–¹å¼ã€‚
å®ƒè®©å‰äººæ€»ç»“å‡ºçš„æ€æƒ³ç²¾åï¼Œç©¿è¶Šäº†æ—¶ç©ºçš„é™åˆ¶ä¸æ–­åœ°è¾“å…¥åˆ°è¯»è€…çš„æ€æƒ³ä¸­ã€‚
å½“ç„¶ï¼Œå…¶ä»–æ›´åŠ å®æ—¶å®åœ°çš„è¾“å…¥æ–¹å¼ä¹Ÿæ˜¯ä¸å¯æˆ–ç¼ºçš„ï¼Œæ¯”å¦‚å‚åŠ åˆ«äººçš„æ¼”è®²å’ŒæŠ¥å‘Šã€‚
æˆ‘å¸¸å¸¸è‡ªå˜²ï¼šæœºå™¨å­¦ä¹ é¢†åŸŸçš„å¤§å¸ˆä»¬æŠŠå·¥ä½œå’Œç”Ÿæ´»ä¸­æ€»ç»“å‡ºæ¥çš„å“²ç†åº”ç”¨åˆ°æœºå™¨å­¦ä¹ ç®—æ³•ä¸Šï¼Œè€Œæˆ‘è¿™æ ·çš„æ— åä¹‹è¾ˆä»ä»–ä»¬çš„å·¥ä½œä¸­éƒ½èƒ½å­¦åˆ°å—ä¹‹ä¸å°½çš„å“²ç†ã€‚
å°ç¥ä¹‹è·¯ä»»é‡è€Œé“è¿œå•Šï¼Œå“ˆå“ˆï¼&lt;/p&gt;
&lt;p&gt;æœ€è¿‘å¯¹æœºå™¨å­¦ä¹ æœ‰äº†æ›´åŠ æ·±å…¥çš„ç†è§£ï¼Œè®¤è¯†åˆ°â€œè¾“å…¥â€ã€â€œå¤„ç†â€å’Œâ€œè¾“å‡ºæ£€éªŒâ€è¿™ä¸‰ä¸ªç¯èŠ‚ï¼Œç¼ºä¸€ä¸å¯ã€‚
è€Œåæ€ä¸‹æˆ‘è‡ªå·±ï¼Œè¾“å…¥ï¼ˆé˜…è¯»ï¼‰å’Œå¤„ç†ï¼ˆæ€è€ƒï¼‰æ­£åœ¨ç¨³æ­¥è¿›è¡Œï¼Œä½†è¾“å‡ºï¼ˆè¡¨è¿°ï¼‰å´è¿˜åšå¾—è¿œè¿œä¸å¤Ÿå•Šã€‚
è¿™ä¹Ÿæ˜¯æˆ‘æœ€è¿‘å†³å®šåŸ¹å…»è®°ç¬”è®°ä¹ æƒ¯çš„ä¸€ä¸ªé‡è¦åŸå› ã€‚
é‚£ä¹ˆåœ¨æˆ‘ä¸šä½™çˆ±å¥½çš„æ‘„å½±é¢†åŸŸï¼Œâ€œè¾“å‡ºâ€å°±ä¸æ˜¯è®°ç¬”è®°æˆ–è·Ÿåˆ«äººè¯­è¨€äº¤æµé‚£ä¹ˆç®€å•äº†ï¼Œè€Œæ›´å¤šåœ°åº”è¯¥æ˜¯é€šè¿‡æ‹æ‘„ç…§ç‰‡æ¥è¡¨è¿°è‡ªå·±ï¼š&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æˆåŠŸåœ°è¡¨è¾¾ä½ çš„ä¿¡æ¯ï¼Œæ˜¯åˆ›æ„æ‘„å½±çš„æ ¹æœ¬ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;å…³äºè¡¨è¿°&#34;&gt;å…³äºè¡¨è¿°&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å•çº¯åœ°å‘åˆ«äººæ±‡æŠ¥ä½ çœ‹åˆ°çš„åœºæ™¯ï¼Œé‚£æ˜¯é€ƒé¿è´£ä»»ï¼›æŠŠåœºæ™¯æ¼”ç»å‡ºæ¥ï¼Œæ‰æ˜¯æ¥å—æŒ‘æˆ˜ã€‚è™½ç„¶åœºæ™¯å¯èƒ½ä¸æ˜¯ä½ åˆ›é€ å‡ºæ¥çš„ï¼Œä½†ç…§ç‰‡å´ä¸€å®šæ˜¯ï¼å› æ­¤ï¼Œä¸è¦æ­¢æ­¥äºä½ çš„æ‰€è§ï¼ŒåŠ å…¥ä½ çš„è¯„è®ºã€æ„Ÿå—å’Œå»ºè®®ï¼ŒæŠŠå®ƒä»¬éƒ½æ”¾åˆ°ç…§ç‰‡ä¸­å§ï¼Œè¡¨è¾¾ä½ çš„è§‚ç‚¹ï¼Œé˜æ˜ä½ çš„ç«‹åœºï¼Œè®©è¯»è€…ä¿¡æœä½ çš„ç»“è®ºã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿™æ®µè¯æ˜¯åœ¨è®²è‰ºæœ¯æ‘„å½±çš„è¡¨è¾¾æ–¹å¼ï¼šä»è‡ªå·±çœ‹åˆ°çš„åœºæ™¯ï¼ˆè¾“å…¥ï¼‰ä¸­æç‚¼å‡ºè‡ªå·±çš„è§‚ç‚¹ï¼ˆå¤„ç†ï¼‰ï¼Œå¹¶æŠŠè¿™äº›è§‚ç‚¹è¡¨ç°åœ¨è‡ªå·±çš„ä½œå“ä¸­ï¼ˆè¾“å‡ºï¼‰ã€‚
åŒæ—¶è¿™æ®µè¯ä¹Ÿé˜è¿°äº†è¡¨è¾¾æ–¹å¼çš„ä¸‰ä¸ªå±‚æ¬¡ï¼šä¸åŠ æ€ç´¢æ‹å‡ºçš„å¿«ç…§æ˜¯æœ€åº•å±‚çš„ã€ç®€å•çš„è®°å½•ï¼Œèå…¥äº†è‡ªå·±è§‚ç‚¹æˆ–è§†è§’çš„ä½œå“æ˜¯ä¸­å±‚çš„ï¼Œèƒ½å¤Ÿè®©è§‚ä¼—æ˜ç™½ä½ ä¼ è¾¾å‡ºçš„è§‚ç‚¹æ‰æ˜¯æœ€é«˜å±‚çš„ã€‚
é‚£ä¹ˆå†™ä½œåˆä½•å°ä¸æ˜¯å¦‚æ­¤å‘¢ï¼Ÿ
ç®€å•çš„è®°å™æ˜¯æœ€åº•å±‚çš„ï¼Œå°±ç®—å¤šç”¨äº›åä¸½çš„è¯è—»ï¼ˆå¯¹åº”åˆ°æ‘„å½±ä¸Šå°±æ˜¯å¥—ç”¨æ„å›¾çš„èŒƒå¼ã€ï¼‰ï¼Œä¹Ÿè¿˜æ˜¯ä¸€ç¯‡æ²¡æœ‰æ€æƒ³çš„æ–‡å­—ï¼›è¡¨è¾¾äº†è‡ªå·±æ€æƒ³çš„æ–‡å­—æ˜¯å‹‡æ•¢çš„ï¼Œè‡³äºè§‚ç‚¹èƒ½ä¸èƒ½è¢«è¯»è€…æ¥å—ï¼Œæ˜¯ä¸€ä¸ªéœ€è¦ä¸æ–­åæ€çš„è¿‡ç¨‹ï¼šæ˜¯è‡ªå·±çš„è¡¨è¾¾æ–¹å¼ä¸å¤Ÿæœ‰è¯´æœåŠ›ï¼Œè¿˜æ˜¯æƒ³æ³•å¤ªè¶…å‰è€Œä¸ä¸ºä¸–äººæ¥å—ï¼Ÿ&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;äº†è§£è‡ªå·±è¦è¯´ä»€ä¹ˆï¼ äº†è§£è‡ªå·±è¦æ€ä¹ˆè¯´ï¼ ç„¶åæ¯«ä¸å¦¥ååœ°è¯´å‡ºæ¥&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;æœ‰æ„æ€çš„æ˜¯ï¼Œè¿™ä¹Ÿæ­£æ˜¯æˆ‘è‡ªå·±æœ€è¿‘å…³äºå†™ä½œçš„å¿ƒå¾—ï¼Œå°¤å…¶æ˜¯åœ¨æŒ‡å¯¼ç ”ç©¶ç”Ÿæ¯•ä¸šè®ºæ–‡çš„æ—¶å€™ï¼Œæˆ‘å¯¹ä»–ä»¬è¯´å‡ºäº†å‡ ä¹åŒæ ·è¯ï¼Œåªä¸è¿‡æˆ‘ç¬¬ä¸‰éƒ¨åˆ†çš„è§‚ç‚¹è¦æ›´æ‰¹åˆ¤æ€§ä¸€äº›ï¼šç«™åœ¨è¯»è€…çš„å®¢è§‚è§’åº¦å»å®¡è§†è‡ªå·±æœ‰æ²¡æœ‰å¾ˆå¥½åœ°ä¼ è¾¾è§‚ç‚¹ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ä¸Šä¸€ç›´åœ¨è¯´â€œæœºå™¨å­¦ä¹ â€å’Œâ€œé˜…è¯»â€å¯¹æˆ‘å­¦ä¹ ã€æ‘„å½±åˆ›ä½œçš„å½±å“ä»¥åŠä»–ä»¬ä¹‹å‰çš„å…±æ€§ã€‚
åæ€æœºå™¨å­¦ä¹ ï¼Œå…¶å®ä¸Šé¢æåˆ°çš„è¡¨è¾¾ä¸‰å±‚æ¬¡å¯¹æœºå™¨å­¦ä¹ çš„ç ”ç©¶ä¹Ÿæ˜¯å…·æœ‰æŒ‡å¯¼æ„ä¹‰çš„ï¼šç®€å•çš„è®°è¿°ï¼ˆautoencoderï¼‰æ˜¯æœ€ä½å±‚æ¬¡ï¼›å…·æœ‰åˆ›ä½œæ€§çš„è¡¨è¾¾æ–¹å¼ï¼ˆå¯¹è¯ã€æ‘˜è¦ã€ç¿»è¯‘ç­‰ç­‰ï¼‰æ˜¯ä¸­å±‚çš„ï¼›è€Œå¦‚ä½•è®©æœºå™¨çš„è¡¨è¾¾æ›´ä¸ºäººç±»æ‰€æ¥å—ï¼ˆæ˜¯å¦åˆç†ã€è¿è´¯ã€æœ‰è¶£ï¼‰ï¼Œæ­£æ˜¯ç°åœ¨é¢†åŸŸå†…çš„ç ”ç©¶éš¾ç‚¹ã€‚
è¿™ä¹Ÿè®¸å¯¹æœºå™¨å­¦ä¹ çš„å¤šä»»åŠ¡è®­ç»ƒæœ‰äº›ä»·å€¼ï¼Ÿ&lt;/p&gt;
&lt;h2 id=&#34;å…³äºæ‘„å½±çš„çœŸå®&#34;&gt;å…³äºæ‘„å½±çš„â€œçœŸå®â€&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æˆ‘è®¤ä¸ºå¤§éƒ¨åˆ†è‰ºæœ¯å®¶æ‰€ä¸»è¦è¿½å¯»çš„ä¸æ˜¯çœŸå®ï¼Œè€Œæ˜¯ä¸€ç§æ°å½“çš„æ–¹å¼ï¼Œä»¥è¡¨è¾¾ä»–ä»¬æ‰€ç†è§£çš„çœŸå®ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;äººéƒ½æ˜¯æœ‰æ€æƒ³çš„ï¼Œæœ‰æ€æƒ³å°±ä¸€å®šä¼šæœ‰ä¸»è§‚ï¼Œé‚£ä¹ˆä¸€ä¸ªä½œè€…åŸºäºç°å®äº‹ç‰©çš„åˆ›ä½œï¼Œæ— è®ºæ˜¯æ‘„å½±è¿˜æ˜¯æ–‡å­¦ï¼Œéƒ½ä¸€å®šæ˜¯ä¸»è§‚çš„ã€‚
å°±ç®—ä½ èƒ½å¤Ÿä¿è¯è‡ªå·±ä½œå“çš„ç»å¯¹å®¢è§‚å’ŒçœŸå®ï¼Œä½ ä¹Ÿæ²¡æœ‰åŠæ³•ä¿è¯è¯»è€…å¸¦ç€ä¸»è§‚æƒ…ç»ªæ¥å®¡è§†ä½ çš„ä½œå“ï¼Œé‚£ä¹ˆåœ¨ä»–ä»¬çœ¼é‡Œï¼Œä½ è¿˜æ˜¯ä¸»è§‚çš„ã€‚
æ˜¯ä¸æ˜¯å¾ˆæœ‰äº›ç›¸å¯¹è®ºçš„æ„æ€ï¼Œå“ˆå“ˆã€‚&lt;/p&gt;
&lt;p&gt;çœŸæ­£çš„å®¢è§‚å’ŒçœŸå®æ˜¯ä¸å­˜åœ¨çš„ï¼Œä¹Ÿä¸åº”è¯¥ä½œä¸ºç»ˆæè¿½æ±‚ã€‚
æ­£å¦‚æˆ‘ä¸Šé¢æ‰€è¯´çš„å¯¹è¡¨è¾¾æ–¹å¼çš„åæ€ï¼Œåº”è¯¥åœ¨è‡ªå·±çš„è¡¨è¾¾å’Œåˆ«äººçš„æ¥å—åº¦ä¸Šå–ä¸€ä¸ªåˆç†çš„å¹³è¡¡ï¼Œè¿™æ‰æ˜¯åˆ›ä½œçš„ç²¾é«“å§ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>æˆ‘éœ€è¦æ˜ç¡®è‡ªå·±çš„æ‘„å½±ä¸»é¢˜</title>
      <link>https://shaojiejiang.github.io/post/zh/photography-subject/</link>
      <pubDate>Wed, 29 Apr 2020 14:17:37 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/photography-subject/</guid>
      <description>&lt;p&gt;å‰æ³¨ï¼šå¦‚æ— ç‰¹æ®Šè¯´æ˜ï¼Œæœ¬æ–‡çš„æ‘˜å½•å‡å–è‡ª&amp;quot;æ‘„å½±çš„è‰ºæœ¯ (ä¸–ç•Œé¡¶çº§æ‘„å½±å¤§å¸ˆ)&amp;quot; by Bruce Barnbaum, æ¨Šæ™ºæ¯…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä½ çš„å…´è¶£æ˜¯ä»€ä¹ˆï¼Ÿåªæœ‰ä½ è‡ªå·±å¯ä»¥å›ç­”ã€‚ä½†è¿™ä¸ªå›ç­”æ˜¯éå¸¸é‡è¦çš„ï¼Œå› ä¸ºå¦‚æœä½ è¦åˆ›ä½œæœ‰æ„ä¹‰çš„æ‘„å½±ä½œå“ï¼Œå°±å¿…é¡»ä¸“æ³¨äºé‚£äº›ä½ æœ€æ„Ÿå…´è¶£çš„é¢†åŸŸã€‚ä¸ä»…å¦‚æ­¤ï¼Œä½ è¿˜å¿…é¡»ä¸“æ³¨äºé‚£äº›ä½ å…·æœ‰å¼ºçƒˆä¸ªäººæƒ³æ³•çš„é¢†åŸŸã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿™æ˜¯ä¸€ä¸ªè¢«å¤§å¸ˆä»¬å¤šæ¬¡å¼ºè°ƒçš„å¿ƒå¾—ï¼šå…´è¶£æ˜¯ç¬¬ä¸€å¯¼å¸ˆã€‚
è€Œå¦‚æœæ²¡æœ‰å…´è¶£ä¼šæ€ä¹ˆæ ·ï¼Ÿ
ä¸å¾—ä¸è¯´ï¼Œä¸‹é¢è¿™æ®µè¯é‡Œæè¿°çš„æ„Ÿå—ä¼¼æ›¾ç›¸è¯†ã€‚ã€‚&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;åœ¨æ—¥å¸¸å¯¹è¯ä¸­ï¼Œä½ æ˜¯å¦å°è¯•è¿‡åœ¨ä½ ä¸æ„Ÿå…´è¶£æˆ–æ²¡ä»€ä¹ˆè§è§£çš„ä¸»é¢˜ä¸Šï¼Œè¯´ä¸€äº›å…·æœ‰æ„ä¹‰çš„è¯ï¼Ÿè¿™æ˜¯ä¸å¯èƒ½çš„ï¼ä½ æ— è¯å¯è¯´ï¼Œå› ä¸ºä½ ä¸æ„Ÿå…´è¶£ã€‚ä¸è¿‡ï¼Œè¿™ä¸€èˆ¬ä¸ä¼šå¦¨ç¢ä½ ç»§ç»­è®¨è®ºã€‚æ­£å¦‚äººä»¬è°ˆè®ºæ²¡æœ‰å…´è¶£çš„è¯é¢˜ä¸€æ ·ï¼Œä»–ä»¬ä¹Ÿå¯ä»¥æ‹æ‘„å…¶ä¸æ„Ÿå…´è¶£çš„äº‹ç‰©ï¼Œè€Œç»“æœæ˜¯ä¸€æ ·çš„ï¼šæ¯ç‡¥ä¹å‘³ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿™ä¸ªä¾‹å­ä¹Ÿå¾ˆç”ŸåŠ¨ï¼š&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä»¥ä¸€ä¸ªä¼Ÿå¤§çš„æ¼”è¯´å®¶ï¼ˆä¾‹å¦‚ä¸˜å‰å°”æˆ–è€…é©¬ä¸Â·è·¯å¾·Â·é‡‘ï¼‰ä¸ºä¾‹ï¼Œå¦‚æœæˆ‘ä»¬è®©ä»–ä»¬å¯¹ç¼è¢«å­è¿™ä¸ªä¸»é¢˜ä½œä¸€æ¬¡æ¿€æƒ…æ´‹æº¢çš„æ¼”è®²ï¼Œä»–ä»¬æ˜¯æ²¡æ³•åšåˆ°çš„ï¼ä»–ä»¬æ— è¯å¯è¯´ï¼Œå› ä¸ºè¿™ä¸æ˜¯å…¶è¯é¢˜æ‰€åœ¨ã€å…¶æ¿€æƒ…æ‰€åœ¨ã€‚ä»–ä»¬éœ€è¦åœ¨è‡ªå·±çš„ä¸»é¢˜ä¸Šå±•ç¤ºä¼Ÿå¤§çš„æ¼”è¯´æ‰åå’Œè¯´æœæŠ€å·§ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;å¤§å¸ˆä»¬çš„ä½œå“ä¸€èˆ¬éƒ½æ˜¯é£æ ¼éå¸¸ä¸€è‡´çš„ï¼Œå› ä¸ºä»–ä»¬å¾ˆä¸“æ³¨ã€‚
è€Œæ­£æ˜¯ä¸“æ³¨ï¼Œæ‰è®©ä»–ä»¬æˆä¸ºå¤§å¸ˆã€‚
ä¸–ç•Œä¸Šçš„æ²ƒåœŸé‚£ä¹ˆå¤šï¼Œä½†å¦‚æœä»…ä»…å±€é™äºèµ°é©¬è§‚èŠ±ä¼¼åœ°å»æ¸¸è§ˆè¿™äº›åœŸåœ°ï¼Œè€Œä¸æ˜¯é€‰æ‹©ä¸€å—è¿›è¡Œæ·±è€•ï¼Œä¹Ÿæ˜¯ä¸ä¼šæœ‰ä»»ä½•æ”¶è·çš„ï¼Œè¿™åŒæ—¶ä¹Ÿæ˜¯æœ€è¿‘è‡ªå·±å¯¹ç§‘ç ”çš„ä¸€äº›æ„Ÿæ‚Ÿã€‚
ç°åœ¨æ…¢æ…¢è®¤è¯†åˆ°ï¼Œæ‰€è°“çš„â€œé„™è§†é“¾â€ï¼Œä¸€èˆ¬éƒ½æ˜¯å¤„åœ¨èµ·è·‘çº¿ä¸ŠçŠ¹è±«çš„é€‰æ‰‹å¯¹äºå‰é€”æœ›è€Œç”Ÿç•çš„æ„Ÿå¹ã€‚
çœŸæ­£åœ¨èµ›é“ä¸Šå¥”é©°çš„ä¸“ä¸šè¿åŠ¨å‘˜ï¼Œæ˜¯ä¸ä¼šæœ‰æ—¶é—´å’Œå¿ƒæƒ…å»æ€è€ƒè¿™äº›çš„ï¼Œå› ä¸ºå¯¹äºä»–ä»¬æ¥è¯´ï¼Œä»–ä»¬çš„é¢†åŸŸè·Ÿâ€œä¸Šæ¸¸â€å’Œâ€œä¸‹æ¸¸â€é¢†åŸŸä¹‹é—´çš„åŒºåˆ«ä»…ä»…æ˜¯èµ›åœºçš„ä¸åŒï¼Œè€Œæ¯ä¸ªèµ›åœºéƒ½æœ‰è¶³å¤Ÿæ¿€çƒˆçš„æ¯”èµ›æ¥ä¾›ä»–ä»¬å¿™ç¢Œã€æœ‰è¶³å¤Ÿå¤§çš„è£èª‰æ¥ä¾›ä»–ä»¬äº‰å–ã€‚
ç”¨Stephen Coveyçš„è¯æ¥è¯´ï¼Œè¿™å«å¯Œè¶³æ€ç»´ï¼ˆabundance mentalityï¼‰ã€‚&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;è€Œä¼Ÿå¤§çš„æ‘„å½±å®¶åˆ™çŸ¥é“ä»€ä¹ˆæ˜¯ä»–ä»¬æ„Ÿå…´è¶£çš„ã€ä»€ä¹ˆæ˜¯ä»–ä»¬è§‰å¾—ä¹å‘³çš„ï¼Œä¹Ÿèƒ½è®¤è¯†åˆ°è‡ªå·±çš„å¼ºé¡¹å’Œå¼±é¡¹ï¼Œå¹¶ä¸“äºè‡ªå·±çš„å…´è¶£å’Œå¼ºé¡¹ã€‚ä»–ä»¬ä¼šå®šæœŸåœ°åœ¨å…¶ä»–é¢†åŸŸè¿›è¡Œä¸€äº›å°è¯•ï¼Œæ¥æ‰©å¤§è‡ªå·±çš„å…´è¶£èŒƒå›´å¹¶æ”¹è¿›ä»–ä»¬çš„å¼±é¡¹ï¼ˆä½ ä¹Ÿåº”è¯¥è¿™æ ·ï¼‰ï¼Œä½†ä»–ä»¬ä¸ä¼šæŠŠå°è¯•æ€§çš„æ‹æ‘„å’Œä¸¥è‚ƒæ·±åˆ»çš„è¡¨è¾¾æ··æ·†ã€‚ éŸ¦æ–¯é¡¿ä¸ä¼šæ‹æ‘„ç¬é—´å‘ç”Ÿçš„äº‹æƒ…ï¼Œçº½æ›¼ä¸ä¼šæ‹æ‘„é£æ™¯ç…§ï¼Œå°¤æ–¯æ›¼ä¸ä¼šæ‹æ‘„ä¸å¹¸çš„ç¤¾ä¼šæˆå‘˜ï¼Œé˜¿å‹ƒä¸ä¸ä¼šå°åˆ¶å‡ºè¶…ç°å®æ•ˆæœçš„å¤šé‡å½±åƒã€‚ä»–ä»¬çš„æ¯ä¸€ä½éƒ½ä¸“æ³¨äºè‡ªå·±å…´è¶£æœ€å¤§ã€æœ¬é¢†æœ€å¼ºçš„é¢†åŸŸã€‚ä»–ä»¬æˆ–è®¸å¯ä»¥åœ¨å…¶ä»–é¢†åŸŸåˆ›ä½œå‡ºä¸é”™çš„ä½œå“ï¼Œä½†è¿™äº›ä½œå“çš„æ°¸æ’æ€§å’Œå†²å‡»åŠ›ä¼šå¤§æ‰“æŠ˜æ‰£ã€‚ä»–ä»¬ï¼Œè¿˜æœ‰å…¶ä»–ä¼Ÿå¤§çš„æ‘„å½±å®¶ï¼Œéƒ½ç¿æ™ºåœ°å†³å®šåœ¨ä»–ä»¬æœ€æ“…é•¿çš„é¢†åŸŸå†…åˆ›ä½œã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;å›åˆ°æ­£é¢˜æ‘„å½±ä¸Šã€‚
æˆ‘ä»ä¸€å¹´åŠå‰è®¤çœŸå¯¹å¾…æ‘„å½±ä»¥æ¥ï¼Œè¿˜æ²¡æœ‰çœŸæ­£å¯¹ä»€ä¹ˆä¸»é¢˜æ„Ÿå…´è¶£è¿‡ï¼Œæˆ–è€…è¯´æ²¡æœ‰åšæŒä¸‹æ¥ã€‚
åˆšå¼€å§‹æ˜¯å¸¦ç€18-200mmå¥—æœºé•œå¤´ï¼Œåœ¨å¤§è¡—å°å··é‡Œæ¼«æ— ç›®çš„åœ°ç©¿æ¢­ï¼Œä¸çŸ¥é“è‡ªå·±æƒ³æ‹ä»€ä¹ˆã€‚
å°±ç®—å¶å°”çœ‹åˆ°æ„Ÿè§‰æœ‰æ„æ€çš„ä¸œè¥¿ï¼Œä¹Ÿä¸çŸ¥é“è¯¥æ€ä¹ˆæŠŠå®ƒä»¬è®°å½•ä¸‹æ¥ã€‚
å‡ ä¹æ‰€æœ‰çš„é¢˜æéƒ½æ˜¯ä¸€æ—¶å…´èµ·ã€æµ…å°è¾„æ­¢ï¼Œå°¤å…¶æ˜¯åœ¨ä¹°äº†æ–°è®¾å¤‡ä¹‹åæ‹ä¸€äº›é€‚åˆæ–°é•œå¤´çš„ä¸»é¢˜ã€‚
æ¯”å¦‚å½“æ—¶ä¹°äº†85mm f/1.8é•œå¤´å’Œä¸‰è„šæ¶ï¼Œä¹Ÿåœ¨æ‹äº†ä¸€ä¸¤æ¬¡è‡ªæ‹ç…§ä¹‹åä¾¿å†æ²¡æœ‰å°è¯•è¿‡ï¼Œè™½ç„¶åæ¥æœ‰è¿‡ä¸€ä¸¤æ¬¡çš„å†²åŠ¨ï¼›ä¹°äº†70-300mmé•¿ç„¦é•œå¤´ä¹Ÿå°±æ‹äº†ä¸€ä¸¤æ¬¡é¸Ÿï¼›ä¹°105mmå¾®è·åªæ‹äº†å‡ æ¬¡è˜‘è‡ï¼›ä¹°10-20mmå¹¿è§’é•œå¤´ä¹Ÿåªæ‹äº†å‡ æ¬¡å»ºç­‘ã€‚
è¯´æ¥å®åœ¨æƒ­æ„§ï¼Œå¾®è·å’Œå¹¿è§’é•œå¤´è®©è‡ªå·±æ»¡æ„çš„ä¸¤å¼ ç…§ç‰‡ï¼Œè¿˜éƒ½åˆ†åˆ«æ˜¯åœ¨åˆšä¹°è¿™ä¸¤æ”¯é•œå¤´é‚£ä¼šæ‹ä¸‹çš„ï¼š


















&lt;figure  id=&#34;figure-è˜‘è‡105mmå¾®è·&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;è˜‘è‡ï¼Œ105mmå¾®è·&#34; srcset=&#34;
               /post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_bfad4daf1ffcd6c102def86aa31ae798.webp 400w,
               /post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_70c9005b3313dcca607fb93f69d63b6e.webp 760w,
               /post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_bfad4daf1ffcd6c102def86aa31ae798.webp&#34;
               width=&#34;600&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      è˜‘è‡ï¼Œ105mmå¾®è·
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-ç®­å¤´10-20mmå¹¿è§’&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;â€œç®­å¤´â€ï¼Œ10-20mmå¹¿è§’&#34; srcset=&#34;
               /post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_2055bfaa515325ec6094166f4cc021ff.webp 400w,
               /post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_e25923abca67ba00fd03f88146b59bfd.webp 760w,
               /post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_2055bfaa515325ec6094166f4cc021ff.webp&#34;
               width=&#34;500&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      â€œç®­å¤´â€ï¼Œ10-20mmå¹¿è§’
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;å…¶å®å¾ˆå¤šæ‘„å½±é¢˜æéƒ½æœ‰å¸å¼•æˆ‘çš„åœ°æ–¹ã€‚
æ‹æ‘„æ—…æ¸¸é¢˜æå¯ä»¥æ”¶è·å„åœ°çš„ç¾æ™¯ç…§ï¼Œä½†æ˜¯å‰æœŸéœ€è¦å¯¹ç›®çš„åœ°çš„äººæ–‡ã€å†å²ç”šè‡³å¤©æ°”å’Œäº¤é€šç­‰éƒ½è¦æœ‰å……è¶³çš„äº†è§£ï¼›æ‹æ‘„å¾®è·å¯ä»¥æŠŠæ¸ºå°çš„äº‹ç‰©ä»¥éœ‡æ’¼çš„è§’åº¦å±•ç¤ºå‡ºæ¥ï¼Œå‰ææ˜¯æˆ‘æœ‰è¶³å¤Ÿçš„è€å¿ƒå»åšç»†è‡´å…¥å¾®çš„è§‚å¯Ÿï¼›é•¿ç„¦æ‘„å½±å¯ä»¥è®©æˆ‘æŠŠç¾å¦™çš„é‡ç”ŸåŠ¨ç‰©æ‹‰è¿‘åˆ°çœ¼å‰å¹¶å±•ç°ç»™è§‚ä¼—ï¼Œä½†æ˜¯éš¾åº¦ä¸äºšäºç‹™å‡»ä¸€ä¸ªä¸ç¡®å®šçš„ç›®æ ‡ï¼Œè€Œä¸”å‰æœŸå¯¹åŠ¨ç‰©ä¹ æ€§çš„äº†è§£ä¹Ÿæ˜¯å°‘ä¸äº†çš„ï¼›é£æ™¯æ‘„å½±å¯ä»¥å±•ç¤ºäººç±»å»ºç­‘æˆ–æ˜¯å¤§è‡ªç„¶ç¾æ™¯çš„éœ‡æ’¼ï¼Œä½†æ˜¯åˆ›æ„è§’åº¦çš„é€‰å–ã€æ€ä¹ˆé¿å¼€äººç¾¤æˆ–è€…è®©ä»–ä»¬å¾ˆå¥½åœ°èå…¥ç”»é¢ã€åœ¨ä»€ä¹ˆæ ·çš„å¤©æ°”æ‹æ‘„ç­‰ç­‰ä¹Ÿæ˜¯è®©æˆ‘å¾ˆå¤´å¤§ï¼›è€Œäººåƒæ‘„å½±ä¸ä»…éœ€è¦è‡ªå·±çš„å®¡ç¾è§‚ç‚¹ï¼Œè¿˜éœ€è¦å»ºç«‹å¥½è·Ÿæ¨¡ç‰¹çš„å…³ç³»æˆ–äº¤æµâ€”â€”å¦‚æœæˆ‘æœ‰æ¨¡ç‰¹çš„è¯ã€‚&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;åœ¨æ›´æ·±çš„å±‚æ¬¡ä¸Šï¼Œé™¤éæ‘„å½±å¸ˆå’Œä¸»è§’ä¹‹é—´æœ‰ç€å‹å¥½çš„å…³ç³»ï¼Œå¦åˆ™ä»»ä½•å°è¯•éƒ½ä¸å¯ä»¥ä¸ºä½ çš„è‚–åƒæ‘„å½±å¢è‰²ï¼ˆå³ä½¿æ²¡æœ‰å‹å¥½çš„å…³ç³»ï¼Œä¹Ÿè‡³å°‘æœ‰å®è´¨æ€§çš„æ²Ÿé€šæˆ–å¼ºçƒˆçš„ç¬¬ä¸€å°è±¡ï¼‰ã€‚æ‘„å½±å¸ˆåº”è¯¥è®¤è¯†ä¸»è§’ï¼Œå¯¹ä»–æ„Ÿå…´è¶£ï¼Œå¯¹ä»–æœ‰ä¸€å®šçš„çœ‹æ³•ï¼Œå¹¶åŠªåŠ›æŠŠä¸»è§’çš„ä¸ªæ€§ä»¥æœ€å¼ºçƒˆçš„æ–¹å¼è¡¨è¾¾å‡ºæ¥ã€‚æœ‰æ—¶ï¼Œæ‘„å½±å¸ˆå¿…é¡»å¼ºçƒˆä¾èµ–äºç¬¬ä¸€å°è±¡ï¼Œå› ä¸ºä»–å¾€å¾€å¾ˆéš¾èŠ±ä¸Šè¶³å¤Ÿçš„æ—¶é—´æ¥å®Œå…¨äº†è§£ä¸»è§’ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;æ€»è€Œè¨€ä¹‹ï¼Œä»»ä½•é¢˜æä¸‹å¥½çš„ä½œå“ä¼¼ä¹éƒ½ç¦»ä¸å¼€å‰æœŸåšå¤§é‡çš„åŠŸè¯¾ã€‚
æˆ‘æ˜æ˜¾æ˜¯æ‘„å½±æ€åº¦è¿˜ä¸å¤Ÿç«¯æ­£ï¼šæƒ³è¦å¥½çš„ç…§ç‰‡åˆæ€»å«Œå¼ƒè¿‡ç¨‹å¤ªè¾›è‹¦ï¼Œèƒ½é™ä¸‹å¿ƒæ¥çœ‹äº›æ‘„å½±æ•™æä¹Ÿæ˜¯ä»å‰ä¸ä¹…æ‰å¼€å§‹çš„ã€‚&lt;/p&gt;
&lt;p&gt;æ²¡æœ‰ä»€ä¹ˆå¥½çš„ç…§ç‰‡æ˜¯é è¿æ°”å¾—æ¥çš„ï¼›å°±ç®—æœ‰ï¼Œä¹Ÿéœ€è¦æå‰æŠŠåŸºæœ¬åŠŸç»ƒå¥½æ‰èƒ½æŠ“ä½ç¨çºµå³é€çš„æœºä¼šã€‚
æˆ‘æ— æ³•å¿˜è®°åˆšä¹°D7200çš„æ—¶å€™ï¼Œç°å®ç»™æˆ‘çš„ä¸€æ¬¡æ•™è®­ã€‚
å½“æ—¶æ‹¿ç€åˆšåˆ°æ‰‹çš„æ–°ç›¸æœºï¼Œå…´è‡´å†²å†²åœ°å°±ä¸Šè¡—å–æ™¯äº†ï¼Œç„¶åæ‹ä¸‹äº†è¿™å¼ åæ¥å¾ˆå¿«æ„è¯†åˆ°é—®é¢˜å¾ˆå¤šçš„ç…§ç‰‡ï¼š


















&lt;figure  id=&#34;figure-ç½‘çº¢å¢™ä¸‹ç¡è§‰çš„çŒ«æœªè°ƒä¿®&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ç½‘çº¢å¢™ä¸‹ç¡è§‰çš„çŒ«ï¼Œæœªè°ƒä¿®&#34; srcset=&#34;
               /post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_c689bab0096ef9110d9bc1bf6d82c0b3.webp 400w,
               /post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_32055c83bc31962ab69502014f5e1042.webp 760w,
               /post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_c689bab0096ef9110d9bc1bf6d82c0b3.webp&#34;
               width=&#34;600&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ç½‘çº¢å¢™ä¸‹ç¡è§‰çš„çŒ«ï¼Œæœªè°ƒä¿®
    &lt;/figcaption&gt;&lt;/figure&gt;

é‚£å¤©æˆ‘å¾ˆå¹¸è¿ï¼Œå› ä¸ºå¶é‡äº†è¿™é¢ç½‘çº¢å¢™ã€‚å½“æ—¶è¿˜ä¸çŸ¥é“è¿™é¢å¢™å°æœ‰åæ°”ï¼Œåªæ˜¯è§‰å¾—è¿™å¥è¯å¾ˆé†’ç›®ã€å¾ˆæœ‰ä¸ªæ€§ï¼Œæœ€å…³é”®çš„æ˜¯ä¸‹é¢é•¿å‡³ä¸Šæœ‰åªç¡è§‰çš„çŒ«ï¼Œå°±åƒæ˜¯æœ‰äººä¸“é—¨å®‰æ’çš„ä¸€æ ·ï¼
ç„¶è€Œé‚£å¤©æˆ‘ä¹Ÿæ˜¯å¾ˆä¸å¹¸çš„ï¼Œå› ä¸ºé¦–å…ˆæˆ‘å½“æ—¶æ‹ç…§ä¸ä¼šæ„å›¾ï¼ŒæŠŠç”»é¢é‡ç‚¹çš„çŒ«æ”¾åˆ°äº†å·¦ä¸‹è§’éå¸¸è¾¹ç¼˜çš„ä½ç½®ï¼›å…¶æ¬¡æ²¡æœ‰è¶³å¤Ÿäº†è§£ç›¸æœºçš„è®¾ç½®ï¼Œæ‰€ä»¥æ²¡æœ‰è®°å½•ä¸‹æ¥æ— æŸçš„ç…§ç‰‡ï¼Œç»™åæœŸè°ƒæ•´å¸¦æ¥äº†å¾ˆå¤§çš„é™åˆ¶ã€‚
æ‰€ä»¥å½“æˆ‘æŠŠè¿™å¼ ç…§ç‰‡åˆ†äº«åˆ°ç¤¾äº¤åª’ä½“ä¸Šä¹‹ååå“å¹³å¹³ï¼Œç”šè‡³å¾ˆå¤šäººéƒ½æ²¡æ³¨æ„åˆ°é‚£åªåœ¨é˜´å½±é‡Œçš„çŒ«ï¼Œåè€Œè¢«å æ®ç”»é¢å¤§éƒ¨åˆ†çš„ã€é«˜å¯¹æ¯”çš„å¢™å¸å¼•äº†ã€‚
å…¶å®å³ä¾¿æˆ‘ç°åœ¨æœ‰è¿™æ ·çš„æœºä¼šï¼Œä¹Ÿè¿˜æ˜¯å¾ˆéš¾æ‹å¥½è¿™ä¸ªåœºé¢çš„ï¼šå¢™ä¸Šçš„æ¶‚é¸¦å¤ªå¤§ã€å¤ªé†’ç›®ï¼›çŒ«çš„ä½ç½®ä¸åˆé€‚ï¼Œå¾ˆéš¾ä½œä¸ºä¸»ä½“è¢«çªå‡ºã€‚
å½“ç„¶ï¼Œè¦æƒ³æ‹çš„æ¯”æˆ‘è¿™å¼ å¥½ï¼Œè¿˜æ˜¯å¾ˆå®¹æ˜“çš„ï¼Œæ¯•ç«Ÿè¿™å¼ ç…§ç‰‡å·²ç»ä¸èƒ½å†çƒ‚äº†ï¼Œå“ˆå“ˆï¼
æˆ‘å½“å¤©æ‹äº†å¥½å¤šå¼ ï¼Œå…¶ä¸­ä¹Ÿæœ‰äº›è§’åº¦å¥½ç‚¹çš„ï¼Œä½†æ— å¥ˆå½“æ—¶è‡ªå·±å¤ªå‚»ï¼ŒæŠŠé‚£äº›ç…§ç‰‡ä»¥â€œé”åº¦ä¸å¤Ÿâ€åˆ æ‰äº†ã€‚ã€‚ã€‚
éƒ½æ˜¯æ–°æ‰‹å­¦è´¹å•Šï¼
è¿™æ ·çš„äº‹æƒ…å½“ç„¶æ˜¯å°‘æœ‰çš„å¯æƒœã€‚
ä½†å¦‚æœæˆ‘ä¸ç»ƒå¥½åŸºæœ¬åŠŸã€ä¸æ‰¾åˆ°è‡ªå·±çš„æ‘„å½±é£æ ¼ï¼Œæˆ‘è¿˜ä¼šç»§ç»­æµªè´¹å¿«é—¨æ•°ã€‚&lt;/p&gt;
&lt;p&gt;åæ¥æˆ‘åˆå¤šæ¬¡æ‹œè®¿è¿‡è¿™é¢å¢™ï¼Œå¯æƒœå†ä¹Ÿæ²¡è§åˆ°æœ‰çŒ«åœ¨ä¸‹é¢ç¡è§‰äº†ã€‚ã€‚&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;åè®°ï¼šåœ¨åç»­å¯¹æœ¬ä¹¦çš„å­¦ä¹ ä¸­ï¼Œçœ‹åˆ°äº†ä½œè€…å¯¹å¦‚ä¸‹ä½œå“çš„ä»‹ç»ï¼Œè®©æˆ‘é‡æ–°ç‡ƒå¯¹çŒ«é‚£å¼ ç…§ç‰‡çš„ä¸€ç‚¹ä¿¡å¿ƒã€‚&lt;/p&gt;


















&lt;figure  id=&#34;figure-ä¹å¥³-by-bruce-barnbaum&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/563fac1de4b07f78f2db1c2c/1447459082594-OAR3ZKKQIR87D1PII2Y9/ke17ZwdGBToddI8pDm48kJcL8RUadGdk4gpl41YTwHNZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PICa0vOBoO_YRM0aI4T8IW9lHW4ggziL-I7oURYsi2vL8KMshLAGzx4R3EDFOm1kBS/TheBeggarWoman.jpg?&#34; alt=&#34;ä¹å¥³ by Bruce Barnbaum&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;600&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ä¹å¥³ by Bruce Barnbaum
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;è™½ç„¶ä¹ä¸æ˜¯è§†è§‰ä¸­å¿ƒï¼Œä½†ä½ ä¸ä¼šç«‹åˆ»å°±èƒ½å‘ç°å¥¹ã€‚å¥¹å¤ªå°äº†ï¼Œæ— æ³•é©¬ä¸Šå°±çœ‹å¾—åˆ°ã€‚ä½†åªè¦ä½ å‘ç°äº†ï¼Œå½±åƒçš„æ€§è´¨å°±å®Œå…¨æ”¹å˜äº†ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ä»Barnbaumçš„è¿™å¹…ä½œå“é‡Œï¼Œæˆ‘è®¤è¯†åˆ°æœ‰æ—¶å€™å¯¹ä½œå“çš„ä¸€äº›æ‰“ç ´å¸¸è§„çš„è§£é‡Šæ˜¯æœ‰å¿…è¦çš„ï¼šæ—¢ç„¶å¤§å¸ˆå¯ä»¥æŠŠä¸»ä½“æ”¾åœ¨ä¸æ˜¾çœ¼çš„ä½ç½®ï¼Œé‚£ä¹ˆæˆ‘ä¸ºä»€ä¹ˆä¸å¯ä»¥å‘¢ï¼Ÿ
åªè¦æˆ‘ç»™è§‚è€…è¶³å¤Ÿçš„å¼•å¯¼å’Œè§£é‡Šï¼Œæ¯”å¦‚è¿™é‡Œæ ‡é¢˜åªæçŒ«ï¼Œç­‰è§‚è€…å‘ç°çŒ«çš„æ—¶å€™ï¼Œä¾ç„¶å¯ä»¥ä½“ä¼šåˆ°æˆ‘å½“æ˜¯çœ‹åˆ°è¿™ä¸ªåœºæ™¯æ—¶çš„æ„Ÿå—ã€‚&lt;/p&gt;


















&lt;figure  id=&#34;figure-ç¡è§‰çš„çŒ«æŠ¢æ•‘ç‰ˆ&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ç¡è§‰çš„çŒ«ï¼ŒæŠ¢æ•‘ç‰ˆ&#34; srcset=&#34;
               /post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_9f4f7b33e40b5909e220b1bb7600afee.webp 400w,
               /post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_bfbf5b134aea0fcf9e79bee6963dad0f.webp 760w,
               /post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_9f4f7b33e40b5909e220b1bb7600afee.webp&#34;
               width=&#34;400&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ç¡è§‰çš„çŒ«ï¼ŒæŠ¢æ•‘ç‰ˆ
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;è¿™é‡Œå¯¹ç…§ç‰‡è¿›è¡Œäº†è£å‰ªå’Œé»‘ç™½å¤„ç†ä»¥è¿‡æ»¤æ‰é¢œè‰²å¹²æ‰°ï¼Œå¹¶åšäº†split toningä»¥ä½¿çŒ«å’Œå¢™çš„è‰²å½©ç•¥æœ‰åˆ†ç¦»ã€‚
å½“ç„¶ï¼Œè¿™é‡Œå«å®ƒâ€œæŠ¢æ•‘ç‰ˆâ€ï¼Œæ˜¯å› ä¸ºå®ƒç¦»ä¸€å¹…å¥½çš„ä½œå“ä¾ç„¶è¿˜å¾ˆè¿œã€‚
å¦‚æœæˆ‘å†æœ‰è¿™æ ·çš„æœºä¼šï¼Œæˆ‘ä¼šæŠŠæœºä½å·¦ç§»ï¼Œä»¥å°½é‡æŠŠçŒ«æ”¾åœ¨æ›´æ˜¾çœ¼çš„ä½ç½®ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Computation Time</title>
      <link>https://shaojiejiang.github.io/post/en/adaptive-computation-time/</link>
      <pubDate>Tue, 28 Apr 2020 10:46:44 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/adaptive-computation-time/</guid>
      <description>&lt;p&gt;My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;additive-vs-multiplicative-halting-probability&#34;&gt;Additive vs multiplicative halting probability&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Multiplicative:&lt;/strong&gt; In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time.
It is acknowledged by the authors that using the logits $h_n^t$ as the halting probability at step $n$ might be more straightforward.
Therefore, the overall halting probability is calculated as $$p_t^n = h_t^n \prod_{u=1}^{n-1} (1 - h_t^u).$$
We use $(1 - h_t^u)$ for previous update steps to indicate that the updating is &lt;em&gt;not&lt;/em&gt; stopped until $n$.&lt;/p&gt;
&lt;p&gt;As each $p_t^n \in (0, 1)$ is relatively independent with each other and $\sum p_t^n$ is not bound to 1, this approach &lt;em&gt;does not&lt;/em&gt; restrict the update depth to grow arbitrarily.
The model can be of course trained to lower the expected ponder time $\rho_t = \sum n p_t^n$, but it is observed in the experiments that the resulting model is not preferable in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$h_t^1$ is usually just below threshold, intermediate $h_t^n = 0$, and final $h_t^N$ is high enough to halt the update.&lt;/li&gt;
&lt;li&gt;as the expectation is low, $p_t^N \ll p_t^1$, but the network learns to have a much higher magnitude of output states at step $N$, so that the final output is still dominated by the final state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Additive:&lt;/strong&gt; In contrast, the additive approach have an constraint of $\sum p_t^n = 1$, so that the probability is decreased monotonically with the number of updates growing larger.
Though being non-differentiable, the total ponder time (total updates at all positions) is penalized to avoid consuming unnecessary computation.
There is still one drawback of this approach, however.
The performance is sensitive to the penalty factor $\tau$, which is not intuitive to choose as a hyperparameter.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.08983&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Computation Time for Recurrent Neural Networks&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>å‡¯æ©æ–¯ä¸»ä¹‰â€”â€”ä¸­å›½ä»¥åŸºå»ºä¸ºæ ¸å¿ƒçš„ç»æµç­–ç•¥ç†è®ºèƒŒæ™¯</title>
      <link>https://shaojiejiang.github.io/post/zh/keynesianism/</link>
      <pubDate>Mon, 27 Apr 2020 22:18:24 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/keynesianism/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æ”¶å…¥ç­‰äºäººä»¬çš„éœ€æ±‚æ¶ˆè´¹çš„æ€»é‡ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æŠ•èµ„â€”â€”è´­ä¹°å‚æˆ¿ã€è®¾å¤‡ç­‰â€”â€”ç­‰äºå°†æ”¯å‡ºâ€œæ³¨å…¥â€ç»æµä¸­ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;åœ¨è¿™é‡Œï¼Œæ°´é¾™å¤´æŒ‡åˆ©æ¯ï¼Œå³å€Ÿè´·çš„ä»·æ ¼ã€‚å½“åˆ©æ¯é™ä½â€”â€”æ‰“å¼€æ°´é¾™å¤´â€”â€”å€Ÿè´·å˜å¾—ä¾¿å®œï¼Œæ›´å¤šçš„äººä¼šè¿›è¡Œè´·æ¬¾ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä¸–ç•Œå……æ»¡äº†ä¸ç¡®å®šï¼Œäººä»¬å¹¶ä¸ä¸€å®šè¦å°†è‡ªå·±çš„å‚¨è“„ä¸å‚æˆ¿å’Œå·¥å‚æŒ‚é’©ã€‚æˆ–è®¸ä½ å¯èƒ½åªæƒ³æŠŠé’±æ”¾åœ¨åºŠå«ä¸‹é¢ä»¥å¤‡ä¸æ—¶ä¹‹éœ€ã€‚åœ¨å‡¯æ©æ–¯çœ‹æ¥ï¼Œåˆ©æ¯å¹¶ä¸èƒ½æœ‰åŠ©äºå°†å¤šä½™çš„å‚¨è“„è½¬ä¸ºæŠ•èµ„ã€‚äº‹å®ä¸Šï¼Œå‚¨è“„å’ŒæŠ•èµ„ä¹‹é—´å¹¶æ²¡æœ‰å…³è”ã€‚&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å‡¯æ©æ–¯è®¤ä¸ºï¼Œå½“æµå‡ºé‡å¤§äºæµå…¥é‡æ—¶ä¾¿ä¼šå‘ç”Ÿç»æµè¡°é€€ã€‚&amp;rdquo; (from &amp;ldquo;ç»æµå­¦é€šè¯†è¯¾ï¼ˆè€¶é²å¤§å­¦å‡ºå“ï¼è€¶é²å¤§å­¦ç»æµå­¦å…¥é—¨è¯¾ï¼Œæ™®é€šäººä¹Ÿèƒ½è¯»æ‡‚çš„ç»æµå­¦ï¼ç†è®ºåˆ°ç°å®ï¼Œæ­èµ·ç”¨ç»æµå­¦æ”¹å–„ç°å®ç”Ÿæ´»çš„æ¡¥æ¢ ) (åšé›†ç»ç®¡å•†åŠ¡å¿…è¯»ç³»åˆ—)&amp;rdquo; by å°¼å°”Â·åŸºä»€ç‰¹å°¼, å¼ ç¼˜, åˆ˜å©§)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;ä¼ ç»Ÿç»æµå­¦è®¤ä¸ºï¼Œä¸€ä¸ªå›½å®¶çš„æ”¶å…¥ç­‰äºç»æµäº§èƒ½ï¼Œä½†å‡¯æ©æ–¯è®¤ä¸ºæ”¶å…¥ç­‰äºäººä»¬çš„æ¶ˆè´¹æ€»é‡ã€‚æ¶ˆè´¹ç­‰äºä¸ºç»æµæ³¨å…¥æ´»åŠ›ï¼Œè€Œæ²¡æœ‰å¾—åˆ°åˆ©ç”¨çš„å‚¨è“„æ„å‘³ç€æ´»åŠ›çš„æµå¤±ã€‚å¦‚æœæ— æ³•é˜»æ­¢äººä»¬æŠŠé’±éƒ½å­˜èµ·æ¥ï¼Œå¹¶ä¸”æ— æ³•æœ‰æ•ˆåˆ©ç”¨äººä»¬çš„å‚¨è“„ï¼Œé‚£ä¹ˆç»æµè¡°é€€å°±ä¼šå‘ç”Ÿã€‚&lt;/p&gt;
&lt;p&gt;ä¹Ÿè®¸ä¸­å›½çš„ç»æµå­¦å®¶æ—©å°±è®¤å¯äº†è¿™ä¸ªç†è®ºï¼Œæ‰€ä»¥ä»–ä»¬åˆ¶å®šäº†ä¸€ä¸ªç”±åŸºç¡€å»ºè®¾ä¸ºæ ¸å¿ƒçš„ç»æµå‘å±•ç­–ç•¥ã€‚ä¸­å›½çš„äººæ°‘å˜å¾—å–œæ¬¢å­˜é’±ï¼Œä»–ä»¬å­˜èµ·æ¥çš„é’±ä¸€éƒ¨åˆ†è¢«ç”¨æ¥æŠ•èµ„å…¬å…±åŸºç¡€è®¾æ–½ï¼šå…¬è·¯ã€é“è·¯ã€ç”µåŠ›ã€æ°´åˆ©ã€äº’è”ç½‘ç­‰ç­‰ï¼Œè€Œä¸”è¿™äº›è®¾æ–½æœ‰åˆ©äºä¸­å›½å…¨é¢çš„å°¤å…¶æ˜¯å†…é™†åœ°åŒºçš„ç»æµå‘å±•ã€‚æ­¤å¤–ï¼Œç”±å…¨é¢å‘å±•å¸¦çš„çš„ç»æµè¿›æ­¥ï¼Œä¿ƒä½¿äººä»¬æ‹¥æœ‰äº†æ›´å¤šçš„å‚¨è“„ã€‚ä»–ä»¬çš„å‚¨è“„å¹¶ä¸ä¼šæ°¸ä¹…å¢åŠ ï¼Œå› ä¸ºå½“ä¸‹çš„ç¤¾ä¼šé£æ°”è¿«ä½¿ä»–ä»¬æŠŠå‡ ä¹æ‰€æœ‰çš„å‚¨è“„éƒ½ç”¨åœ¨äº†ä¹°æˆ¿ã€ä¹°è½¦ã€å…»è€ã€åŒ»ç–—ã€æ—…æ¸¸ä»¥åŠåä»£æ•™è‚²ä¸Šã€‚äººä»¬æŒ£å¾—è¶Šæ¥è¶Šå¤šï¼Œè€Œä¸”ä¸­å›½çš„ç»æµå­¦å®¶ä»¬æ€»æœ‰åŠæ³•è®©äººä»¬æŠŠæ‰‹é‡Œçš„é’±èŠ±å‡ºå»ï¼Œäºæ˜¯ä¸­å›½çš„ç»æµæ‰ä¼šæŒç»­ä¸æ–­åœ°ç„•å‘æ–°æ´»åŠ›ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>æ‹¼å¤šå¤šæˆ–è®¸é€ ç¦äº†ä¸­å›½çš„è´«ä¸‹ä¸­å†œä»¥åŠç»æµä¸ç§‘æŠ€</title>
      <link>https://shaojiejiang.github.io/post/zh/pinduoduo/</link>
      <pubDate>Mon, 27 Apr 2020 22:03:10 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/pinduoduo/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä¼ä¸šå®¶è·å¾—æˆåŠŸçš„åŒæ—¶ä¹Ÿå¾—åˆ°äº†è´¢å¯Œã€‚ä»–ä»¬çš„æ–°å•†å“åœ¨ç»æµä½“ä¸­ä¼ æ’­ï¼Œäººä»¬å‘ç°è‡ªå·±æƒ³è¦ä¸€ä¸ªç•™å£°æœºæˆ–ç”µè§†ï¼Œä¾¿å‡ºé—¨è´­ä¹°ã€‚äº¨åˆ©Â·ç¦ç‰¹å’Œå®‰å¾·é²Â·å¡å†…åŸºåˆ†åˆ«é ç€ç”Ÿäº§é€‚ç”¨äºå¤§ä¼—çš„å»‰ä»·æ±½è½¦å’Œåœ¨é’¢é“åˆ¶é€ ä¸­å¼•å…¥æ–°æ–¹æ³•è€Œå‘è´¢è‡´å¯Œã€‚ å¾ˆå¿«ï¼Œä»¿æ•ˆè€…ä»¬å¼€å§‹ä»¿æ•ˆæœ€åˆçš„ä¼ä¸šå®¶ï¼Œç”Ÿäº§å‡ºäº†åŒæ ·çš„æ±½è½¦ã€ç†”ç‚‰æˆ–æŸ“æ–™ã€‚æ–°å•†å“å’ŒæŠ€æœ¯ä¼ æ’­åœ°æ›´è¿œäº†ï¼Œè¿™å¼•èµ·äº†æ•´ä¸ªè¡Œä¸šçš„å˜é©ï¼Œå¹¶æ‰©å¤§äº†ç»æµä½“é‡ã€‚æœ€ç»ˆï¼Œä¸€äº›ä¼ä¸šå€’é—­ï¼Œç»æµå¼€å§‹èç¼©ï¼Œç›´è‡³æ–°ä¸€è½®åˆ›æ–°å‡ºç°ã€‚èµ„æœ¬ä¸»ä¹‰çš„è£è¡°ä¸æµ®æ²‰éƒ½æºè‡ªå±‚å‡ºä¸ç©·çš„åˆ›æ–°æµªæ½®ä»¥åŠåˆ›ä¸šå’Œæ¨¡ä»¿çš„æ¶ˆé•¿ã€‚&amp;rdquo; (from &amp;ldquo;ç»æµå­¦é€šè¯†è¯¾ï¼ˆè€¶é²å¤§å­¦å‡ºå“ï¼è€¶é²å¤§å­¦ç»æµå­¦å…¥é—¨è¯¾ï¼Œæ™®é€šäººä¹Ÿèƒ½è¯»æ‡‚çš„ç»æµå­¦ï¼ç†è®ºåˆ°ç°å®ï¼Œæ­èµ·ç”¨ç»æµå­¦æ”¹å–„ç°å®ç”Ÿæ´»çš„æ¡¥æ¢ ) (åšé›†ç»ç®¡å•†åŠ¡å¿…è¯»ç³»åˆ—)&amp;rdquo; by å°¼å°”Â·åŸºä»€ç‰¹å°¼, å¼ ç¼˜, åˆ˜å©§)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;è¯šç„¶ï¼Œæ‹¼å¤šå¤šä¹Ÿè®¸åœ¨æŠ€æœ¯ä¸Šã€è¥é”€ä¸Šå¹¶æ²¡æœ‰ä¸ºåŒè¡Œä»¬å¸¦æ¥ä»€ä¹ˆåˆ›æ–°ï¼Œè€Œä¸”è¿˜ç ´åäº†å¾ˆå¤šå·¥è–ªé˜¶å±‚å¯¹ç½‘è´­å¹³å°çš„å°è±¡ï¼Œç„¶è€Œä¹Ÿè®¸è¿™äº›éƒ½åªæ˜¯è´Ÿé¢ä½œç”¨ã€‚æ•´ä½“ä¸Šæ‹¼å¤šå¤šä½¿å¾—æ–¹ä¾¿çš„ç½‘è´­æœå®æƒ åŠåˆ°åè¿œçš„ä¹¡æ‘å®¶åº­é‡Œï¼Œå¯ä»¥è¯´æœ€å¤§åŒ–äº†å½“å‰ç½‘è´­äº§ä¸šçš„åˆ©ç”¨ç‡ï¼Œç”šè‡³è¿˜å¯èƒ½ä¼šå¸¦åŠ¨ä¹¡æ‘ç»æµçš„å‘å±•ã€‚&lt;/p&gt;
&lt;p&gt;å°±åƒå¼•æ–‡é‡Œæåˆ°çš„ç¦ç‰¹ã€‚ä¹Ÿè®¸ä»–åˆšå¼€å§‹ä¹Ÿå› ä¸ºç”Ÿäº§å»‰ä»·çš„æ±½è½¦è€Œä¸ºè´µæ—æ‰€ä¸é½¿ï¼Œä¹Ÿè®¸ä»–çš„æ±½è½¦ä¹Ÿä¼¤è¿‡ä¸€æ—¶è´ªå›¾ä¾¿å®œçš„è´µæ—çš„å¿ƒï¼Œä½†ä¸å¯å¦è®¤çš„æ˜¯ï¼Œæ­£æ˜¯ä»–çš„è¿™ç§åŠªåŠ›æ‰ä½¿å¾—æ±½è½¦æˆä¸ºå¹³æ°‘å¤§ä¼—çš„äº¤é€šå·¥å…·è€Œä¸æ˜¯è´µæ—ç”¨æ¥ç‚«è€€çš„å¥¢ä¾ˆå“ã€‚é‚£ä¹ˆæ‹¼å¤šå¤šä¹Ÿæ˜¯æ‰¿æ‹…äº†è¿™æ ·ä¸€ä¸ªè§’è‰²ã€‚&lt;/p&gt;
&lt;p&gt;ä½œä¸ºç”Ÿåœ¨æ–°æ—¶ä»£çš„æˆ‘ä»¬ï¼Œäº¦æˆ–æ˜¯ç”Ÿæ´»åœ¨å¤§åŸå¸‚ã€ååœ¨èˆ’é€‚åŠå…¬æ¡Œå‰çš„æˆ‘ä»¬ï¼Œå¯èƒ½æ— æ³•ç†è§£åœ¨ç½‘è´­é¢†åŸŸå·²ç»æœ‰æ·˜å®å¤©çŒ«äº¬ä¸œç­‰å¹³å°è¦†ç›–äº†ä»å»‰ä»·åˆ°å“è´¨å„ä¸ªä»·æ ¼åŒºé—´ï¼Œä¸ºä»€ä¹ˆè¿˜ä¼šæœ‰æ‹¼å¤šå¤šæ¥è¿›ä¸€æ­¥æ‹‰ä½å“è´¨ä¸ä»·æ ¼çš„ä¸‹é™ã€‚æˆ‘ä»¬ä¹‹æ‰€ä»¥ä¼šæœ‰è¿™æ ·çš„æƒ³æ³•ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬åœ¨ä¸ç½‘è´­ä¸€èµ·æˆé•¿ï¼Œæˆ‘ä»¬æ¥å—äº†ç½‘è´­åŒæ—¶ä¹Ÿæˆå°±äº†ç½‘è´­ã€‚ä¹ æƒ¯æˆè‡ªç„¶çš„æˆ‘ä»¬å¿½è§†äº†æœ‰ä¸€ä¸ªç¾¤ä½“ä¸€ç›´éƒ½ä¸åœ¨æˆ‘ä»¬ä¸ç½‘è´­å½¢æˆçš„å…±ç”Ÿä½“ä¸­â€”â€”é‚£äº›ç”Ÿæ´»åœ¨ä¹¡æ‘çš„è´«è‹¦å†œæ°‘ä»¬ã€‚&lt;/p&gt;
&lt;p&gt;ä»–ä»¬ä¸èƒ½åƒæˆ‘ä»¬ä¸€æ ·å¾ˆå¿«é€‚åº”æ–°ç§‘æŠ€ã€æ–°æ½®æµçš„å‘å±•ï¼Œæ›´ä¸èƒ½åƒæˆ‘ä»¬ä¸€æ ·åœ¨ä»·æ ¼ä¸å“è´¨ä¹‹é—´éšæ€§é€‰æ‹©ã€‚è®°å¾—ä¹‹å‰åœ¨ä¸€ç¯‡æ—¶äº‹ç‚¹è¯„é‡Œï¼ˆã€æ¯æ—¥äººç‰©ã€å…³äºæ‹¼å¤šå¤šçš„ç‚¹è¯„ã€Š&lt;a href=&#34;https://mp.weixin.qq.com/s/kfj6cAIsajiyumnURNxpeQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ä»–ä»¬ï¼Œåœ¨æ‹¼å¤šå¤šä¸Šæ‹¼è¿æ°”&lt;/a&gt;ã€‹ã€‚æƒŠè®¶å¾—å‘ç°æ–‡ç« å·²ç»æ˜¯2018å¹´8æœˆçš„äº†ï¼Œæˆ‘æ˜æ˜è®°å¾—æ˜¯å»å¹´çœ‹è¿‡çš„å•Šã€‚ã€‚ï¼‰çœ‹åˆ°è¿‡ï¼Œæˆ‘ä»¬çœ¼é‡Œçš„å‡å†’ä¼ªåŠ£äº§å“ï¼Œå¯èƒ½æ˜¯é‚£äº›ç©·äººçœ¼é‡Œçš„ä¸€æ¬¡æ¶ˆè´¹å‡çº§ã€‚ä»–ä»¬ä¸ä¼šç”¨æ·˜å®å¤©çŒ«äº¬ä¸œï¼Œä¹Ÿæ²¡æœ‰åŠ¨åŠ›å»å­¦ä¹ å¦‚ä½•ä½¿ç”¨â€”â€”æ¯•ç«Ÿä»–ä»¬éœ€æ±‚ä½ï¼Œå°±ç®—æœ‰éœ€æ±‚ï¼Œè¿™äº›å¹³å°çš„å•†å“ä¹Ÿå¯èƒ½ä¼šè¶…å‡ºä»–ä»¬çš„ï¼ˆå¿ƒç†ï¼‰æ‰¿å—èŒƒå›´ã€‚è‹¥ä¸æ˜¯æ‹¼å¤šå¤šä¾é ä½ä»·ã€äº²å‹å¸®å¿™ç ä»·ç­‰ç­‰ç­–ç•¥è®©è¿™ä¸ªç¾¤ä½“æ¥å—è¿™ç§æ–°çš„æ¶ˆè´¹æ–¹å¼ï¼Œç½‘ç»œè´­ç‰©ä½•æ—¶æ‰èƒ½æ¸—é€åˆ°ç¤¾ä¼šçš„ç¥ç»æœ«æ¢¢é‡Œå»ï¼Ÿæ‹¼å¤šå¤šæ˜¯ä¸€ä¸ªæ¨¡ä»¿è€…ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæ¨å¹¿è€…ã€ä¸€è‚¡æ·±åŒ–æŠ€æœ¯å¯¹ç¤¾ä¼šå˜é©ä¸å¯ç¼ºå°‘çš„åŠ›é‡ã€‚&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Hub for Transformer Blogs and Papers</title>
      <link>https://shaojiejiang.github.io/post/en/transformer-blog-paper-hub/</link>
      <pubDate>Mon, 02 Mar 2020 14:26:59 +0100</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/transformer-blog-paper-hub/</guid>
      <description>&lt;p&gt;This is a growing list of pointers to useful blog posts and papers related to transformers.&lt;/p&gt;
&lt;h2 id=&#34;transformers-explained&#34;&gt;Transformers explained&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: The Illustrated Transformer&lt;/a&gt; has many intuitive animations of how transformer models work&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mostafadehghani.com/2019/05/05/universal-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Universal Transformers&lt;/a&gt; introduces the idea of &lt;em&gt;recurrence among layers&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformer vs RNN and CNN for Translation Task&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gnns-similarities-and-differences&#34;&gt;GNNs: similarities and differences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://graphdeeplearning.github.io/post/transformers-are-gnns/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformers are Graph Neural Networks&lt;/a&gt; bridges transformer models and Graph Neural Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;transformer-improvements&#34;&gt;Transformer improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/deepmind-releases-a-new-architecture-and-a-new-dataset-to-improve-long-term-memory-in-deep-22f4b098153&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems&lt;/a&gt; Nural Turing Machine + transformer?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TLDR: Token Loss Dynamic Reweighting for Reducing Repetitive Utterance Generation</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2020-tldr/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2020-tldr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What&#39;s New in XLNet?</title>
      <link>https://shaojiejiang.github.io/post/en/xlnet/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/xlnet/</guid>
      <description>&lt;h2 id=&#34;rip-bert&#34;&gt;R.I.P BERT&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT&lt;/a&gt; got a head shot yesterday, by another guy called &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;XLNet&lt;/a&gt;.
It is reported that XLNet defeated BERT on 20 NLP tasks, and achieved 18 new state-of-the-art results.
Isn&amp;rsquo;t it impressive?
So, farewell, BERT.


















&lt;figure  id=&#34;figure-rip-bert&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;R.I.P BERT&#34; srcset=&#34;
               /post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_ad3b60d050a67b9089255b10065b08b1.webp 400w,
               /post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_910a11ae63908cf22e4e12ec92059faf.webp 760w,
               /post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_ad3b60d050a67b9089255b10065b08b1.webp&#34;
               width=&#34;570&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      R.I.P BERT
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;is-bert-really-dead&#34;&gt;Is BERT really dead?&lt;/h2&gt;
&lt;p&gt;Since I love BERT, I decided to read the paper to find out what killed him.
While reading, I was thinking wait a minute, is BERT really dead?
After finished the paper, I was so glad to know that BERT is still well alive!
He is just wearing another coat named &lt;em&gt;Two-Stream Self-Attention (TSSA)&lt;/em&gt;, with some other gadgets!
Because:&lt;br&gt;
&lt;code&gt;XLNet = BERT + TSSA + bidirectional data input&lt;/code&gt;&lt;br&gt;
Bert you&amp;rsquo;re so tough, buddy!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at what were trying to kill BERT.&lt;/p&gt;
&lt;h3 id=&#34;two-stream-self-attention-tssa&#34;&gt;Two-stream self-attention (TSSA)&lt;/h3&gt;
&lt;p&gt;Why TSSA is needed to kill BERT?
Well, let&amp;rsquo;s first see some weaknesses BERT has.&lt;/p&gt;
&lt;p&gt;BERT is using a masked language model (MLM) training objective, which is essentially why it achieves bidirectional representation.


















&lt;figure  id=&#34;figure-image-sourcehttpsnlpstanfordeduseminardetailsjdevlinpdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;[Image source](https://nlp.stanford.edu/seminar/details/jdevlin.pdf)&#34; srcset=&#34;
               /post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_bbb8a61fc9a0697db6e27484ef59e402.webp 400w,
               /post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_a370a21d4f992ecb82ae8e2649177c0d.webp 760w,
               /post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_bbb8a61fc9a0697db6e27484ef59e402.webp&#34;
               width=&#34;760&#34;
               height=&#34;103&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://nlp.stanford.edu/seminar/details/jdevlin.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Image source&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this example, both words &amp;ldquo;store&amp;rdquo; and &amp;ldquo;gallon&amp;rdquo; are intended to be predicted by BERT, and their input word embeddings are replaced by the embedding of a special token &lt;em&gt;[MASK]&lt;/em&gt;.
Usually this isn&amp;rsquo;t a problem, but what if the prediction of &amp;ldquo;store&amp;rdquo; requires knowing the word &amp;ldquo;gallon&amp;rdquo;?
That is exactly where BERT falls short.&lt;/p&gt;
&lt;p&gt;TSSA is what you can use to overcome that downside of MLM:


















&lt;figure  id=&#34;figure-query-stream-sourcehttpsarxivorgabs190608237&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Query stream, [source](https://arxiv.org/abs/1906.08237)&#34; srcset=&#34;
               /post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_a84c83c6b945dba172c71d33c7936aac.webp 400w,
               /post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_0758dcc20abd12daa219dd5d9bddf6da.webp 760w,
               /post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_a84c83c6b945dba172c71d33c7936aac.webp&#34;
               width=&#34;760&#34;
               height=&#34;645&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Query stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this illustration, query stream gives you the &lt;code&gt;query&lt;/code&gt; vector needed for attention calculation, and this stream is designed in such a way that it doesn&amp;rsquo;t leak the info of the word it&amp;rsquo;s going to predict, but guarantees all information from other positions.
Take $x_1$ for example: $x_1$&amp;rsquo;s embedding (and hidden state) is not used at all, but embeddings and hidden states from other positions are used in each layer.&lt;/p&gt;


















&lt;figure  id=&#34;figure-content-stream-sourcehttpsarxivorgabs190608237&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Content stream, [source](https://arxiv.org/abs/1906.08237)&#34; srcset=&#34;
               /post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_f91903c52b2690d818283e5376124698.webp 400w,
               /post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_77f32608974273a37acf00c5c6de89e2.webp 760w,
               /post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_f91903c52b2690d818283e5376124698.webp&#34;
               width=&#34;760&#34;
               height=&#34;561&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Content stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Content stream, on the other hand, gives you the &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; vectors needed for context vector calculation.
This stream uses a strategy similar to that in a standard &lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer decoder&lt;/a&gt; by masking future positions.
The only difference is that in content stream, the order of tokens is &lt;em&gt;randomly permuted&lt;/em&gt;.
For example $x_2$ is right after $x_3$, and therefore $h_2^{(1)}$ can only see the embedding of itself and that of $x_3$ (and $mem^{(0)}$), but not that of $x_1$ or $x_4$.&lt;/p&gt;
&lt;h3 id=&#34;mask-a-span&#34;&gt;Mask a span&lt;/h3&gt;
&lt;p&gt;Another difference from BERT is masking a span of consecutive words.
The reason I guess, is that this guarantees the dependence of masked words (as claimed to be what BERT can&amp;rsquo;t model).
This is not a fresh-new idea, though.
Recently there are two ERNIE papers (BERT based) that propose masking named entities (often of multiple words, &lt;a href=&#34;https://arxiv.org/pdf/1905.07129.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;) and/or phrases (&lt;a href=&#34;https://arxiv.org/pdf/1904.09223.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-data-input&#34;&gt;Bidirectional data input&lt;/h3&gt;
&lt;p&gt;Another notably different thing in XLNet is the usage of bidirectional data input.
The idea (I guess) is to decide the factorization direction (either forward or backward), so that the idea of &amp;ldquo;masking future positions&amp;rdquo; used in a standard Transformer decoder can also be easily used together with XLNet.&lt;/p&gt;
&lt;p&gt;Masking a span makes XLNet look like a denoising autoencoder; but by using bidirectional data input (or masking future positions), XLNet performs more like a autoregressive language model in the masked region.&lt;/p&gt;
&lt;h2 id=&#34;closing-remarks&#34;&gt;Closing remarks&lt;/h2&gt;
&lt;p&gt;So now you probably can see the similarities and differences between XLNet and BERT.
If not, here is a quick summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of masking random words, mask a span of words&lt;/li&gt;
&lt;li&gt;Use bidirectional data input to decide which direction you treat as &amp;ldquo;future&amp;rdquo;, and then apply the idea of masking future positions&lt;/li&gt;
&lt;li&gt;To avoid leaking the information of the position to be predicted, use Two-Stream Self-Attention (TSSA)&lt;/li&gt;
&lt;li&gt;Other minor things like segment recurrence, relative positional encoding, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it doesn&amp;rsquo;t seem to be enough changes to make all those improvements.
What if BERT is also trained using the additional data (Giga5, ClueWeb, Common Crawl), will XLNet still be able to defeat BERT?&lt;/p&gt;
&lt;p&gt;EDIT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another model named &lt;a href=&#34;https://arxiv.org/abs/1905.02450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MASS&lt;/a&gt; employs a very similar idea.&lt;/li&gt;
&lt;li&gt;According to Jacob Devlin (author of BERT), relative positional embedding might be of great importance.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://shaojiejiang.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com/pdf-export/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} One {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} Three {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  {{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2019-improving/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2019-improving/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2018-sequence/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2018-sequence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Struck tracker via color Haar-like feature and selective updating</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2017-robust/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2017-robust/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Project</title>
      <link>https://shaojiejiang.github.io/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/project/example/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://shaojiejiang.github.io/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Object tracking via dual linear structured SVM and explicit feature map</title>
      <link>https://shaojiejiang.github.io/publication/ning-2016-object/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/ning-2016-object/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/home-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/home-zh/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Google Search this site å…¨ç«™è°·æ­Œæœç´¢</title>
      <link>https://shaojiejiang.github.io/search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/search/</guid>
      <description>&lt;script async src=&#34;https://cse.google.com/cse.js?cx=018409216753371592144:rrbl3bjsbqc&#34;&gt;&lt;/script&gt;
&lt;div class=&#34;gcse-search&#34;&gt;&lt;/div&gt;
&lt;p&gt;Tips: Please change to Light theme æç¤ºï¼šè¯·æ¢æˆæ˜äº®ä¸»é¢˜&lt;/p&gt;
&lt;p&gt;Note: If the integrated search function fails to return content, try Google search.
è¯´æ˜ï¼šå¦‚æœé›†æˆæœç´¢åŠŸèƒ½å¤±æ•ˆï¼Œè¯·ä½¿ç”¨è°·æ­Œæœç´¢ã€‚&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shaojie Jiang</title>
    <link>https://shaojiejiang.github.io/</link>
      <atom:link href="https://shaojiejiang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Shaojie Jiang</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Shaojie Jiang</copyright>
    <image>
      <url>https://shaojiejiang.github.io/images/icon_hubc11b102ed638ad8eb587d60e1601a1f_37041_512x512_fill_lanczos_center_2.png</url>
      <title>Shaojie Jiang</title>
      <link>https://shaojiejiang.github.io/</link>
    </image>
    
    <item>
      <title>Transformer Align Model</title>
      <link>https://shaojiejiang.github.io/post/en/transformer-align-model/</link>
      <pubDate>Sat, 16 May 2020 16:40:07 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/transformer-align-model/</guid>
      <description>&lt;p&gt;In this paper&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, transformer is trained to perform both translation and alignment tasks.&lt;/p&gt;
&lt;h2 id=&#34;application-scenarios-of-word-alignments-in-nmt&#34;&gt;Application scenarios of word alignments in NMT&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generating bilingual lexica from parallel corpora&lt;/li&gt;
&lt;li&gt;External dictionary assisted translation to improve translation of low frequency words&lt;/li&gt;
&lt;li&gt;Trust, explanation, error analysis&lt;/li&gt;
&lt;li&gt;Preserving style on webpages&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-design&#34;&gt;Model design&lt;/h2&gt;
&lt;p&gt;The attention mechanism has long been motivated by word alignments in statistical machine translation, but ensure the alignment quality, additional supervision is needed.&lt;/p&gt;
&lt;p&gt;There is a tendency that the attention probabilities from the penultimate layer of a normally trained transformer MT model corresponds to word alignments.
Therefore, one attention head (clever!) in the penultimate layer is trained as the alignment head.
The motivation of selecting only one attention head for alignment is to give the freedom to the model of choosing whether to rely more on the alignment or other attention heads.&lt;/p&gt;
&lt;!-- While in Beamer alignment, the freedom is fully preserved in the attention layer, and the alignment is used for RNN hidden states. --&gt;
&lt;h2 id=&#34;how-two-train-the-alignment-head&#34;&gt;How two train the alignment head&lt;/h2&gt;
&lt;p&gt;There are two approaches existing in the literature:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Label alignments beforehand and train the attention weights through KL-divergence.&lt;/li&gt;
&lt;li&gt;Use the attentional vector to also predict either the target word or the properties such as POS tags of the target tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Stopped reading at experiments.
May come back later.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1909.02074&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jointly Learning to Align and Translate with Transformer Models&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Compressive Transformers</title>
      <link>https://shaojiejiang.github.io/post/en/compressive-transformers/</link>
      <pubDate>Tue, 12 May 2020 14:29:44 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/compressive-transformers/</guid>
      <description>&lt;p&gt;Built on top of Transformer-XL, Compressive Transformer&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; condenses old memories (hidden states) and stores them in the compressed memory buffer, before completely discarding them.
This model is suitable for long-range sequence learning but may cause too much computational burden for tasks that only have short sequences.
Compressive Transformers can also be used as memory components in conjunction with other models.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In the beginning, the authors draw the connection between their work and human brains by mentioning that humans memorize things via lossy compression.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We aggressively select, filter, or integrate input stimuli based on factors of surprise, perceived danger, or repetition &amp;ndash; amongst other signals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It&amp;rsquo;s often, if not always, good to see such insights of how AI works are inspired by humans.
It&amp;rsquo;s also good to see that they relate their work to previous works, i.e. RNNs, transformers and sparse attention.&lt;/p&gt;
&lt;p&gt;An RNN compresses previous memories into a fixed size hidden vector, which is space-efficient, but also results in its temporal nature and hence difficult to parallelize.
Transformers, on the other hand, store all the past memories uncompressed, which can be beneficial for achieving better performances such as precision, BLEU, perplexity, etc, but it costs more and more computation and memory space with the sequence length growing.
Sparse attention can be used to reduce computation, while the spatial cost remains the same.&lt;/p&gt;
&lt;h2 id=&#34;model-design-and-training&#34;&gt;Model design and training&lt;/h2&gt;
&lt;p&gt;The proposed Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, trained to query both its short-term granular memory and longer-term coarse memory.&lt;/p&gt;
&lt;p&gt;If trained using original task-relevant loss only, it requires backpropagating-through-time (BPTT) over long unrolls for very old memories.
A better solution is to use local auxiliary losses by stopping gradients and reconstructing either the original memory vectors (lossless objective) or attention vectors (lossy objective; reportedly to work better).
The second choice for the auxiliary loss, in other words, means that we don&amp;rsquo;t care whether the original memory can be reconstructed or not, as long as the attention vector can be reconstructed, given the same query (brilliant!).&lt;/p&gt;
&lt;h3 id=&#34;some-practical-concerns&#34;&gt;Some practical concerns&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The auxiliary loss is only used to train the compression module, as it harms the learning when the gradients flow back to the main network.
This might also explain why I couldn&amp;rsquo;t reproduce 
&lt;a href=&#34;../adaptive-computation-time&#34;&gt;ACT&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;Batch accumulation (4x bigger batch size) is used for better performance.
It is observed in some works that bigger batch sizes lead to better generalization, but some other works found the opposite to be true (discussed in the papers and talks mentioned 
&lt;a href=&#34;../visualizing-loss&#34;&gt;in my other post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Model optimization is very sensitive to gradient scales, so the gradient norms are clipped to 0.1 for stable results.
This is typical for transformer variants.&lt;/li&gt;
&lt;li&gt;Convolution works best for memory compression.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;further-thoughsquestions&#34;&gt;Further thoughs/questions:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Compressive Transformer improves the modeling of rare words.
But why?&lt;/li&gt;
&lt;li&gt;In the discussion section, the authors pointed out that future directions could include the investigation of adaptive compression rates by layer, the use of long-range shallow memory layers together with deep short-range memory, and even the use of RNNs as compressors.&lt;/li&gt;
&lt;/ol&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compressive Transformers for Long-Range Sequence Modelling&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing the Loss Landscape of Neural Nets</title>
      <link>https://shaojiejiang.github.io/post/en/visualizing-loss/</link>
      <pubDate>Wed, 06 May 2020 10:13:43 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/visualizing-loss/</guid>
      <description>&lt;p&gt;Here are some notes take while reading the NeurlIPS 2018 paper 
&lt;a href=&#34;http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt;.
This work helps explain why some models are easier to train/generalize than others.
The above image is a good illustration: with a much smoother loss landscape, DenseNet with 121 layers is much easier to train than a ResNet-110 without skip connections, and generalizes better in the mean time.&lt;/p&gt;
&lt;p&gt;The traditional way of visualizing loss functions of neural models in 2D contour plots is by choosing a center point $\theta^*$ (normally the converged model parameters), two random direction vectors $\delta$ and $\eta$, then plot the function:
$$f(\alpha, \beta) = L(\theta^* + \alpha \delta + \beta \eta)$$
Batch norm parameters are unchanged.&lt;/p&gt;
&lt;p&gt;The above method fails to capture the intrinsic geometry of loss surfaces, and cannot be used to compare the geometry of two different minimizers or two different networks.
This is because of the &lt;em&gt;scale invariance&lt;/em&gt; in network weights (this statement only applies to rectified networks as per the paper).
To tackle this, the authors normalize each filter in a direction vector $d$ ($\delta$ or $\eta$) to have the same norm of the corresponding filter in $\theta$:
$$d_{i, j} \leftarrow \frac{d_{i, j}}{||d_{i, j}||} ||\theta_{i, j} ||.$$
$i$ is the layer number and $j$ the filter number.
With the proposed filter-wise normalized direction vectors, the authors found that the sharpness of local minima correlates well with generalization error, even better than layer-wise normalization (for direction vectors).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why flat minima:&lt;/strong&gt; In a recent talk&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, Tom Goldstein (the last author) pointed out that flat minima correspond to large margin classifiers, which is more tolerant to domain shifts of data, thus having better generalization ability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Known influential factors:&lt;/strong&gt;
Small-batch training results in flat minima, while large-batch training results in sharp minima.
Increased width prevents chaotic behavior, and skip connections dramatically widen minimizers (see figure in the beginning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting with precaution:&lt;/strong&gt;
The loss surface is viewed under a dramatic dimensionality reduction.
According to the authors&amp;rsquo; analysis, if non-convexity is present in the dimensionality reduced plot, then non-convexity must be present in the full-dimensional surface as well.
However, apparent convexity in the low-dimensional surface does not mean the high-dimensional function is truly convex. Rather it means that the positive curvatures are dominant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In a nutshell:&lt;/strong&gt; It&amp;rsquo;s a great work trying to visualize the mystery of what&amp;rsquo;s going well/bad when training a neural model.
Although claiming the study to be empirical, I personally found their experiments and results very convincing.
Appendix B about visualizing optimization paths is also very insightful, and the authors probably also thought so, so they decided to move it as a main section in their latest 
&lt;a href=&#34;https://arxiv.org/pdf/1712.09913.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arxiv version&lt;/a&gt; ð!&lt;/p&gt;
&lt;p&gt;Further thoughts/questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Has it been done for visualizing NLP models?&lt;/li&gt;
&lt;li&gt;Is it more appropriate to visualize loss for NLG or other measures?
This might depend on how to define &amp;ldquo;labels&amp;rdquo; in NLG tasks.&lt;/li&gt;
&lt;li&gt;How big a convolution filter normally is?&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s similar between RNN and skip connections?&lt;/li&gt;
&lt;li&gt;This work can be used together with automatic neural architecture search, but is there any other more efficient way of getting better models?&lt;/li&gt;
&lt;/ol&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;http://iclr2020deepdiffeq.rice.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalization in neural nets:  a perspective from science (not math)&lt;/a&gt; Starting at 1:54:00 in the video. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>å­¦ä¹ çä¸è¦ç´ </title>
      <link>https://shaojiejiang.github.io/post/zh/three-key-elements-of-learning/</link>
      <pubDate>Thu, 30 Apr 2020 09:12:07 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/three-key-elements-of-learning/</guid>
      <description>&lt;p&gt;åæ³¨ï¼å¦æ ç¹æ®è¯´æï¼æ¬æçæå½ååèª&amp;quot;æå½±çèºæ¯ (ä¸çé¡¶çº§æå½±å¤§å¸)&amp;rdquo; by Bruce Barnbaum, æ¨æºæ¯&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;èªæå®¡é®è¦æä¸ä¸ªåççæéãå¨å ä¸ºè¿äºèªçèæå°ç¦èä¹åï¼ä½ åºè¯¥éè¿ææä¸äºç§çæ¥å¯¹å¤äº¤æµã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;å¨è¯»å°è¿æ®µè¯çæ¶åï¼ææ³èµ·äºæºå¨å­¦ä¹ å¯¹æä¸ªäººå­¦ä¹ æ¹å¼çä¸äºå¯åï¼åæ¶ä¹æ¯æåå£«å¯¼å¸Maartenå¯¹æçæå¯¼ï¼è¡¨è¿°ï¼æ¯å¦æ¼è®²ãæ¥åãäº¤æµååä½ç­ï¼æ¯å­¦ä¹ çä¸ä¸ªéè¦ç¯èã
å­å­è¯´ï¼&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å­¦èä¸æåç½ï¼æèä¸å­¦åè´»ã&amp;rdquo; ââè®ºè¯­Â·ä¸ºæ¿&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ä½æè®¤ä¸ºï¼å¦ææ²¡æä¸ä¸ªè¡¨è¿°çè¿ç¨ï¼âå­¦âåâæâå¾æå¯è½ä¼åæç©ºä¸­æ¥¼éã
åªæéè¿å°èªå·±çè§ç¹è¡¨è¾¾åºæ¥ãæ¾å°ç°å®ä¸­è®©å®ä»¬å»ç»åæ¶é´çèéªãç»åå«äººçæ¹è¯ï¼æè½çæ­£å°å°èªå·±çæå­¦ææè½å°å®å°ã
å æ­¤æè®¤ä¸ºï¼å­¦ä¹ çä¸è¦ç´ å¯ä»¥å½çº³ä¸ºï¼å­¦ãæåè¿°ã&lt;/p&gt;
&lt;h2 id=&#34;åæºå¨å­¦ä¹ å­¦ä¹ &#34;&gt;åâæºå¨å­¦ä¹ âå­¦ä¹ &lt;/h2&gt;
&lt;p&gt;è¿æ¥éçå¯¹âæºå¨å­¦ä¹ âæèçæ·±å¥ï¼æéæ¸ä»ä¸­å¾å°ä¸äºå¯åï¼æ¯å¦æå¼å§éè§éè¯»å°±æ¯å ä¸ºè®¤è¯å°äºå¤§æ°æ®éå¯¹è®­ç»æºå¨å­¦ä¹ ç®æ³çéè¦æ§ï¼å¦æç®æ³éè¦è¾å¥å¤§éçæ°æ®ä»¥ååå¤å°å­¦ä¹ æè½å¾å°çæ³çæ§è½ï¼é£ä¹ææ¯ä¸æ¯ä¹åºè¯¥è¿ä¹åå¢ï¼
éè¯»å°±æ¯äººç±»å­¦ä¹ çä¸ä¸ªéè¦âè¾å¥âæ¹å¼ã
å®è®©åäººæ»ç»åºçææ³ç²¾åï¼ç©¿è¶äºæ¶ç©ºçéå¶ä¸æ­å°è¾å¥å°è¯»èçææ³ä¸­ã
å½ç¶ï¼å¶ä»æ´å å®æ¶å®å°çè¾å¥æ¹å¼ä¹æ¯ä¸å¯æç¼ºçï¼æ¯å¦åå å«äººçæ¼è®²åæ¥åã
æå¸¸å¸¸èªå²ï¼æºå¨å­¦ä¹ é¢åçå¤§å¸ä»¬æå·¥ä½åçæ´»ä¸­æ»ç»åºæ¥çå²çåºç¨å°æºå¨å­¦ä¹ ç®æ³ä¸ï¼èæè¿æ ·çæ åä¹è¾ä»ä»ä»¬çå·¥ä½ä¸­é½è½å­¦å°åä¹ä¸å°½çå²çã
å°ç¥ä¹è·¯ä»»éèéè¿åï¼ååï¼&lt;/p&gt;
&lt;p&gt;æè¿å¯¹æºå¨å­¦ä¹ æäºæ´å æ·±å¥ççè§£ï¼è®¤è¯å°âè¾å¥âãâå¤çâåâè¾åºæ£éªâè¿ä¸ä¸ªç¯èï¼ç¼ºä¸ä¸å¯ã
èåæä¸æèªå·±ï¼è¾å¥ï¼éè¯»ï¼åå¤çï¼æèï¼æ­£å¨ç¨³æ­¥è¿è¡ï¼ä½è¾åºï¼è¡¨è¿°ï¼å´è¿åå¾è¿è¿ä¸å¤åã
è¿ä¹æ¯ææè¿å³å®å¹å»è®°ç¬è®°ä¹ æ¯çä¸ä¸ªéè¦åå ã
é£ä¹å¨æä¸ä½ç±å¥½çæå½±é¢åï¼âè¾åºâå°±ä¸æ¯è®°ç¬è®°æè·å«äººè¯­è¨äº¤æµé£ä¹ç®åäºï¼èæ´å¤å°åºè¯¥æ¯éè¿ææç§çæ¥è¡¨è¿°èªå·±ï¼&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æåå°è¡¨è¾¾ä½ çä¿¡æ¯ï¼æ¯åææå½±çæ ¹æ¬ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;å³äºè¡¨è¿°&#34;&gt;å³äºè¡¨è¿°&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;åçº¯å°åå«äººæ±æ¥ä½ çå°çåºæ¯ï¼é£æ¯éé¿è´£ä»»ï¼æåºæ¯æ¼ç»åºæ¥ï¼ææ¯æ¥åææãè½ç¶åºæ¯å¯è½ä¸æ¯ä½ åé åºæ¥çï¼ä½ç§çå´ä¸å®æ¯ï¼å æ­¤ï¼ä¸è¦æ­¢æ­¥äºä½ çæè§ï¼å å¥ä½ çè¯è®ºãæååå»ºè®®ï¼æå®ä»¬é½æ¾å°ç§çä¸­å§ï¼è¡¨è¾¾ä½ çè§ç¹ï¼éæä½ çç«åºï¼è®©è¯»èä¿¡æä½ çç»è®ºã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿æ®µè¯æ¯å¨è®²èºæ¯æå½±çè¡¨è¾¾æ¹å¼ï¼ä»èªå·±çå°çåºæ¯ï¼è¾å¥ï¼ä¸­æç¼åºèªå·±çè§ç¹ï¼å¤çï¼ï¼å¹¶æè¿äºè§ç¹è¡¨ç°å¨èªå·±çä½åä¸­ï¼è¾åºï¼ã
åæ¶è¿æ®µè¯ä¹éè¿°äºè¡¨è¾¾æ¹å¼çä¸ä¸ªå±æ¬¡ï¼ä¸å æç´¢æåºçå¿«ç§æ¯æåºå±çãç®åçè®°å½ï¼èå¥äºèªå·±è§ç¹æè§è§çä½åæ¯ä¸­å±çï¼è½å¤è®©è§ä¼æç½ä½ ä¼ è¾¾åºçè§ç¹ææ¯æé«å±çã
é£ä¹åä½åä½å°ä¸æ¯å¦æ­¤å¢ï¼
ç®åçè®°åæ¯æåºå±çï¼å°±ç®å¤ç¨äºåä¸½çè¯è»ï¼å¯¹åºå°æå½±ä¸å°±æ¯å¥ç¨æå¾çèå¼ãï¼ï¼ä¹è¿æ¯ä¸ç¯æ²¡æææ³çæå­ï¼è¡¨è¾¾äºèªå·±ææ³çæå­æ¯åæ¢çï¼è³äºè§ç¹è½ä¸è½è¢«è¯»èæ¥åï¼æ¯ä¸ä¸ªéè¦ä¸æ­åæçè¿ç¨ï¼æ¯èªå·±çè¡¨è¾¾æ¹å¼ä¸å¤æè¯´æåï¼è¿æ¯æ³æ³å¤ªè¶åèä¸ä¸ºä¸äººæ¥åï¼&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;äºè§£èªå·±è¦è¯´ä»ä¹ï¼ äºè§£èªå·±è¦æä¹è¯´ï¼ ç¶åæ¯«ä¸å¦¥åå°è¯´åºæ¥&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;æææçæ¯ï¼è¿ä¹æ­£æ¯æèªå·±æè¿å³äºåä½çå¿å¾ï¼å°¤å¶æ¯å¨æå¯¼ç ç©¶çæ¯ä¸è®ºæçæ¶åï¼æå¯¹ä»ä»¬è¯´åºäºå ä¹åæ ·è¯ï¼åªä¸è¿æç¬¬ä¸é¨åçè§ç¹è¦æ´æ¹å¤æ§ä¸äºï¼ç«å¨è¯»èçå®¢è§è§åº¦å»å®¡è§èªå·±ææ²¡æå¾å¥½å°ä¼ è¾¾è§ç¹ã&lt;/p&gt;
&lt;p&gt;ä»¥ä¸ä¸ç´å¨è¯´âæºå¨å­¦ä¹ âåâéè¯»âå¯¹æå­¦ä¹ ãæå½±åä½çå½±åä»¥åä»ä»¬ä¹åçå±æ§ã
åææºå¨å­¦ä¹ ï¼å¶å®ä¸é¢æå°çè¡¨è¾¾ä¸å±æ¬¡å¯¹æºå¨å­¦ä¹ çç ç©¶ä¹æ¯å·ææå¯¼æä¹çï¼ç®åçè®°è¿°ï¼autoencoderï¼æ¯æä½å±æ¬¡ï¼å·æåä½æ§çè¡¨è¾¾æ¹å¼ï¼å¯¹è¯ãæè¦ãç¿»è¯ç­ç­ï¼æ¯ä¸­å±çï¼èå¦ä½è®©æºå¨çè¡¨è¾¾æ´ä¸ºäººç±»ææ¥åï¼æ¯å¦åçãè¿è´¯ãæè¶£ï¼ï¼æ­£æ¯ç°å¨é¢ååçç ç©¶é¾ç¹ã
è¿ä¹è®¸å¯¹æºå¨å­¦ä¹ çå¤ä»»å¡è®­ç»æäºä»·å¼ï¼&lt;/p&gt;
&lt;h2 id=&#34;å³äºæå½±ççå®&#34;&gt;å³äºæå½±çâçå®â&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æè®¤ä¸ºå¤§é¨åèºæ¯å®¶æä¸»è¦è¿½å¯»çä¸æ¯çå®ï¼èæ¯ä¸ç§æ°å½çæ¹å¼ï¼ä»¥è¡¨è¾¾ä»ä»¬æçè§£ççå®ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;äººé½æ¯æææ³çï¼æææ³å°±ä¸å®ä¼æä¸»è§ï¼é£ä¹ä¸ä¸ªä½èåºäºç°å®äºç©çåä½ï¼æ è®ºæ¯æå½±è¿æ¯æå­¦ï¼é½ä¸å®æ¯ä¸»è§çã
å°±ç®ä½ è½å¤ä¿è¯èªå·±ä½åçç»å¯¹å®¢è§åçå®ï¼ä½ ä¹æ²¡æåæ³ä¿è¯è¯»èå¸¦çä¸»è§æç»ªæ¥å®¡è§ä½ çä½åï¼é£ä¹å¨ä»ä»¬ç¼éï¼ä½ è¿æ¯ä¸»è§çã
æ¯ä¸æ¯å¾æäºç¸å¯¹è®ºçææï¼ååã&lt;/p&gt;
&lt;p&gt;çæ­£çå®¢è§åçå®æ¯ä¸å­å¨çï¼ä¹ä¸åºè¯¥ä½ä¸ºç»æè¿½æ±ã
æ­£å¦æä¸é¢æè¯´çå¯¹è¡¨è¾¾æ¹å¼çåæï¼åºè¯¥å¨èªå·±çè¡¨è¾¾åå«äººçæ¥ååº¦ä¸åä¸ä¸ªåççå¹³è¡¡ï¼è¿ææ¯åä½çç²¾é«å§ã&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>æéè¦æç¡®èªå·±çæå½±ä¸»é¢</title>
      <link>https://shaojiejiang.github.io/post/zh/photography-subject/</link>
      <pubDate>Wed, 29 Apr 2020 14:17:37 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/photography-subject/</guid>
      <description>&lt;p&gt;åæ³¨ï¼å¦æ ç¹æ®è¯´æï¼æ¬æçæå½ååèª&amp;quot;æå½±çèºæ¯ (ä¸çé¡¶çº§æå½±å¤§å¸)&amp;rdquo; by Bruce Barnbaum, æ¨æºæ¯&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä½ çå´è¶£æ¯ä»ä¹ï¼åªæä½ èªå·±å¯ä»¥åç­ãä½è¿ä¸ªåç­æ¯éå¸¸éè¦çï¼å ä¸ºå¦æä½ è¦åä½ææä¹çæå½±ä½åï¼å°±å¿é¡»ä¸æ³¨äºé£äºä½ ææå´è¶£çé¢åãä¸ä»å¦æ­¤ï¼ä½ è¿å¿é¡»ä¸æ³¨äºé£äºä½ å·æå¼ºçä¸ªäººæ³æ³çé¢åã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿æ¯ä¸ä¸ªè¢«å¤§å¸ä»¬å¤æ¬¡å¼ºè°çå¿å¾ï¼å´è¶£æ¯ç¬¬ä¸å¯¼å¸ã
èå¦ææ²¡æå´è¶£ä¼æä¹æ ·ï¼
ä¸å¾ä¸è¯´ï¼ä¸é¢è¿æ®µè¯éæè¿°çæåä¼¼æ¾ç¸è¯ãã&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å¨æ¥å¸¸å¯¹è¯ä¸­ï¼ä½ æ¯å¦å°è¯è¿å¨ä½ ä¸æå´è¶£ææ²¡ä»ä¹è§è§£çä¸»é¢ä¸ï¼è¯´ä¸äºå·ææä¹çè¯ï¼è¿æ¯ä¸å¯è½çï¼ä½ æ è¯å¯è¯´ï¼å ä¸ºä½ ä¸æå´è¶£ãä¸è¿ï¼è¿ä¸è¬ä¸ä¼å¦¨ç¢ä½ ç»§ç»­è®¨è®ºãæ­£å¦äººä»¬è°è®ºæ²¡æå´è¶£çè¯é¢ä¸æ ·ï¼ä»ä»¬ä¹å¯ä»¥ææå¶ä¸æå´è¶£çäºç©ï¼èç»ææ¯ä¸æ ·çï¼æ¯ç¥ä¹å³ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;è¿ä¸ªä¾å­ä¹å¾çå¨ï¼&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä»¥ä¸ä¸ªä¼å¤§çæ¼è¯´å®¶ï¼ä¾å¦ä¸åå°æèé©¬ä¸Â·è·¯å¾·Â·éï¼ä¸ºä¾ï¼å¦ææä»¬è®©ä»ä»¬å¯¹ç¼è¢«å­è¿ä¸ªä¸»é¢ä½ä¸æ¬¡æ¿ææ´æº¢çæ¼è®²ï¼ä»ä»¬æ¯æ²¡æ³åå°çï¼ä»ä»¬æ è¯å¯è¯´ï¼å ä¸ºè¿ä¸æ¯å¶è¯é¢æå¨ãå¶æ¿ææå¨ãä»ä»¬éè¦å¨èªå·±çä¸»é¢ä¸å±ç¤ºä¼å¤§çæ¼è¯´æååè¯´ææå·§ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;å¤§å¸ä»¬çä½åä¸è¬é½æ¯é£æ ¼éå¸¸ä¸è´çï¼å ä¸ºä»ä»¬å¾ä¸æ³¨ã
èæ­£æ¯ä¸æ³¨ï¼æè®©ä»ä»¬æä¸ºå¤§å¸ã
ä¸çä¸çæ²åé£ä¹å¤ï¼ä½å¦æä»ä»å±éäºèµ°é©¬è§è±ä¼¼å°å»æ¸¸è§è¿äºåå°ï¼èä¸æ¯éæ©ä¸åè¿è¡æ·±èï¼ä¹æ¯ä¸ä¼æä»»ä½æ¶è·çï¼è¿åæ¶ä¹æ¯æè¿èªå·±å¯¹ç§ç çä¸äºææã
ç°å¨æ¢æ¢è®¤è¯å°ï¼æè°çâéè§é¾âï¼ä¸è¬é½æ¯å¤å¨èµ·è·çº¿ä¸ç¹è±«çéæå¯¹äºåéæèçççæå¹ã
çæ­£å¨èµéä¸å¥é©°çä¸ä¸è¿å¨åï¼æ¯ä¸ä¼ææ¶é´åå¿æå»æèè¿äºçï¼å ä¸ºå¯¹äºä»ä»¬æ¥è¯´ï¼ä»ä»¬çé¢åè·âä¸æ¸¸âåâä¸æ¸¸âé¢åä¹é´çåºå«ä»ä»æ¯èµåºçä¸åï¼èæ¯ä¸ªèµåºé½æè¶³å¤æ¿ççæ¯èµæ¥ä¾ä»ä»¬å¿ç¢ãæè¶³å¤å¤§çè£èªæ¥ä¾ä»ä»¬äºåã
ç¨Stephen Coveyçè¯æ¥è¯´ï¼è¿å«å¯è¶³æç»´ï¼abundance mentalityï¼ã&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;èä¼å¤§çæå½±å®¶åç¥éä»ä¹æ¯ä»ä»¬æå´è¶£çãä»ä¹æ¯ä»ä»¬è§å¾ä¹å³çï¼ä¹è½è®¤è¯å°èªå·±çå¼ºé¡¹åå¼±é¡¹ï¼å¹¶ä¸äºèªå·±çå´è¶£åå¼ºé¡¹ãä»ä»¬ä¼å®æå°å¨å¶ä»é¢åè¿è¡ä¸äºå°è¯ï¼æ¥æ©å¤§èªå·±çå´è¶£èå´å¹¶æ¹è¿ä»ä»¬çå¼±é¡¹ï¼ä½ ä¹åºè¯¥è¿æ ·ï¼ï¼ä½ä»ä»¬ä¸ä¼æå°è¯æ§çææåä¸¥èæ·±å»çè¡¨è¾¾æ··æ·ã é¦æ¯é¡¿ä¸ä¼ææç¬é´åççäºæï¼çº½æ¼ä¸ä¼ææé£æ¯ç§ï¼å°¤æ¯æ¼ä¸ä¼ææä¸å¹¸çç¤¾ä¼æåï¼é¿åä¸ä¸ä¼å°å¶åºè¶ç°å®ææçå¤éå½±åãä»ä»¬çæ¯ä¸ä½é½ä¸æ³¨äºèªå·±å´è¶£æå¤§ãæ¬é¢æå¼ºçé¢åãä»ä»¬æè®¸å¯ä»¥å¨å¶ä»é¢ååä½åºä¸éçä½åï¼ä½è¿äºä½åçæ°¸ææ§åå²å»åä¼å¤§æææ£ãä»ä»¬ï¼è¿æå¶ä»ä¼å¤§çæå½±å®¶ï¼é½ç¿æºå°å³å®å¨ä»ä»¬ææé¿çé¢åååä½ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;åå°æ­£é¢æå½±ä¸ã
æä»ä¸å¹´ååè®¤çå¯¹å¾æå½±ä»¥æ¥ï¼è¿æ²¡æçæ­£å¯¹ä»ä¹ä¸»é¢æå´è¶£è¿ï¼æèè¯´æ²¡æåæä¸æ¥ã
åå¼å§æ¯å¸¦ç18-200mmå¥æºéå¤´ï¼å¨å¤§è¡å°å··éæ¼«æ ç®çå°ç©¿æ¢­ï¼ä¸ç¥éèªå·±æ³æä»ä¹ã
å°±ç®å¶å°çå°æè§æææçä¸è¥¿ï¼ä¹ä¸ç¥éè¯¥æä¹æå®ä»¬è®°å½ä¸æ¥ã
å ä¹ææçé¢æé½æ¯ä¸æ¶å´èµ·ãæµå°è¾æ­¢ï¼å°¤å¶æ¯å¨ä¹°äºæ°è®¾å¤ä¹åæä¸äºéåæ°éå¤´çä¸»é¢ã
æ¯å¦å½æ¶ä¹°äº85mm f/1.8éå¤´åä¸èæ¶ï¼ä¹å¨æäºä¸ä¸¤æ¬¡èªæç§ä¹åä¾¿åæ²¡æå°è¯è¿ï¼è½ç¶åæ¥æè¿ä¸ä¸¤æ¬¡çå²å¨ï¼ä¹°äº70-300mmé¿ç¦éå¤´ä¹å°±æäºä¸ä¸¤æ¬¡é¸ï¼ä¹°105mmå¾®è·åªæäºå æ¬¡èèï¼ä¹°10-20mmå¹¿è§éå¤´ä¹åªæäºå æ¬¡å»ºç­ã
è¯´æ¥å®å¨æ­æ§ï¼å¾®è·åå¹¿è§éå¤´è®©èªå·±æ»¡æçä¸¤å¼ ç§çï¼è¿é½åå«æ¯å¨åä¹°è¿ä¸¤æ¯éå¤´é£ä¼æä¸çï¼





  
  











&lt;figure id=&#34;figure-èè105mmå¾®è·&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;èèï¼105mmå¾®è·&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;600&#34; height=&#34;4000&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    èèï¼105mmå¾®è·
  &lt;/figcaption&gt;


&lt;/figure&gt;






  
  











&lt;figure id=&#34;figure-ç®­å¤´10-20mmå¹¿è§&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;âç®­å¤´âï¼10-20mmå¹¿è§&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;5816&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    âç®­å¤´âï¼10-20mmå¹¿è§
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;å¶å®å¾å¤æå½±é¢æé½æå¸å¼æçå°æ¹ã
ææææ¸¸é¢æå¯ä»¥æ¶è·åå°çç¾æ¯ç§ï¼ä½æ¯åæéè¦å¯¹ç®çå°çäººæãåå²çè³å¤©æ°åäº¤éç­é½è¦æåè¶³çäºè§£ï¼ææå¾®è·å¯ä»¥ææ¸ºå°çäºç©ä»¥éæ¼çè§åº¦å±ç¤ºåºæ¥ï¼åææ¯ææè¶³å¤çèå¿å»åç»è´å¥å¾®çè§å¯ï¼é¿ç¦æå½±å¯ä»¥è®©ææç¾å¦çéçå¨ç©æè¿å°ç¼åå¹¶å±ç°ç»è§ä¼ï¼ä½æ¯é¾åº¦ä¸äºäºçå»ä¸ä¸ªä¸ç¡®å®çç®æ ï¼èä¸åæå¯¹å¨ç©ä¹ æ§çäºè§£ä¹æ¯å°ä¸äºçï¼é£æ¯æå½±å¯ä»¥å±ç¤ºäººç±»å»ºç­ææ¯å¤§èªç¶ç¾æ¯çéæ¼ï¼ä½æ¯åæè§åº¦çéåãæä¹é¿å¼äººç¾¤æèè®©ä»ä»¬å¾å¥½å°èå¥ç»é¢ãå¨ä»ä¹æ ·çå¤©æ°ææç­ç­ä¹æ¯è®©æå¾å¤´å¤§ï¼èäººåæå½±ä¸ä»éè¦èªå·±çå®¡ç¾è§ç¹ï¼è¿éè¦å»ºç«å¥½è·æ¨¡ç¹çå³ç³»æäº¤æµââå¦ææææ¨¡ç¹çè¯ã&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å¨æ´æ·±çå±æ¬¡ä¸ï¼é¤éæå½±å¸åä¸»è§ä¹é´æçåå¥½çå³ç³»ï¼å¦åä»»ä½å°è¯é½ä¸å¯ä»¥ä¸ºä½ çèåæå½±å¢è²ï¼å³ä½¿æ²¡æåå¥½çå³ç³»ï¼ä¹è³å°æå®è´¨æ§çæ²éæå¼ºççç¬¬ä¸å°è±¡ï¼ãæå½±å¸åºè¯¥è®¤è¯ä¸»è§ï¼å¯¹ä»æå´è¶£ï¼å¯¹ä»æä¸å®ççæ³ï¼å¹¶åªåæä¸»è§çä¸ªæ§ä»¥æå¼ºççæ¹å¼è¡¨è¾¾åºæ¥ãææ¶ï¼æå½±å¸å¿é¡»å¼ºçä¾èµäºç¬¬ä¸å°è±¡ï¼å ä¸ºä»å¾å¾å¾é¾è±ä¸è¶³å¤çæ¶é´æ¥å®å¨äºè§£ä¸»è§ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;æ»èè¨ä¹ï¼ä»»ä½é¢æä¸å¥½çä½åä¼¼ä¹é½ç¦»ä¸å¼åæåå¤§éçåè¯¾ã
æææ¾æ¯æå½±æåº¦è¿ä¸å¤ç«¯æ­£ï¼æ³è¦å¥½çç§çåæ»å«å¼è¿ç¨å¤ªè¾è¦ï¼è½éä¸å¿æ¥çäºæå½±ææä¹æ¯ä»åä¸ä¹æå¼å§çã&lt;/p&gt;
&lt;p&gt;æ²¡æä»ä¹å¥½çç§çæ¯é è¿æ°å¾æ¥çï¼å°±ç®æï¼ä¹éè¦æåæåºæ¬åç»å¥½æè½æä½ç¨çºµå³éçæºä¼ã
ææ æ³å¿è®°åä¹°D7200çæ¶åï¼ç°å®ç»æçä¸æ¬¡æè®­ã
å½æ¶æ¿çåå°æçæ°ç¸æºï¼å´è´å²å²å°å°±ä¸è¡åæ¯äºï¼ç¶åæä¸äºè¿å¼ åæ¥å¾å¿«æè¯å°é®é¢å¾å¤çç§çï¼





  
  











&lt;figure id=&#34;figure-ç½çº¢å¢ä¸ç¡è§çç«æªè°ä¿®&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;ç½çº¢å¢ä¸ç¡è§çç«ï¼æªè°ä¿®&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;600&#34; height=&#34;4000&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ç½çº¢å¢ä¸ç¡è§çç«ï¼æªè°ä¿®
  &lt;/figcaption&gt;


&lt;/figure&gt;

é£å¤©æå¾å¹¸è¿ï¼å ä¸ºå¶éäºè¿é¢ç½çº¢å¢ãå½æ¶è¿ä¸ç¥éè¿é¢å¢å°æåæ°ï¼åªæ¯è§å¾è¿å¥è¯å¾éç®ãå¾æä¸ªæ§ï¼æå³é®çæ¯ä¸é¢é¿å³ä¸æåªç¡è§çç«ï¼å°±åæ¯æäººä¸é¨å®æçä¸æ ·ï¼
ç¶èé£å¤©æä¹æ¯å¾ä¸å¹¸çï¼å ä¸ºé¦åæå½æ¶æç§ä¸ä¼æå¾ï¼æç»é¢éç¹çç«æ¾å°äºå·¦ä¸è§éå¸¸è¾¹ç¼çä½ç½®ï¼å¶æ¬¡æ²¡æè¶³å¤äºè§£ç¸æºçè®¾ç½®ï¼æä»¥æ²¡æè®°å½ä¸æ¥æ æçç§çï¼ç»åæè°æ´å¸¦æ¥äºå¾å¤§çéå¶ã
æä»¥å½ææè¿å¼ ç§çåäº«å°ç¤¾äº¤åªä½ä¸ä¹åååå¹³å¹³ï¼çè³å¾å¤äººé½æ²¡æ³¨æå°é£åªå¨é´å½±éçç«ï¼åèè¢«å æ®ç»é¢å¤§é¨åçãé«å¯¹æ¯çå¢å¸å¼äºã
å¶å®å³ä¾¿æç°å¨æè¿æ ·çæºä¼ï¼ä¹è¿æ¯å¾é¾æå¥½è¿ä¸ªåºé¢çï¼å¢ä¸çæ¶é¸¦å¤ªå¤§ãå¤ªéç®ï¼ç«çä½ç½®ä¸åéï¼å¾é¾ä½ä¸ºä¸»ä½è¢«çªåºã
å½ç¶ï¼è¦æ³æçæ¯æè¿å¼ å¥½ï¼è¿æ¯å¾å®¹æçï¼æ¯ç«è¿å¼ ç§çå·²ç»ä¸è½åçäºï¼ååï¼
æå½å¤©æäºå¥½å¤å¼ ï¼å¶ä¸­ä¹æäºè§åº¦å¥½ç¹çï¼ä½æ å¥å½æ¶èªå·±å¤ªå»ï¼æé£äºç§çä»¥âéåº¦ä¸å¤âå æäºããã
é½æ¯æ°æå­¦è´¹åï¼
è¿æ ·çäºæå½ç¶æ¯å°æçå¯æã
ä½å¦ææä¸ç»å¥½åºæ¬åãä¸æ¾å°èªå·±çæå½±é£æ ¼ï¼æè¿ä¼ç»§ç»­æµªè´¹å¿«é¨æ°ã&lt;/p&gt;
&lt;p&gt;åæ¥æåå¤æ¬¡æè®¿è¿è¿é¢å¢ï¼å¯æåä¹æ²¡è§å°æç«å¨ä¸é¢ç¡è§äºãã&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;åè®°ï¼å¨åç»­å¯¹æ¬ä¹¦çå­¦ä¹ ä¸­ï¼çå°äºä½èå¯¹å¦ä¸ä½åçä»ç»ï¼è®©æéæ°çå¯¹ç«é£å¼ ç§ççä¸ç¹ä¿¡å¿ã&lt;/p&gt;















&lt;figure id=&#34;figure-ä¹å¥³-by-bruce-barnbaum&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://images.squarespace-cdn.com/content/v1/563fac1de4b07f78f2db1c2c/1447459082594-OAR3ZKKQIR87D1PII2Y9/ke17ZwdGBToddI8pDm48kJcL8RUadGdk4gpl41YTwHNZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PICa0vOBoO_YRM0aI4T8IW9lHW4ggziL-I7oURYsi2vL8KMshLAGzx4R3EDFOm1kBS/TheBeggarWoman.jpg?&#34; data-caption=&#34;ä¹å¥³ by Bruce Barnbaum&#34;&gt;


  &lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/563fac1de4b07f78f2db1c2c/1447459082594-OAR3ZKKQIR87D1PII2Y9/ke17ZwdGBToddI8pDm48kJcL8RUadGdk4gpl41YTwHNZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PICa0vOBoO_YRM0aI4T8IW9lHW4ggziL-I7oURYsi2vL8KMshLAGzx4R3EDFOm1kBS/TheBeggarWoman.jpg?&#34; alt=&#34;&#34; width=&#34;600&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ä¹å¥³ by Bruce Barnbaum
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;è½ç¶ä¹ä¸æ¯è§è§ä¸­å¿ï¼ä½ä½ ä¸ä¼ç«å»å°±è½åç°å¥¹ãå¥¹å¤ªå°äºï¼æ æ³é©¬ä¸å°±çå¾å°ãä½åªè¦ä½ åç°äºï¼å½±åçæ§è´¨å°±å®å¨æ¹åäºã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ä»Barnbaumçè¿å¹ä½åéï¼æè®¤è¯å°ææ¶åå¯¹ä½åçä¸äºæç ´å¸¸è§çè§£éæ¯æå¿è¦çï¼æ¢ç¶å¤§å¸å¯ä»¥æä¸»ä½æ¾å¨ä¸æ¾ç¼çä½ç½®ï¼é£ä¹æä¸ºä»ä¹ä¸å¯ä»¥å¢ï¼
åªè¦æç»è§èè¶³å¤çå¼å¯¼åè§£éï¼æ¯å¦è¿éæ é¢åªæç«ï¼ç­è§èåç°ç«çæ¶åï¼ä¾ç¶å¯ä»¥ä½ä¼å°æå½æ¯çå°è¿ä¸ªåºæ¯æ¶çæåã&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-ç¡è§çç«æ¢æç&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;ç¡è§çç«ï¼æ¢æç&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;400&#34; height=&#34;4000&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ç¡è§çç«ï¼æ¢æç
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;è¿éå¯¹ç§çè¿è¡äºè£åªåé»ç½å¤çä»¥è¿æ»¤æé¢è²å¹²æ°ï¼å¹¶åäºsplit toningä»¥ä½¿ç«åå¢çè²å½©ç¥æåç¦»ã
å½ç¶ï¼è¿éå«å®âæ¢æçâï¼æ¯å ä¸ºå®ç¦»ä¸å¹å¥½çä½åä¾ç¶è¿å¾è¿ã
å¦ææåæè¿æ ·çæºä¼ï¼æä¼ææºä½å·¦ç§»ï¼ä»¥å°½éæç«æ¾å¨æ´æ¾ç¼çä½ç½®ã&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Computation Time</title>
      <link>https://shaojiejiang.github.io/post/en/adaptive-computation-time/</link>
      <pubDate>Tue, 28 Apr 2020 10:46:44 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/adaptive-computation-time/</guid>
      <description>&lt;p&gt;My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;additive-vs-multiplicative-halting-probability&#34;&gt;Additive vs multiplicative halting probability&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Multiplicative:&lt;/strong&gt; In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time.
It is acknowledged by the authors that using the logits $h_n^t$ as the halting probability at step $n$ might be more straightforward.
Therefore, the overall halting probability is calculated as $$p_t^n = h_t^n \prod_{u=1}^{n-1} (1 - h_t^u).$$
We use $(1 - h_t^u)$ for previous update steps to indicate that the updating is *not* stopped until $n$.&lt;/p&gt;
&lt;p&gt;As each $p_t^n \in (0, 1)$ is relatively independent with each other and $\sum p_t^n$ is not bound to 1, this approach &lt;em&gt;does not&lt;/em&gt; restrict the update depth to grow arbitrarily.
The model can be of course trained to lower the expected ponder time $\rho_t = \sum n p_t^n$, but it is observed in the experiments that the resulting model is not preferable in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$h_t^1$ is usually just below threshold, intermediate $h_t^n = 0$, and final $h_t^N$ is high enough to halt the update.&lt;/li&gt;
&lt;li&gt;as the expectation is low, $p_t^N \ll p_t^1$, but the network learns to have a much higher magnitude of output states at step $N$, so that the final output is still dominated by the final state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Additive:&lt;/strong&gt; In contrast, the additive approach have an constraint of $\sum p_t^n = 1$, so that the probability is decreased monotonically with the number of updates growing larger.
Though being non-differentiable, the total ponder time (total updates at all positions) is penalized to avoid consuming unnecessary computation.
There is still one drawback of this approach, however.
The performance is sensitive to the penalty factor $\tau$, which is not intuitive to choose as a hyperparameter.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1603.08983&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Computation Time for Recurrent Neural Networks&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>å¯æ©æ¯ä¸»ä¹ââä¸­å½ä»¥åºå»ºä¸ºæ ¸å¿çç»æµç­ç¥çè®ºèæ¯</title>
      <link>https://shaojiejiang.github.io/post/zh/keynesianism/</link>
      <pubDate>Mon, 27 Apr 2020 22:18:24 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/keynesianism/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æ¶å¥ç­äºäººä»¬çéæ±æ¶è´¹çæ»éã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;æèµââè´­ä¹°åæ¿ãè®¾å¤ç­ââç­äºå°æ¯åºâæ³¨å¥âç»æµä¸­ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å¨è¿éï¼æ°´é¾å¤´æå©æ¯ï¼å³åè´·çä»·æ ¼ãå½å©æ¯éä½ââæå¼æ°´é¾å¤´ââåè´·åå¾ä¾¿å®ï¼æ´å¤çäººä¼è¿è¡è´·æ¬¾ã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä¸çåæ»¡äºä¸ç¡®å®ï¼äººä»¬å¹¶ä¸ä¸å®è¦å°èªå·±çå¨èä¸åæ¿åå·¥åæé©ãæè®¸ä½ å¯è½åªæ³æé±æ¾å¨åºå«ä¸é¢ä»¥å¤ä¸æ¶ä¹éãå¨å¯æ©æ¯çæ¥ï¼å©æ¯å¹¶ä¸è½æå©äºå°å¤ä½çå¨èè½¬ä¸ºæèµãäºå®ä¸ï¼å¨èåæèµä¹é´å¹¶æ²¡æå³èã&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;å¯æ©æ¯è®¤ä¸ºï¼å½æµåºéå¤§äºæµå¥éæ¶ä¾¿ä¼åçç»æµè¡°éã&amp;rdquo; (from &amp;ldquo;ç»æµå­¦éè¯è¯¾ï¼è¶é²å¤§å­¦åºåï¼è¶é²å¤§å­¦ç»æµå­¦å¥é¨è¯¾ï¼æ®éäººä¹è½è¯»æçç»æµå­¦ï¼çè®ºå°ç°å®ï¼æ­èµ·ç¨ç»æµå­¦æ¹åç°å®çæ´»çæ¡¥æ¢ ) (åéç»ç®¡åå¡å¿è¯»ç³»å)&amp;rdquo; by å°¼å°Â·åºä»ç¹å°¼, å¼ ç¼, åå©§)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;ä¼ ç»ç»æµå­¦è®¤ä¸ºï¼ä¸ä¸ªå½å®¶çæ¶å¥ç­äºç»æµäº§è½ï¼ä½å¯æ©æ¯è®¤ä¸ºæ¶å¥ç­äºäººä»¬çæ¶è´¹æ»éãæ¶è´¹ç­äºä¸ºç»æµæ³¨å¥æ´»åï¼èæ²¡æå¾å°å©ç¨çå¨èæå³çæ´»åçæµå¤±ãå¦ææ æ³é»æ­¢äººä»¬æé±é½å­èµ·æ¥ï¼å¹¶ä¸æ æ³ææå©ç¨äººä»¬çå¨èï¼é£ä¹ç»æµè¡°éå°±ä¼åçã&lt;/p&gt;
&lt;p&gt;ä¹è®¸ä¸­å½çç»æµå­¦å®¶æ©å°±è®¤å¯äºè¿ä¸ªçè®ºï¼æä»¥ä»ä»¬å¶å®äºä¸ä¸ªç±åºç¡å»ºè®¾ä¸ºæ ¸å¿çç»æµåå±ç­ç¥ãä¸­å½çäººæ°åå¾åæ¬¢å­é±ï¼ä»ä»¬å­èµ·æ¥çé±ä¸é¨åè¢«ç¨æ¥æèµå¬å±åºç¡è®¾æ½ï¼å¬è·¯ãéè·¯ãçµåãæ°´å©ãäºèç½ç­ç­ï¼èä¸è¿äºè®¾æ½æå©äºä¸­å½å¨é¢çå°¤å¶æ¯åéå°åºçç»æµåå±ãæ­¤å¤ï¼ç±å¨é¢åå±å¸¦ççç»æµè¿æ­¥ï¼ä¿ä½¿äººä»¬æ¥æäºæ´å¤çå¨èãä»ä»¬çå¨èå¹¶ä¸ä¼æ°¸ä¹å¢å ï¼å ä¸ºå½ä¸çç¤¾ä¼é£æ°è¿«ä½¿ä»ä»¬æå ä¹ææçå¨èé½ç¨å¨äºä¹°æ¿ãä¹°è½¦ãå»èãå»çãææ¸¸ä»¥ååä»£æè²ä¸ãäººä»¬æ£å¾è¶æ¥è¶å¤ï¼èä¸ä¸­å½çç»æµå­¦å®¶ä»¬æ»æåæ³è®©äººä»¬ææéçé±è±åºå»ï¼äºæ¯ä¸­å½çç»æµæä¼æç»­ä¸æ­å°çåæ°æ´»åã&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>æ¼å¤å¤æè®¸é ç¦äºä¸­å½çè´«ä¸ä¸­åä»¥åç»æµä¸ç§æ</title>
      <link>https://shaojiejiang.github.io/post/zh/pinduoduo/</link>
      <pubDate>Mon, 27 Apr 2020 22:03:10 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/pinduoduo/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;ä¼ä¸å®¶è·å¾æåçåæ¶ä¹å¾å°äºè´¢å¯ãä»ä»¬çæ°ååå¨ç»æµä½ä¸­ä¼ æ­ï¼äººä»¬åç°èªå·±æ³è¦ä¸ä¸ªçå£°æºæçµè§ï¼ä¾¿åºé¨è´­ä¹°ãäº¨å©Â·ç¦ç¹åå®å¾·é²Â·å¡ååºåå«é ççäº§éç¨äºå¤§ä¼çå»ä»·æ±½è½¦åå¨é¢éå¶é ä¸­å¼å¥æ°æ¹æ³èåè´¢è´å¯ã å¾å¿«ï¼ä»¿æèä»¬å¼å§ä»¿ææåçä¼ä¸å®¶ï¼çäº§åºäºåæ ·çæ±½è½¦ãççæææãæ°åååææ¯ä¼ æ­å°æ´è¿äºï¼è¿å¼èµ·äºæ´ä¸ªè¡ä¸çåé©ï¼å¹¶æ©å¤§äºç»æµä½éãæç»ï¼ä¸äºä¼ä¸åé­ï¼ç»æµå¼å§èç¼©ï¼ç´è³æ°ä¸è½®åæ°åºç°ãèµæ¬ä¸»ä¹çè£è¡°ä¸æµ®æ²é½æºèªå±åºä¸ç©·çåæ°æµªæ½®ä»¥ååä¸åæ¨¡ä»¿çæ¶é¿ã&amp;rdquo; (from &amp;ldquo;ç»æµå­¦éè¯è¯¾ï¼è¶é²å¤§å­¦åºåï¼è¶é²å¤§å­¦ç»æµå­¦å¥é¨è¯¾ï¼æ®éäººä¹è½è¯»æçç»æµå­¦ï¼çè®ºå°ç°å®ï¼æ­èµ·ç¨ç»æµå­¦æ¹åç°å®çæ´»çæ¡¥æ¢ ) (åéç»ç®¡åå¡å¿è¯»ç³»å)&amp;rdquo; by å°¼å°Â·åºä»ç¹å°¼, å¼ ç¼, åå©§)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;è¯ç¶ï¼æ¼å¤å¤ä¹è®¸å¨ææ¯ä¸ãè¥éä¸å¹¶æ²¡æä¸ºåè¡ä»¬å¸¦æ¥ä»ä¹åæ°ï¼èä¸è¿ç ´åäºå¾å¤å·¥èªé¶å±å¯¹ç½è´­å¹³å°çå°è±¡ï¼ç¶èä¹è®¸è¿äºé½åªæ¯è´é¢ä½ç¨ãæ´ä½ä¸æ¼å¤å¤ä½¿å¾æ¹ä¾¿çç½è´­æå®æ åå°åè¿çä¹¡æå®¶åº­éï¼å¯ä»¥è¯´æå¤§åäºå½åç½è´­äº§ä¸çå©ç¨çï¼çè³è¿å¯è½ä¼å¸¦å¨ä¹¡æç»æµçåå±ã&lt;/p&gt;
&lt;p&gt;å°±åå¼æéæå°çç¦ç¹ãä¹è®¸ä»åå¼å§ä¹å ä¸ºçäº§å»ä»·çæ±½è½¦èä¸ºè´µææä¸é½¿ï¼ä¹è®¸ä»çæ±½è½¦ä¹ä¼¤è¿ä¸æ¶è´ªå¾ä¾¿å®çè´µæçå¿ï¼ä½ä¸å¯å¦è®¤çæ¯ï¼æ­£æ¯ä»çè¿ç§åªåæä½¿å¾æ±½è½¦æä¸ºå¹³æ°å¤§ä¼çäº¤éå·¥å·èä¸æ¯è´µæç¨æ¥ç«èçå¥¢ä¾åãé£ä¹æ¼å¤å¤ä¹æ¯æ¿æäºè¿æ ·ä¸ä¸ªè§è²ã&lt;/p&gt;
&lt;p&gt;ä½ä¸ºçå¨æ°æ¶ä»£çæä»¬ï¼äº¦ææ¯çæ´»å¨å¤§åå¸ãåå¨èéåå¬æ¡åçæä»¬ï¼å¯è½æ æ³çè§£å¨ç½è´­é¢åå·²ç»ææ·å®å¤©ç«äº¬ä¸ç­å¹³å°è¦çäºä»å»ä»·å°åè´¨åä¸ªä»·æ ¼åºé´ï¼ä¸ºä»ä¹è¿ä¼ææ¼å¤å¤æ¥è¿ä¸æ­¥æä½åè´¨ä¸ä»·æ ¼çä¸éãæä»¬ä¹æä»¥ä¼æè¿æ ·çæ³æ³ï¼æ¯å ä¸ºæä»¬å¨ä¸ç½è´­ä¸èµ·æé¿ï¼æä»¬æ¥åäºç½è´­åæ¶ä¹æå°±äºç½è´­ãä¹ æ¯æèªç¶çæä»¬å¿½è§äºæä¸ä¸ªç¾¤ä½ä¸ç´é½ä¸å¨æä»¬ä¸ç½è´­å½¢æçå±çä½ä¸­ââé£äºçæ´»å¨ä¹¡æçè´«è¦åæ°ä»¬ã&lt;/p&gt;
&lt;p&gt;ä»ä»¬ä¸è½åæä»¬ä¸æ ·å¾å¿«éåºæ°ç§æãæ°æ½®æµçåå±ï¼æ´ä¸è½åæä»¬ä¸æ ·å¨ä»·æ ¼ä¸åè´¨ä¹é´éæ§éæ©ãè®°å¾ä¹åå¨ä¸ç¯æ¶äºç¹è¯éï¼ãæ¯æ¥äººç©ãå³äºæ¼å¤å¤çç¹è¯ã
&lt;a href=&#34;https://mp.weixin.qq.com/s/kfj6cAIsajiyumnURNxpeQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ä»ä»¬ï¼å¨æ¼å¤å¤ä¸æ¼è¿æ°&lt;/a&gt;ããæè®¶å¾åç°æç« å·²ç»æ¯2018å¹´8æçäºï¼æææè®°å¾æ¯å»å¹´çè¿çåããï¼çå°è¿ï¼æä»¬ç¼éçååä¼ªå£äº§åï¼å¯è½æ¯é£äºç©·äººç¼éçä¸æ¬¡æ¶è´¹åçº§ãä»ä»¬ä¸ä¼ç¨æ·å®å¤©ç«äº¬ä¸ï¼ä¹æ²¡æå¨åå»å­¦ä¹ å¦ä½ä½¿ç¨ââæ¯ç«ä»ä»¬éæ±ä½ï¼å°±ç®æéæ±ï¼è¿äºå¹³å°çååä¹å¯è½ä¼è¶åºä»ä»¬çï¼å¿çï¼æ¿åèå´ãè¥ä¸æ¯æ¼å¤å¤ä¾é ä½ä»·ãäº²åå¸®å¿ç ä»·ç­ç­ç­ç¥è®©è¿ä¸ªç¾¤ä½æ¥åè¿ç§æ°çæ¶è´¹æ¹å¼ï¼ç½ç»è´­ç©ä½æ¶æè½æ¸éå°ç¤¾ä¼çç¥ç»æ«æ¢¢éå»ï¼æ¼å¤å¤æ¯ä¸ä¸ªæ¨¡ä»¿èï¼ä¹æ¯ä¸ä¸ªæ¨å¹¿èãä¸è¡æ·±åææ¯å¯¹ç¤¾ä¼åé©ä¸å¯ç¼ºå°çåéã&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Hub for Transformer Blogs and Papers</title>
      <link>https://shaojiejiang.github.io/post/en/transformer-blog-paper-hub/</link>
      <pubDate>Mon, 02 Mar 2020 14:26:59 +0100</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/transformer-blog-paper-hub/</guid>
      <description>&lt;p&gt;This is a growing list of pointers to useful blog posts and papers related to transformers.&lt;/p&gt;
&lt;h2 id=&#34;transformers-explained&#34;&gt;Transformers explained&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: The Illustrated Transformer&lt;/a&gt; has many intuitive animations of how transformer models work&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://mostafadehghani.com/2019/05/05/universal-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Universal Transformers&lt;/a&gt; introduces the idea of &lt;em&gt;recurrence among layers&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformer vs RNN and CNN for Translation Task&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gnns-similarities-and-differences&#34;&gt;GNNs: similarities and differences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://graphdeeplearning.github.io/post/transformers-are-gnns/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformers are Graph Neural Networks&lt;/a&gt; bridges transformer models and Graph Neural Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;transformer-improvements&#34;&gt;Transformer improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/deepmind-releases-a-new-architecture-and-a-new-dataset-to-improve-long-term-memory-in-deep-22f4b098153&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems&lt;/a&gt; Nural Turing Machine + transformer?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TLDR: Token Loss Dynamic Reweighting for Reducing Repetitive Utterance Generation</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2020-tldr/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2020-tldr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What&#39;s New in XLNet?</title>
      <link>https://shaojiejiang.github.io/post/en/xlnet/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/xlnet/</guid>
      <description>&lt;h2 id=&#34;rip-bert&#34;&gt;R.I.P BERT&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT&lt;/a&gt; got a head shot yesterday, by another guy called 
&lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;XLNet&lt;/a&gt;.
It is reported that XLNet defeated BERT on 20 NLP tasks, and achieved 18 new state-of-the-art results.
Isn&amp;rsquo;t it impressive?
So, farewell, BERT.





  
  











&lt;figure id=&#34;figure-rip-bert&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;R.I.P BERT&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;570&#34; height=&#34;570&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    R.I.P BERT
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;is-bert-really-dead&#34;&gt;Is BERT really dead?&lt;/h2&gt;
&lt;p&gt;Since I love BERT, I decided to read the paper to find out what killed him.
While reading, I was thinking wait a minute, is BERT really dead?
After finished the paper, I was so glad to know that BERT is still well alive!
He is just wearing another coat named &lt;em&gt;Two-Stream Self-Attention (TSSA)&lt;/em&gt;, with some other gadgets!
Because:&lt;br&gt;
&lt;code&gt;XLNet = BERT + TSSA + bidirectional data input&lt;/code&gt;&lt;br&gt;
Bert you&amp;rsquo;re so tough, buddy!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at what were trying to kill BERT.&lt;/p&gt;
&lt;h3 id=&#34;two-stream-self-attention-tssa&#34;&gt;Two-stream self-attention (TSSA)&lt;/h3&gt;
&lt;p&gt;Why TSSA is needed to kill BERT?
Well, let&amp;rsquo;s first see some weaknesses BERT has.&lt;/p&gt;
&lt;p&gt;BERT is using a masked language model (MLM) training objective, which is essentially why it achieves bidirectional representation.





  
  











&lt;figure id=&#34;figure-image-sourcehttpsnlpstanfordeduseminardetailsjdevlinpdf&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Image source&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1138&#34; height=&#34;154&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;a href=&#34;https://nlp.stanford.edu/seminar/details/jdevlin.pdf&#34;&gt;Image source&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this example, both words &amp;ldquo;store&amp;rdquo; and &amp;ldquo;gallon&amp;rdquo; are intended to be predicted by BERT, and their input word embeddings are replaced by the embedding of a special token &lt;em&gt;[MASK]&lt;/em&gt;.
Usually this isn&amp;rsquo;t a problem, but what if the prediction of &amp;ldquo;store&amp;rdquo; requires knowing the word &amp;ldquo;gallon&amp;rdquo;?
That is exactly where BERT falls short.&lt;/p&gt;
&lt;p&gt;TSSA is what you can use to overcome that downside of MLM:





  
  











&lt;figure id=&#34;figure-query-stream-sourcehttpsarxivorgabs190608237&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Query stream, source&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;936&#34; height=&#34;794&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Query stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;source&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this illustration, query stream gives you the &lt;code&gt;query&lt;/code&gt; vector needed for attention calculation, and this stream is designed in such a way that it doesn&amp;rsquo;t leak the info of the word it&amp;rsquo;s going to predict, but guarantees all information from other positions.
Take $x_1$ for example: $x_1$&#39;s embedding (and hidden state) is not used at all, but embeddings and hidden states from other positions are used in each layer.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-content-stream-sourcehttpsarxivorgabs190608237&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Content stream, source&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;894&#34; height=&#34;660&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Content stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;source&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Content stream, on the other hand, gives you the &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; vectors needed for context vector calculation.
This stream uses a strategy similar to that in a standard 
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer decoder&lt;/a&gt; by masking future positions.
The only difference is that in content stream, the order of tokens is &lt;em&gt;randomly permuted&lt;/em&gt;.
For example $x_2$ is right after $x_3$, and therefore $h_2^{(1)}$ can only see the embedding of itself and that of $x_3$ (and $mem^{(0)}$), but not that of $x_1$ or $x_4$.&lt;/p&gt;
&lt;h3 id=&#34;mask-a-span&#34;&gt;Mask a span&lt;/h3&gt;
&lt;p&gt;Another difference from BERT is masking a span of consecutive words.
The reason I guess, is that this guarantees the dependence of masked words (as claimed to be what BERT can&amp;rsquo;t model).
This is not a fresh-new idea, though.
Recently there are two ERNIE papers (BERT based) that propose masking named entities (often of multiple words, 
&lt;a href=&#34;https://arxiv.org/pdf/1905.07129.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;) and/or phrases (
&lt;a href=&#34;https://arxiv.org/pdf/1904.09223.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-data-input&#34;&gt;Bidirectional data input&lt;/h3&gt;
&lt;p&gt;Another notably different thing in XLNet is the usage of bidirectional data input.
The idea (I guess) is to decide the factorization direction (either forward or backward), so that the idea of &amp;ldquo;masking future positions&amp;rdquo; used in a standard Transformer decoder can also be easily used together with XLNet.&lt;/p&gt;
&lt;p&gt;Masking a span makes XLNet look like a denoising autoencoder; but by using bidirectional data input (or masking future positions), XLNet performs more like a autoregressive language model in the masked region.&lt;/p&gt;
&lt;h2 id=&#34;closing-remarks&#34;&gt;Closing remarks&lt;/h2&gt;
&lt;p&gt;So now you probably can see the similarities and differences between XLNet and BERT.
If not, here is a quick summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of masking random words, mask a span of words&lt;/li&gt;
&lt;li&gt;Use bidirectional data input to decide which direction you treat as &amp;ldquo;future&amp;rdquo;, and then apply the idea of masking future positions&lt;/li&gt;
&lt;li&gt;To avoid leaking the information of the position to be predicted, use Two-Stream Self-Attention (TSSA)&lt;/li&gt;
&lt;li&gt;Other minor things like segment recurrence, relative positional encoding, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it doesn&amp;rsquo;t seem to be enough changes to make all those improvements.
What if BERT is also trained using the additional data (Giga5, ClueWeb, Common Crawl), will XLNet still be able to defeat BERT?&lt;/p&gt;
&lt;p&gt;EDIT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another model named 
&lt;a href=&#34;https://arxiv.org/abs/1905.02450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MASS&lt;/a&gt; employs a very similar idea.&lt;/li&gt;
&lt;li&gt;According to Jacob Devlin (author of BERT), relative positional embedding might be of great importance.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2019-improving/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2019-improving/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2018-sequence/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2018-sequence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Struck tracker via color Haar-like feature and selective updating</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2017-robust/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2017-robust/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Object tracking via dual linear structured SVM and explicit feature map</title>
      <link>https://shaojiejiang.github.io/publication/ning-2016-object/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/ning-2016-object/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/cv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/cv/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/home-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/home-zh/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/search-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/search-zh/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/search/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>

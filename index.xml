<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shaojie Jiang&#39;s Homepage</title>
    <link>https://shaojiejiang.github.io/</link>
      <atom:link href="https://shaojiejiang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Shaojie Jiang&#39;s Homepage</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://shaojiejiang.github.io/media/icon_huf1850796dc0c27e76df1b37fe2f35b33_25680_512x512_fill_lanczos_center_3.png</url>
      <title>Shaojie Jiang&#39;s Homepage</title>
      <link>https://shaojiejiang.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://shaojiejiang.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Job Invitations From Adult Industry Are Welcome, but Not in the Way You Are Thinking Of</title>
      <link>https://shaojiejiang.github.io/post/en/responsible-aigac/</link>
      <pubDate>Wed, 20 Sep 2023 20:03:07 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/responsible-aigac/</guid>
      <description>&lt;p&gt;I have received several job invitations from the adult industry.
They said they are in a blooming market with the popularity of LLMs and chatbots, which I have no doubt.
They all also mentioned &lt;a href=&#34;https://replika.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Replika&lt;/a&gt; in our chats, which was not surprising at all &amp;ndash; I did an internship there in 2021, and they had already worked on sexting for years.&lt;/p&gt;
&lt;h2 id=&#34;my-pitch&#34;&gt;My pitch&lt;/h2&gt;
&lt;p&gt;Since this is kind of a sensitive topic, I think it&amp;rsquo;s important to set the pitch first.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Although I have received higher education, I don&amp;rsquo;t despise at all the legitimate adult industry.&lt;/li&gt;
&lt;li&gt;Although I&amp;rsquo;m from a country that is very famous for its strictness over the adult industry, I have no prejudice over sex-related work, thanks to my experience in NL, Amsterdam especially.&lt;/li&gt;
&lt;li&gt;Business is business. I respect every profession as long as they are legal and moral.&lt;/li&gt;
&lt;li&gt;Sex is part of human nature, so many professions around it are moral in the right context.&lt;/li&gt;
&lt;li&gt;I respect sex-related workers even further if they donate part of their (expectedly high) profit to social good.&lt;/li&gt;
&lt;li&gt;I interned at Replika, but my work was not related to the erotic part.&lt;/li&gt;
&lt;li&gt;I have no intention of stepping into this industry yet, and the reason is that it&amp;rsquo;s not the time yet.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As you see from the teaser image, I adapted the catchword a bit, which suits my lifelong mission perfectly: I want to cleverly die as a responsible adult.
Please hear me out.&lt;/p&gt;
&lt;h2 id=&#34;my-story-with-replika&#34;&gt;My story with Replika&lt;/h2&gt;
&lt;p&gt;When I started my internship at Replika, I didn&amp;rsquo;t know that the company was working on erotic roleplay.
I received the offer because of two reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The perfect match with my PhD research topic&lt;/li&gt;
&lt;li&gt;Its emotional-support aspect, which I think Replika has been doing a decent job&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my opinion, Replika has been a very responsible company.
They separated very well the subscribe-only, erotic part from the free, normal, and emotional support part.
Besides, they were happy that some users used their app to learn English.
What more can you expect from a for-profit company/startup?
They charge nothing for the part they influence society positively; they deepen the emotional support part further for people who are suffering from the lack of intimate relationships.
Agreeably, having both adult and children-safe content in the same app can pose some harm to juveniles.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;
But dangerous as adult content to children as fire and blade, are everywhere in life.
It&amp;rsquo;s the responsibility of many parties to protect juveniles from such harmful things.
Besides, Replika is working on separating the erotic part,&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; which in my opinion is another responsible action/response.&lt;/p&gt;
&lt;h2 id=&#34;why-do-i-think-its-not-the-right-time-for-me-to-do-adult-business&#34;&gt;Why do I think it&amp;rsquo;s not the right time for me to do adult business?&lt;/h2&gt;
&lt;p&gt;The short answer is that my value can be better utilized in a way more beneficial to society.
Going against the prosperity of the LLMs and chatbots, my team at Huawei working on the exact topic is experiencing adversity.
This offered me a chance to reconsider the value of my work on a broader scope: how did I perform responsibly as an Earth citizen?
Although I believe Huawei is in general a responsible and honorable company, it doesn&amp;rsquo;t mean that every division of it is responsible.
My work more specifically, hasn&amp;rsquo;t had much chance to carry out its responsibility towards the social good.
I need a recalibration of my compass, therefore, I wouldn&amp;rsquo;t accept any distraction to my long-term goal: social responsibility.&lt;/p&gt;
&lt;h2 id=&#34;what-i-would-be-happy-to-see&#34;&gt;What I would be happy to see?&lt;/h2&gt;
&lt;p&gt;Neither am I ready to devote myself to the adult business nor is the business ready for me, it seems.
I would definitely be happy to contribute my knowledge and experience in the following aspects of the adult business:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fight human trafficking and abusing&lt;/li&gt;
&lt;li&gt;Anti-harassment&lt;/li&gt;
&lt;li&gt;Privacy protection&lt;/li&gt;
&lt;li&gt;Anti addiction&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you&amp;rsquo;re such a team, I&amp;rsquo;ll highly appreciate it if you reach out to me.
I&amp;rsquo;m a believer in compounding power: a good deed, be it small, can have an overturning effect with enough time.
Some other related industries I&amp;rsquo;m passionate about are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Educational games, so that we can help channel game-addiction into the addiction to something that brings real value to the world&lt;/li&gt;
&lt;li&gt;Healthcare, which might lead to a future of chatbots saving lives every day, which is already happening&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I want to elaborate a bit more on the anti-addiction part.
There are many bad examples in real life other than sex-related addiction.
Here I want to mention some products that are creating &lt;strong&gt;new, unhealthy&lt;/strong&gt; addictions, such as YouTube and TikTok.
Interestingly, if you search Google with the keywords &amp;ldquo;TikTok&amp;rsquo;s effort in fighting addiction&amp;rdquo;, you will see a lot of articles on using TikTok for fighting &lt;strong&gt;substance addiction&lt;/strong&gt;, but you won&amp;rsquo;t see ANY of TikTok&amp;rsquo;s effort in dealing with addiction to its own app.
I&amp;rsquo;m not happy with the finding, especially given the context that my parents, together with many friends and relatives, are TikTok addicts.
Yet these companies have spent little to no effort in fighting the unhealthy addiction they caused.&lt;/p&gt;


















&lt;figure  id=&#34;figure-google-results-of-tiktoks-effort-in-fighting-addiction&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Google results of &amp;#34;TikTok&amp;#39;s effort in fighting addiction&amp;#34;&#34; srcset=&#34;
               /post/en/responsible-aigac/tiktok_addiction_hue52b2f23ba2c37ea2f1d02bbe19bd9cf_201875_94bee70f70cc4439c7e4650a4dbea8ea.webp 400w,
               /post/en/responsible-aigac/tiktok_addiction_hue52b2f23ba2c37ea2f1d02bbe19bd9cf_201875_9b6b8eedbabaa2254ec4b62b31a69262.webp 760w,
               /post/en/responsible-aigac/tiktok_addiction_hue52b2f23ba2c37ea2f1d02bbe19bd9cf_201875_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/responsible-aigac/tiktok_addiction_hue52b2f23ba2c37ea2f1d02bbe19bd9cf_201875_94bee70f70cc4439c7e4650a4dbea8ea.webp&#34;
               width=&#34;470&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Google results of &amp;ldquo;TikTok&amp;rsquo;s effort in fighting addiction&amp;rdquo;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&#34;i-have-a-dream&#34;&gt;I have a dream&lt;/h2&gt;
&lt;p&gt;Here is how I want the world to remember me after I pass away: He is a man who cleverly died as a responsible adult.
By &amp;ldquo;cleverly&amp;rdquo;, I mean not to just die being mentally responsible, do something!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What happens when your AI chatbot stops loving you back?&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.reuters.com/technology/ai-chatbot-company-replika-restores-erotic-roleplay-some-users-2023-03-25/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI chatbot company Replika restores erotic roleplay for some users&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.today.com/health/mom-chatgpt-diagnosis-pain-rcna101843&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A boy saw 17 doctors over 3 years for chronic pain. ChatGPT found the diagnosis&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>学生课间不允许自由活动，然后呢？</title>
      <link>https://shaojiejiang.github.io/post/zh/school-problems-china/</link>
      <pubDate>Wed, 20 Sep 2023 17:41:00 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/school-problems-china/</guid>
      <description>&lt;p&gt;(English version can be found &lt;a href=&#34;https://shaojiejiang.github.io/post/en/school-problems-china/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;我带着愤怒开始写这篇博客。如果学校只是一个勾勒出我们&lt;strong&gt;能做什么&lt;/strong&gt;的地方，那么学校和监狱又有何区别呢？&lt;/p&gt;
&lt;h2 id=&#34;令我震惊的消息&#34;&gt;令我震惊的消息&lt;/h2&gt;
&lt;p&gt;我的外甥吉米于2023年9月1日刚上小学，在天津市一个郊区的学校。他不是一个很外向的孩子。如果我们几个月没见面，再见面时他会害羞得说不出话来，即使几天前我们还在电话中愉快地聊天。我安慰自己：“对于一个内向的孩子来说，害怕上学是正常的。”“他很快就能适应。”&lt;/p&gt;
&lt;p&gt;新学期开始几天后，我碰巧有机会去看他。然而，当我看到他对学校多么抵触的时候，我意识到这不仅仅是他的年龄小或内向的原因。每次他要上学时，他都会哭诉：“学校太无聊了，”“时间过得太慢”这样的话。我能理解，中国的学校并不是一个总是很有趣的地方。但是“时间过得太慢”这句话引起了我的注意：他还是一个6岁的孩子，怎么可能感到时间过得太慢呢？如果他玩得开心，他应该觉得时光飞逝才对。我感觉有些不对劲，于是我决定调查一下。&lt;/p&gt;
&lt;p&gt;我首先问他为什么不喜欢学校，难道学校不是有很多有趣的地方吗？我回忆起自己在小学玩课间玩游戏的快乐记忆。那是上世纪90年代中期，我生活在中国一个非常落后的小镇。我们没有太多的玩具可以玩，所以我们好好发挥了自己的创造力：我们玩各种互动游戏，比如捉迷藏、警察抓小偷；我们还充分利用身边能找到的材料，折纸飞机、玩摔四角（见下文）、拔老根儿（见下文）。还有很多其他的有趣的活动。我不敢说自己总体上喜欢小学时光，但想起这些游戏，笑容仍会爬到我的脸上。我问外甥你们现在没有类似的活动可以玩吗？&lt;/p&gt;


















&lt;figure  id=&#34;figure-摔四角&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://imagepphcloud.thepaper.cn/pph/image/74/93/275.jpg&#34; alt=&#34;摔四角&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      摔四角
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;注&lt;/em&gt;：摔四角游戏是一种将纸折成方块的竞技游戏。在游戏中，玩家轮流在地上、对手的方格上或者旁边摔自己的四角。如果能打翻对手的四角，你就赢得本轮并且赢走对手的四角。玩四角厉害的可以在一个赛季赢下厚厚一摞。&lt;/p&gt;
&lt;/blockquote&gt;


















&lt;figure  id=&#34;figure-拔老根儿&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-fe60a2a26a106cabc95866545f739fd7_720w.webp&#34; alt=&#34;拔老根儿&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      拔老根儿
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;注&lt;/em&gt;：拔老根儿通常用树叶梗或者草棒进行比赛。像上图那样，以扯断对方的老根儿为乐。这跟大男孩们喜欢的MMA有异曲同工之妙——你要么一直赢下去，要么就报废。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当我听到他们课间不让自由活动时，我感到非常震惊。他们课间必须呆在教室里，&lt;strong&gt;除非需要喝水或上厕所&lt;/strong&gt;。是的，你没看错。不能在走廊里追逐打闹，更别提去操场上玩。感到难以理解，我建议他带上智能手表去上学，这样我可以在课间休息时给他打个视频电话，直观感受一下他们课间大家是个什么状态。但我发现智能手表在他入学的第一天就被禁了。我继续鼓励他，建议他带一些玩具，这样他至少可以开心一些。但是很抱歉，玩具当然也是被禁止的。&lt;/p&gt;
&lt;p&gt;我仍然没有放弃，开始向我的朋友和亲戚们打听，希望吉米的学校只是个案。很快我得到了一些回复。有几个亲戚说这件事很常见，而且在北京情况更严重。另一个朋友，现在是我老家一所小学的老师，表示这就像COVID-19一样——已经成了新常态。嗯，现在我需要有人来鼓励我了。&lt;/p&gt;
&lt;h2 id=&#34;体育课&#34;&gt;体育课&lt;/h2&gt;
&lt;p&gt;绝望之外，一丝丝的慰藉是吉米每周四天有体育课。
此外，在每天早上30分钟的大课间里，他们会做广播体操。
这是每天&lt;strong&gt;唯一一次&lt;/strong&gt;允许他们在操场和校园里活动的时间。
我和我姐姐决定对这些户外活动的质量进行一些调查。
我们也满怀乐观地认为，在广播体操之后，学生们可能有机会在户外自由玩耍一会儿。
然而，现实让我们再次感到惊讶。&lt;/p&gt;
&lt;p&gt;下面是我在体操结束后拍摄的一段视频。
孩子们被有序地带回教室。
在校园的另一个角落里，一位老师训诫她的学生：“进入走廊之后不要说话。迅速喝点上厕所。不要让我重复！”&lt;/p&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://shaojiejiang.github.io/post/zh/school-problems-china/media/gymnastics.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;第二段视频记录了他们在体育课上进行的“非常激烈”的体育活动。
他们在40分钟的时间里走了整整三圈，这解释了为什么他们在不走路的时候需要一直好好地休息。
哦对了，他们其他时间还在练习怎么站队，旁边的二年级或者更高年级（因为他们已经有校服了）也是整节课都在打磨队伍的整齐性。











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://shaojiejiang.github.io/post/zh/school-problems-china/media/pe_class.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/p&gt;
&lt;h2 id=&#34;原因&#34;&gt;原因&lt;/h2&gt;
&lt;p&gt;你可能觉得这是COVID-19规定的副产品，就像我一开始所认为的那样。整个社会在那痛苦的3年多里基本上都受到了这种教育，所以学校仍然活在担心爆发新一轮疫情的阴影之下。但是我的老师朋友给我讲了一个真实的故事，让我对背后的原因有了更好的理解。一个男孩的父母因为孩子在学校摔了一跤，导致他的一颗牙上磕掉了跳蚤大小那么一块。家长对学校不依不饶，给学校施压数月，要求学校找到一个令他们满意的解决方案。他们最终成功了。你可能会想：“家长怎么会对学校造成这么大的压力？”你看，现在有各种各样的社交媒体，在已经发生的无数个故事里，父母们通过向媒体曝光或者向教育局投诉来“揭露”学校的“疏忽”（可能你上面摔跤那样学校完全没有责任）。你可能会觉得教育局会主持公道，但更简单的解决办法就是把压力传回给学校，让学校解决他们自己的问题。教育局应该也是这样认为的，所以通常情况下这类事情都这样结束。&lt;/p&gt;
&lt;h2 id=&#34;思考和诉求&#34;&gt;思考和诉求&lt;/h2&gt;
&lt;p&gt;在我询问的朋友中，有一个在深圳一所小学当老师的朋友说他们学校仍然有正常的课间休息。孩子们可以自由地活动。但在她之前给我讲过的另一个令我震惊的故事里，一位老师不得不给一个学生下跪来求他不要退学。现在我对情况有了更深入的了解，我对学校和老师们感到同情。&lt;/p&gt;
&lt;p&gt;然而，我不认为学校是无辜的。恰恰相反，我认为学校对当前的状况负有最大的责任，尽管我认为根本原因是不理性的父母、失职的教育局和社交媒体的滥用。以下是我的理由。在我看来，尽管教育体系包括家庭、学校和社会，但学校在这三方中是主导者。它们是学生度过大部分学习时间的地方，也是大多数跟学生相关的现象（好的或坏的）开始的地方。在不好现象出现后，学校应该承担纠正它们的责任，以防事情继续恶化。学校和教师不能坐视教育体系走向毁灭，不是吗？&lt;/p&gt;
&lt;p&gt;家长们以及对这个社会仍然有信心的公民们也不应该对这些负面趋势坐视不管。即使我们难以在短期改变这种情况，我们也应该继续奋斗。媒体，尤其是传统媒体，你们是这个社会的耳目和喉舌，请不要假装一切都正常，更不要被社会的丑陋当枪使。&lt;/p&gt;
&lt;p&gt;教育局，你们是教育体系的大脑。你们最有权力，但请记住，权力越大责任越大。如果你们睡着了，请醒一醒。如果你们喝醉了，请在工作时间保持清醒。也许你们仅仅是过载了，那么请重新考虑是否承担了太多不必要的责任。不要忘记在人体中，&lt;strong&gt;大脑不是整个神经系统&lt;/strong&gt;，尽管它是神经系统的核心。管理的艺术在于把责任与权利一起分配下去。&lt;/p&gt;
&lt;h2 id=&#34;结语&#34;&gt;结语&lt;/h2&gt;
&lt;p&gt;在吉米所在学校一墙之隔的初中部，也是做完课间操就回教学楼，课间也不见有人在外活动。
如果这种现象持续下去，甚至扩大开来，我可以想象几年后的大学也会采纳同样的规定。
他们没有理由不这样做：学校和大学需要处理的麻烦将大大减少；学生们如此和乖巧守规矩。
当他们开始走向工作时，雇主们也会为他们下属的言听计从而感到十分高兴。
很快，在未来的病毒爆发中，人们将被舒服地享受封锁生活，他们将感觉比他们父母一代的人轻松千万倍。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Break Time Banned in Chinese Schools. What Now?</title>
      <link>https://shaojiejiang.github.io/post/en/school-problems-china/</link>
      <pubDate>Mon, 11 Sep 2023 11:58:24 +0800</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/school-problems-china/</guid>
      <description>&lt;p&gt;（中文版请点击&lt;a href=&#34;https://shaojiejiang.github.io/post/zh/school-problems-china/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;）&lt;/p&gt;
&lt;p&gt;I started this post with anger.
If school is a place that only outlines what we CAN do, then what are the differences between schools and prisons?&lt;/p&gt;
&lt;h2 id=&#34;the-shocking-news&#34;&gt;The shocking news&lt;/h2&gt;
&lt;p&gt;My nephew Jimmy just enrolled in primary school on September 1, 2023, in a suburban district of Tianjin, China.
He isn&amp;rsquo;t a very extrinsic kid.
He would even be too shy to talk with me after being separated for a couple of months, even if we had just talked happily on the phone days ago.
&amp;ldquo;It&amp;rsquo;s natural for an intrinsic kid to be afraid of school.&amp;rdquo; I comforted myself, &amp;ldquo;He&amp;rsquo;ll get through it soon.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Several days after the new semester, I happened to have an opportunity to visit him.
However, when I saw how resistant he was to school, I realized it was more than his young age or intrinsic characteristics.
He had been crying every time he set for school, repeating things like &amp;ldquo;it&amp;rsquo;s too boring,&amp;rdquo; and &amp;ldquo;time goes too slow.&amp;rdquo;
Yeah, I can relate to my old memory, Chinese schools weren&amp;rsquo;t necessarily interesting places.
But the words &amp;ldquo;time goes too slow&amp;rdquo; rang my bell: he is just a 6-year-old, how would he have the feeling of time being too slow??
If he was enjoying, he should have felt time flies.
I sensed something was wrong and decided to investigate.&lt;/p&gt;
&lt;p&gt;I started by asking him why he didn&amp;rsquo;t like school while he had so much fun to enjoy, recalling the happy memory I had in primary school playing games between classes.
It was the mid-1990s, in a very underdeveloped town in China.
We didn&amp;rsquo;t have as many toys to kill time, so we made good use of our creativity: we played all kinds of interactive games, like hide-and-seek, police chasing suspects; we also made good use of materials we found nearby, flying paper planes, square-bashing competition (read on for explanation), stalk-pulling (see below).
The list goes on and on.
I wouldn&amp;rsquo;t say I enjoyed my primary school overall, but remembering those games, I still can&amp;rsquo;t resist putting on smiles.
I asked my nephew weren&amp;rsquo;t these fun activities to do at school?&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-square-bashing-game&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://imagepphcloud.thepaper.cn/pph/image/74/93/275.jpg&#34; alt=&#34;The Square-bashing Game.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Square-bashing Game.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; The square-bashing game is a game with papers folded into squares. In the gameplay, players bash their squares in turn on the ground, nearby, or on their opponents&amp;rsquo; squares. If the opponents&amp;rsquo; squares flip, you win their squares. A good player is proud of piling up the squares he wins &amp;ndash; triumphs to show others how powerful he is.&lt;/p&gt;
&lt;/blockquote&gt;


















&lt;figure  id=&#34;figure-the-game-of-stalk-pulling&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-fe60a2a26a106cabc95866545f739fd7_720w.webp&#34; alt=&#34;The Game of Stalk-pulling.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The Game of Stalk-pulling.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; The stalk-pulling game is played by pulling stalks (usually from fallen tree leaves) as illustrated above. The one whose stalk breaks loses the game. The fun is very similar to the big boys game, MMA &amp;ndash; you keep winning or you&amp;rsquo;re done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was shocked to hear that they are not allowed to enjoy the break-time anymore.
They must remain in the classroom &lt;strong&gt;unless&lt;/strong&gt; they need to get water or use the toilet.
Yes, you read it right.
No chasing around in the corridor, not to mention in the schoolyard.
Feeling too hard to comprehend, I asked him to wear his smartwatch to school, so that I could give him a video call during break-time and observe his environment by myself.
Only to know that smartwatches were said to be banned on the first day of his admission.
Trying to cheer him up, I suggested taking some toys so that he could at least have some fun.
But sorry, toys are banned too, of course.&lt;/p&gt;
&lt;p&gt;Still not giving up, I started asking around my friends and relatives, with the hope that Jimmy&amp;rsquo;s school was just a special case.
Soon I got some responses.
Some relatives said this to be very common, and even more serious in Beijing.
Another friend, who is now a teacher in a primary school in my hometown, expressed that this is just like COVID-19 &amp;ndash; a new norm.
Now I need somebody to cheer me up.&lt;/p&gt;
&lt;h2 id=&#34;pe-classes&#34;&gt;PE classes&lt;/h2&gt;
&lt;p&gt;Beside the hopelessness, a very slight comfort is that Jimmy has a PE class 4 days a week.
They also have group gymnastics every day during the long, 30-minute break in the morning.
These are the &lt;strong&gt;only&lt;/strong&gt; times when they are allowed in the schoolyard and the playground.
My sister and I decided to do some investigation on the quality of such outdoor activities.
We also optimistically thought that after the group gymnastics, the students might have the chance to play freely.
Yet we were surprised again.&lt;/p&gt;
&lt;p&gt;Below is a video I took after the gymnastics.
Kids were taken back to the classroom in order.
In another corner of the schoolyard, one teacher instructed her students: &amp;ldquo;Don&amp;rsquo;t talk while in the corridor. Quickly drink some water and use the toilet if needed. Don&amp;rsquo;t make me repeat!&amp;rdquo;&lt;/p&gt;











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://shaojiejiang.github.io/post/en/school-problems-china/media/gymnastics.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;

&lt;p&gt;This second video witnessed the &amp;ldquo;very fierce&amp;rdquo; sport they did during the PE class.
They walked for &lt;strong&gt;three&lt;/strong&gt; whole rounds during the 40-minute class, which explains why they needed to have good rest when they weren&amp;rsquo;t walking.
Oh right, they also spend quite some time in practicing lining up the troop.
Another class nearby, 2nd grade or even higher (because they have uniforms already), also spent their whole class in practicing the lining.











  





&lt;video controls  &gt;
  &lt;source src=&#34;https://shaojiejiang.github.io/post/en/school-problems-china/media/pe_class.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;/p&gt;
&lt;h2 id=&#34;the-reason&#34;&gt;The reason&lt;/h2&gt;
&lt;p&gt;You may tend to think this is a byproduct of the COVID-19 regulations, as I did in the beginning.
The whole population was basically educated to do this during the miserable 3+ years, so it&amp;rsquo;s somewhat understandable that the schools are still in the shadow of scaring new breakouts, though not forgivable.
But then my teacher-friend told me a true story in which a boy&amp;rsquo;s parents blamed the school for their son&amp;rsquo;s falling on the ground, resulting in a flea-sized crack on one of his teeth.
The parents kept pressuring the school for months to find a satisfying solution for them, and they succeeded.
&amp;ldquo;How could parents put so much pressure on the school?&amp;rdquo; you may wonder.
Well, now with all kinds of social media, there are already countless stories in which parents &amp;ldquo;exposed&amp;rdquo; schools&amp;rsquo; &amp;ldquo;negligence&amp;rdquo; by mentioning/reporting to newspapers or local Education Bureaus.
You may argue, &amp;ldquo;The Education Bureaus can give justice to both parents and schools.&amp;rdquo;
But the simplest solution is to just pass the pressure back to schools and let schools bother themselves with their own problems, and that is what usually what such cases end up with.&lt;/p&gt;
&lt;h2 id=&#34;after-thoughts-and-call-for-changes&#34;&gt;After thoughts and call for changes&lt;/h2&gt;
&lt;p&gt;Among the friends I enquired, there was another primary school teacher in Shenzhen who said their school still has normal break times, when kids can freely wander around.
But I remembered another shocking story where a teacher from her school had to kneel down to get a student back to school.
With a better understanding of the situation, now I feel a bit sorry for schools and teachers.&lt;/p&gt;
&lt;p&gt;However, I don&amp;rsquo;t think schools are innocent.
Quit the opposite, I think schools bear the most responsibility for the current situation, although I think the root causes are irrational parents, failing Education Bureaus, and the misuse of social media.
Here is my justification.
IMHO, although the education system comprises families, schools, and society, schools are the lead among these three parties.
They are the place where students spend most of their study time, and they are the place where most trends, good or bad, start.
In the case of bad trends, schools should take the responsibility of correcting them so that things won&amp;rsquo;t deteriorate.
Schools and teachers can&amp;rsquo;t just sit back and watch the education system being doomed, can they?&lt;/p&gt;
&lt;p&gt;Parents, and citizens who still have faith in this society, shouldn&amp;rsquo;t just sleep on these negative trends.
We should keep fighting even though we won&amp;rsquo;t change the situation in the near future.
Media, especially traditional newspapers, you&amp;rsquo;re the ears, eyes, and mouths of this society, please don&amp;rsquo;t pretend that nothing is wrong, let alone act as the gun of ugliness.&lt;/p&gt;
&lt;p&gt;Education Bureaus, you&amp;rsquo;re the brain of the education system.
You&amp;rsquo;re the most powerful, but remember, with power comes responsibility.
If you&amp;rsquo;re asleep, please wake up.
If you&amp;rsquo;re drunk, please keep sober at work time.
Or if you&amp;rsquo;re burning out, reconsider whether you&amp;rsquo;re taking too much unnecessary responsibility.
Don&amp;rsquo;t forget that in the human body, the brain is not the whole nervous system, though it is the core of it.
The art is to distribute responsibility with rights altogether.&lt;/p&gt;
&lt;h2 id=&#34;remarks&#34;&gt;Remarks&lt;/h2&gt;
&lt;p&gt;In the middle school section of the same school, which is by the side of Jimmy&amp;rsquo;s, the same phenomenon was observed.
If this phenomenon continues or even widens, I can imagine universities in several years will adopt the same regulation.
There is no reason why they won&amp;rsquo;t: schools, colleges, and universities will have much fewer troubles to deal with; students are so calmly obedient.
When they start their careers, their employers will be happy too about their do-as-said part.
Soon enough, in a future virus outbreak, people will be locked down so comfortably, feeling hundreds of thousands of times more relaxed than their parents did.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Has the AI-Era come to video games already?</title>
      <link>https://shaojiejiang.github.io/post/en/ai-gaming-era/</link>
      <pubDate>Sun, 13 Aug 2023 17:05:13 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/ai-gaming-era/</guid>
      <description>&lt;p&gt;With the big noises made by ChatGPT, many different industries have noticed the value of LLM technologies.
Unsurprisingly, the video game industry is one of them.
In this blog, I introduce several cool demos/WIPs that I&amp;rsquo;ve recently found, and share my opinions on why they might have profound influences on the future of video game industry.
I also try to explain the current difficulties, and possible directions for solving them.
In the end, I also share some dreams of future games.
I believe, the era of AI has come to video games!&lt;/p&gt;
&lt;h2 id=&#34;the-matrix-ai-powered-npcs-demo-by-the-replica-studios&#34;&gt;The Matrix AI-Powered NPCs demo by the Replica Studios&lt;/h2&gt;
&lt;p&gt;Players are used to have chats with the NPCs, but most of these conversations are scripted.
The current best conversational experience you can have with NPCs is to select from several possible responses, so you have some freedom of steering dialogues.&lt;/p&gt;


















&lt;figure  id=&#34;figure-dialogue-selection-in-witcher-3&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Dialogue selection in Witcher 3&#34; srcset=&#34;
               /post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_527b0774b1c015883502fc1666882b7e.webp 400w,
               /post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_2c0e283be5301be66b25147cae746fe8.webp 760w,
               /post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/dialogues-in-witcher3_hue09e192b7ac8a4706bd7f9ae742b8051_48100_527b0774b1c015883502fc1666882b7e.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Dialogue selection in Witcher 3
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;If you are a game lover, have you ever dreamt about talking to NPCs like they&amp;rsquo;re other human players?
Well, this is definitely possible now, and the Replica Studios already made a demo about it&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
Instead of looping over pre-scripted lines, the Replica Studios attached LMs (probably OpenAI ChatGPT) to the NPCs, allowing them to all speak characteristically.
You can even chat with NPCs using your voices directly, and they will speak back.
Take a look at this YouTube video&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; of the demo.&lt;/p&gt;
&lt;p&gt;In many games, the plot is driven (or better put, reflected) by chatting with NPCs.
But since LLM chatbots can have randomness in their responses, maybe in the future, the game progression can be take to anywhere, so that every player can have a unique experience in the same game.
This is already partly made true in the AI Dungeon text game&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;The Matrix demo may look sleek in the video, but in reality it can take around 10 seconds to get a response from NPCs.
This lag is probably due to many users are calling the LLM API at the same time, and slow processing of several different modules, such as ASR and TTS.
Besides, current general-purpose LLMs like ChatGPT are very large in terms of number of parameters, and this means long processing time.
Potential solutions can be training bespoke, smaller-sized chatbot models, and maybe even audio-to-audio model so that the processing is simplified.&lt;/p&gt;
&lt;h2 id=&#34;herika-by-dwemer-dynamics&#34;&gt;Herika by Dwemer Dynamics&lt;/h2&gt;
&lt;p&gt;The experience that every player being able to conduct unique conversations with each NPC can already be fascinating.
Isn&amp;rsquo;t it more interesting to have a computer-controlled companion, one that can not only chat with you, but can also follow your voice commands?
Then you definitely want to check out Herika, a mod&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; for &lt;em&gt;The Elder Scrolls V: Skyrim&lt;/em&gt;.
Herika is a ChatGPT-powered AI companion that can understand the player&amp;rsquo;s audio and textual inputs.
She is capable of chit-chatting with the player, commenting on the game scenes and events, following the player&amp;rsquo;s various commands, and more.&lt;/p&gt;


















&lt;figure  id=&#34;figure-system-design-of-herika-image-credit-dwemer-dynamics&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;System design of Herika. Image credit: Dwemer Dynamics&#34; srcset=&#34;
               /post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_8ef29e5e73f8099642bfabb22652b4d5.webp 400w,
               /post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_76bd25ffc50f49e25b364c14c33177b2.webp 760w,
               /post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/herika-system_hu29424bab3769eb4f68d9166875cc0864_509908_8ef29e5e73f8099642bfabb22652b4d5.webp&#34;
               width=&#34;760&#34;
               height=&#34;418&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      System design of Herika. Image credit: Dwemer Dynamics
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Above is an illustration of Herika&amp;rsquo;s system design.
Here is a brief overview of its main components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Audio inputs and outputs are processed to and from texts by ASR and TTS modules&lt;/li&gt;
&lt;li&gt;Game objects, scenes, locations, etc., are extracted from the game as texts&lt;/li&gt;
&lt;li&gt;The chatting and commenting are all achieved by querying the OpenAI API, in the form of role-playing chats&lt;/li&gt;
&lt;li&gt;Given player&amp;rsquo;s command in natural language, the command-following ability is achieved by asking GPT to generate formatted commands that are used by the game engine to control Herika&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Check out this YouTube video&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; to get the feeling of how Herika works.
Although it seems to work astonishingly well in the video, currently Herika has the same problem of long response time like the Matrix demo.
Of course, another issue is that playing with such a companion can burn money quickly, and this is because most of Herika&amp;rsquo;s functionality is achieved by calling paid APIs.
Still a lot of work to do before this kind of gameplay can get popular, but this mod definitely cracks open another line of bright future!&lt;/p&gt;
&lt;h2 id=&#34;ai-playing-tomb-raider&#34;&gt;AI playing Tomb Raider&lt;/h2&gt;
&lt;p&gt;OK, we&amp;rsquo;ve already seen AI controlling our companion in the game, then what&amp;rsquo;s next?
Controlling the player directly, of course!
Here is a video of AI playing Tomb Raider&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.
In this demo, similar techniques to Herika like LLM and TTS are also used.
What&amp;rsquo;s more, it seems that the author has employed several other AI modules, too, such as object detection.
It&amp;rsquo;s not yet clear how the game character is controlled at the time of writing this blog (08/13/2023).&lt;/p&gt;
&lt;h2 id=&#34;more-work-of-ai-playing-games-in-academia&#34;&gt;More work of AI playing games, in academia&lt;/h2&gt;
&lt;p&gt;It worths noting that using modern AI&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; to play games is not new.
Many previous endeavours have already been made, such as the OpenAI Five&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; playing Dota 2.
Many scientific experiments in the RL field were actually conducted on game environments like OpenAI Gym&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt; and Unity ML-Agents&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;.
However, the research characteristic of this line of work makes it far from revolutionizing the video game industry, and indeed, this was usually not the indention of researchers.&lt;/p&gt;


















&lt;figure  id=&#34;figure-an-example-of-openai-gym&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An example of OpenAI Gym.&#34;
           src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/openai-gym.gif&#34;
           loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An example of OpenAI Gym.
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-an-example-of-ml-agents&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;An example of ML-Agents.&#34; srcset=&#34;
               /post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_5b753ca6463c4352b20c3c6d4160653f.webp 400w,
               /post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_192e35e39d2b75df501a70ef62b5bd4f.webp 760w,
               /post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/ai-gaming-era/figures/ml-agents_hud75644e00942eb99ca4ae5a1121fcdf2_44544_5b753ca6463c4352b20c3c6d4160653f.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      An example of ML-Agents.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In the recent months, several other research outcomes related to video games have attracted people&amp;rsquo;s attention, e.g., Generative Agents&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt; by Stanford University, and CALM&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt; by Nvidia.
While Generative Agents might have put more focus on studying human behaviour instead of game playing, CALM presents an algorithms of controlling game characters using textual commands (hence easily with voice through ASR).
What&amp;rsquo;s more interesting about CALM is that the model size it uses to control the game character is as small as several hundreds of parameters, making it easily runnable locally.
Of course, attaching LMs for more flexible natural language understanding can increase the parameter size many times, but still possible to find a good middle ground between performance and latency.&lt;/p&gt;
&lt;h2 id=&#34;outlook&#34;&gt;Outlook&lt;/h2&gt;
&lt;p&gt;It seems that the technologies for applying modern AI in games are already maturing.
Although current AI models, especially those generative ones, are often criticised for problems like hallucination, repetition, and unsafe responses etc., I would argue that such problems will be much less destructive in the game world than in real life.
It would be very interesting to see more and more games with AI companions that chat with you, and give you a hand when asked.
To make it more exciting, how about train your AI companions by yourselves, while you&amp;rsquo;re playing the game?
You already generate a lot of (labelled) data when you play games, and using it to train your AI companion is theoretically possible.
However, popular game engines like Unity and Unreal don&amp;rsquo;t directly support AI training yet, so we still need some time to make it happen.
But games with custom engines is much more flexible, and Human-Like&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt; is such a game that uses your data generated in the game and trains an AI opponent online.&lt;/p&gt;
&lt;p&gt;If tools aren&amp;rsquo;t a problem, what about model sizes?
In the last couple years, we saw best-performing models getting larger and larger, most of which definitely can&amp;rsquo;t run on consumer machines.
I personally believe increasing the model size isn&amp;rsquo;t the ultimate answer.
Luckily, there is another stream of research studying the grokking&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt; phenomenon of models as small as an MLP, with merely several hundreds of parameters.
Probably LMs won&amp;rsquo;t have any sensible performance at this size level, but it&amp;rsquo;s possible to largely decrease the model sizes once we have enough understanding on their mechanisms.&lt;/p&gt;
&lt;p&gt;Taking a step back, not too long ago deep learning and video games were still almost two extremes of the spectrum: the former is often associated with hard-working researchers, while the latter often reminded us of people killing time.
Gamers are usually those young and smart people, who devoted large amount of time and energy in the game they love.
Since finally the &amp;ldquo;two extremes&amp;rdquo; are coming together, maybe something more profound can happen?
Just some personal thoughts, probably unrealistic, but for instance using LLMs as portals that make the player more interested in real world, and even learn about practical skills that they can use in real life?
It might be possible, who knows?&lt;/p&gt;
&lt;h2 id=&#34;updates-on-08202023&#34;&gt;Updates on 08/20/2023&lt;/h2&gt;
&lt;h3 id=&#34;nvidia-omniverse-ace&#34;&gt;NVIDIA Omniverse ACE&lt;/h3&gt;
&lt;p&gt;From ACE&amp;rsquo;s project page:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NVIDIA Omniverse™ Avatar Cloud Engine (ACE) is a suite of real-time AI solutions for end-to-end development and deployment of interactive avatars and digital human applications &amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, with this toolkit, you can build NPCs that can not only communicate with you in speech and synchronise lip movements, but also supplies backend LLMs.
While this was also achieved by combining several independent tools in projects like Herika, NVIDIA ACE provides a one-stop solution.&lt;/p&gt;
&lt;p&gt;Below is its system design.&lt;/p&gt;


















&lt;figure  id=&#34;figure-illustration-of-nvidia-ace&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://developer.nvidia.com/sites/default/files/akamai/omniverse/nvidia-omniverse-ace-dev-zone-pipeline-diagram@2x.png&#34; alt=&#34;Illustration of NVIDIA ACE.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Illustration of NVIDIA ACE.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&#34;mantella-by-art-from-the-machine&#34;&gt;Mantella by Art from the Machine&lt;/h3&gt;
&lt;p&gt;Mantella&lt;sup id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;16&lt;/a&gt;&lt;/sup&gt; is a project similar to Replica&amp;rsquo;s Matrix demo, but in the world of Skyrim like Herika.&lt;/p&gt;
&lt;h3 id=&#34;agentsims&#34;&gt;AgentSims&lt;/h3&gt;
&lt;p&gt;AgentSims&lt;sup id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;17&lt;/a&gt;&lt;/sup&gt; is a work from academia that shares a lot of similarities with Generative Agents, and they were both released around the same time.
A main difference of AgentSims from Generative Agents is that AgentSims allows a player to join the town as the mayor and influence the agents by talking with them.
They also have an online live demo, which can be found on &lt;a href=&#34;https://www.agentsims.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;their website&lt;/a&gt;.&lt;/p&gt;


















&lt;figure  id=&#34;figure-a-screenshot-of-agentsims&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://generation-sessions.s3.amazonaws.com/7fffe1e230aaf47ad7397c3a59f1a690/img/image-1.png&#34; alt=&#34;A screenshot of AgentSims.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      A screenshot of AgentSims.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.replicastudios.com/blog/smart-npc-plugin-release&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Replica Smart NPCs&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SbzBTp_kBIk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI-Powered NPCs: A Game-Changing FREE Demo&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://aidungeon.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Dungeon: A text-based adventure-story game you direct (and star in) while the AI brings it to life.&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nexusmods.com/skyrimspecialedition/mods/89931&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Herika - The ChatGPT Companion&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0svu8WBzeQM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The AI Takes Control of the adventure in Skyrim!&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=0wTf_bbkW2U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Creating a Self-Aware Lara Croft that Plays Tomb Raider&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;As opposed to traditional AI used in games, which are usually implemented with sets of rules, here by modern AI I mean those powered by DL and/or RL algorithms&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/OpenAI_Five&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Five&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/getting-started-with-openai-gym/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Getting Started With OpenAI Gym: The Basic Building Blocks&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://unity.com/products/machine-learning-agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unity Machine Learning Agents&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/joonspk-research/generative_agents&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/par/calm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CALM: Conditional Adversarial Latent Models for Directable Virtual Characters&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://store.steampowered.com/app/1400190/HumanLike/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Human-Like game on Steam&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.02177&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://pair.withgoogle.com/explorables/grokking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Do Machine Learning Models Memorize or Generalize?&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:16&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nexusmods.com/skyrimspecialedition/mods/98631&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mantella - Bring NPCs to Life with AI&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:16&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:17&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/py499372727/AgentSims&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AgentSims: An Open-Source Sandbox for Large Language Model Evaluation&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:17&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>One source of LLM hallucination is exposure bias</title>
      <link>https://shaojiejiang.github.io/post/en/llm-hallucination/</link>
      <pubDate>Wed, 09 Aug 2023 22:16:30 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/llm-hallucination/</guid>
      <description>&lt;p&gt;With the release of closed-source ChatGPT, GPT-4, and open-source LLaMa models, the LLM development has seen tremendous improvements in recent months.
While we are hyped with the fact that these LLMs are capable of many tasks, we have also noticed again and again that these LLMs hallucinate content.
Today I came accross this inspiring paper, &lt;a href=&#34;https://arxiv.org/abs/2305.14552&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sources of Hallucination by Large Language Models on Inference Tasks&lt;/a&gt; by McKenna et al., in which the authors have identified two main sources of hallucination:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge that was memorised by the model during pre-training&lt;/li&gt;
&lt;li&gt;Corpus-based heuristics such as term frequency&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In my opinion, I would put these two reasons into one category: the exposure bias.
This is becuase either the memorised knowledge, or frequent terms, were exposed to the LLM at pre-training state.
The observation made in this paper is very enlightning, and reminded me of an ealier paper of mine, where we also concluded that the low-diversity issue of generative chatbots are caused by frequent terms in the training corpora&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Although LLMs are becoming larger, trained with more sophisticated techniques like RLHF, they have a deep root in the field of statistical models.
Losses are calculated based on terms, which are used to update the model weights, so it&amp;rsquo;s not surprising at all if the trained LLMs respond differently to terms with different frequencies.
And in fact, it would be surprising if these LLMs only learn &lt;strong&gt;perfect&lt;/strong&gt; grammar and semantics and totally shake off the frequency part.
There is nothing wrong for LLMs being statistical.
We human often make decisions based on experience, and isn&amp;rsquo;t that a kind of statistical model?
To make matters even worse, natural languages have a statistical nature too &amp;ndash; most of them, if not all, evolve over time, not neccessarily changing the meaning of words, but definitely changing the frequency speakers use them.&lt;/p&gt;
&lt;p&gt;As pointed out by Konstantine Arkoudas&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, GPT-4 can&amp;rsquo;t reason.
I agree with this statement.
I think LLMs are sophisticated statistical models, and the generation process is more like information retrieval but using the neural network weights and in the granularity of tokens.
Also as mentioned by Arkoudas, the lack of reasoning in LLMs has a connection with the hallucination problem.
I agree with him and many other researchers, retrieval-augmentation could serve as the &amp;ldquo;guardrail&amp;rdquo; of LLM generations, but unlikely to be the silver bullet for eliminating the hallucination problem.&lt;/p&gt;
&lt;p&gt;However, &amp;ldquo;can&amp;rsquo;t be solved&amp;rdquo; is different from &amp;ldquo;can&amp;rsquo;t be improved&amp;rdquo;.
Given that more and more studies have shown the vulnerability of LLMs to the statistical nature of their training data, maybe more effort is needed in thinking of a different way of training the model.&lt;/p&gt;
&lt;p&gt;Lastly, it&amp;rsquo;s worth noting that the McKenna et al. work was studied under NLI.
Although the hallucination problem is more prominent in NLG, it&amp;rsquo;s not straightforwad how to do a similar analysis in the NLG scenario.
But if it can be done, it would be more attention catching.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3308558.3313415&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.preprints.org/manuscript/202308.0148/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-4 Can&amp;rsquo;t Reason&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Weakly Supervised Turn-level Engagingness Evaluator for Dialogues</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2023-weakly/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2023-weakly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2022-simple/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2022-simple/</guid>
      <description></description>
    </item>
    
    <item>
      <title>《所谓情商高，就是会说话》读书笔记</title>
      <link>https://shaojiejiang.github.io/post/zh/eq-speaking/</link>
      <pubDate>Tue, 30 Jun 2020 14:37:56 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/eq-speaking/</guid>
      <description>&lt;p&gt;这是非常短的一本书。作者主要分享了有求于人时的7个突破口，分别是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;投其所好&lt;/li&gt;
&lt;li&gt;儆其所恶&lt;/li&gt;
&lt;li&gt;选择的自由&lt;/li&gt;
&lt;li&gt;被认可欲&lt;/li&gt;
&lt;li&gt;非你不可&lt;/li&gt;
&lt;li&gt;团队化&lt;/li&gt;
&lt;li&gt;感谢&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;结合之前读的《The 7 Habits of Highly Effective People》发现，这些技巧与高效能人士的办事原则并不冲突，反而是相互补充的：原则是大方向的导航，技巧是走好每一步的保障。而且经得起时间检验的技巧也都是以原则为指导的；只顾眼下利益的技巧，更应该叫“套路”，而这用不了几次就会被人识破。&lt;/p&gt;
&lt;p&gt;以上7个技巧分别对应了哪项做事原则呢？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;投其所好、儆其所恶，是“双赢原则”的实践&lt;/li&gt;
&lt;li&gt;提供选择、被认可欲、非你不可，是“要想被理解，先要理解对方”原则的实践&lt;/li&gt;
&lt;li&gt;团队化，是理解、认可对方的一种方式，更是“协同原则”的直接实践&lt;/li&gt;
&lt;li&gt;衷心感谢，也属于对别人付出的理解和认可&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;另外书中提到一点“要想掌握‘措辞菜谱’，输出是最便捷的途径”，正印证了本人&lt;a href=&#34;../three-key-elements-of-learning&#34;&gt;学习的三要素&lt;/a&gt; 博客里关于“输出”对于学习的重要性。&lt;/p&gt;
&lt;p&gt;最后吐槽下该书的不足：作者同样&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;为广告文案员，因此第二章（最后一章）基本都是关于文案写作的语言张力的，与情商没什么关联，有点被欺骗的感觉。作者可能犯了文案员“标题党”的职业病了吧。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;相对于上一本书《好文案一句话就够了》的作者&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>《好文案一句话就够了》读后感</title>
      <link>https://shaojiejiang.github.io/post/zh/copywriting/</link>
      <pubDate>Mon, 29 Jun 2020 18:23:16 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/copywriting/</guid>
      <description>&lt;p&gt;本人读《好文案一句话就够了》这本书（以下统称“该书”），主要是为了学习一些普适的写作技巧，毕竟虽然本人不需要写广告文案，但还是需要写推文、博客、文章等等，如果能练出更打动人心、有张力、吸引目光的写作风格，又何乐不为呢？&lt;/p&gt;
&lt;p&gt;该书总结的广告文案的三大基本原则，我认为很是精炼：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;让受众觉得信息与自己有关&lt;/li&gt;
&lt;li&gt;语言表达要有张力&lt;/li&gt;
&lt;li&gt;勾起读者的疑惑等阅读兴趣&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;随后该书还将三大原则展开，并通过实例分享了共77条文案写作技巧。现将这些技巧的核心内容，根据&lt;strong&gt;本人的喜好以及理解&lt;/strong&gt;进行总结。&lt;/p&gt;
&lt;h2 id=&#34;让受众觉得信息与自己有关&#34;&gt;让受众觉得信息与自己有关&lt;/h2&gt;
&lt;p&gt;首先应该明确受众，与其石沉人海、波澜不惊，不如缩小范围、把话说给懂的人听。为达到这样的目的，通常可以加上人称代词（如果原先没有），或者加上限定词对范围进行明确界定。&lt;/p&gt;
&lt;h2 id=&#34;语言表达要有张力&#34;&gt;语言表达要有张力&lt;/h2&gt;
&lt;p&gt;该书关于语言张力的技巧有很多，主要可以概括为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简洁凝炼、具体形象&lt;/li&gt;
&lt;li&gt;感情真挚、说出心声、平易近人&lt;/li&gt;
&lt;li&gt;纵览全局（过程概览、所需时间、结果展望等）&lt;/li&gt;
&lt;li&gt;表达技巧（强调语气、利用格式节奏、制造冲突、引用名言）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;勾起读者的疑惑等阅读兴趣&#34;&gt;勾起读者的疑惑等阅读兴趣&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;留白或者设问&lt;/li&gt;
&lt;li&gt;创造吸睛新词&lt;/li&gt;
&lt;li&gt;有故事性&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;其实写到这里，通过对全书内容进行回顾，我发现该书并没有给我太多的新信息。我依然认为要写好各种形式的文字，只需要做到四点就可以了：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;明确自己想要表达什么&lt;/li&gt;
&lt;li&gt;依据经验选择最有力的表达方式（这一点需要大量积累）&lt;/li&gt;
&lt;li&gt;站在读者角度思考信息能否流畅传达&lt;/li&gt;
&lt;li&gt;以上步骤反复迭代优化&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;该书的主要作用，也许就是像作者呼吁的那样“放在公司的桌上，当成字典来使用”吧。
顺带提一下本人对该书不太认同的一点，就是某些技巧过于强调“吸睛”了，以至于作者本人都多次提醒读者“作为文案员可以如此写，而作为消费者要保持头脑清醒，不要轻易被广告文案诱惑”。
这也从侧面说明了以上“技巧”中，最普适也最不像“技巧”&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;的一点，就是说话要发自内心，靠套路得来的人心只能是暂时的，而且还有被看穿的危险。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;个人理解所谓技巧，是指可以简单地掌握，并且应用起来见效很快。然而真挚的讲话，对于说惯了客套话的人是需要一定练习的，并且见效可能没那么快。用科维的观点来说，“感情真挚”更应该算是一项原则而非技巧。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Transformer Align Model</title>
      <link>https://shaojiejiang.github.io/post/en/transformer-align-model/</link>
      <pubDate>Sat, 16 May 2020 16:40:07 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/transformer-align-model/</guid>
      <description>&lt;p&gt;In this paper&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, transformer is trained to perform both translation and alignment tasks.&lt;/p&gt;
&lt;h2 id=&#34;application-scenarios-of-word-alignments-in-nmt&#34;&gt;Application scenarios of word alignments in NMT&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Generating bilingual lexica from parallel corpora&lt;/li&gt;
&lt;li&gt;External dictionary assisted translation to improve translation of low frequency words&lt;/li&gt;
&lt;li&gt;Trust, explanation, error analysis&lt;/li&gt;
&lt;li&gt;Preserving style on webpages&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-design&#34;&gt;Model design&lt;/h2&gt;
&lt;p&gt;The attention mechanism has long been motivated by word alignments in statistical machine translation, but ensure the alignment quality, additional supervision is needed.&lt;/p&gt;
&lt;p&gt;There is a tendency that the attention probabilities from the penultimate layer of a normally trained transformer MT model corresponds to word alignments.
Therefore, one attention head (clever!) in the penultimate layer is trained as the alignment head.
The motivation of selecting only one attention head for alignment is to give the freedom to the model of choosing whether to rely more on the alignment or other attention heads.&lt;/p&gt;
&lt;!-- While in Beamer alignment, the freedom is fully preserved in the attention layer, and the alignment is used for RNN hidden states. --&gt;
&lt;h2 id=&#34;how-two-train-the-alignment-head&#34;&gt;How two train the alignment head&lt;/h2&gt;
&lt;p&gt;There are two approaches existing in the literature:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Label alignments beforehand and train the attention weights through KL-divergence.&lt;/li&gt;
&lt;li&gt;Use the attentional vector to also predict either the target word or the properties such as POS tags of the target tokens.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this work, an unsupervised training approach is used to train the alignment head.
An alignment model is first trained on translation, then the penultimate layer attention weights are averaged and used as weak alignment supervision for a translation (and alignment) model.
The alignment model is trained in both directions.&lt;/p&gt;
&lt;p&gt;Previous work reported performance gain by introducing alignment supervision.
In this paper, however, alignment performances are good, but translation results are moderate.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1909.02074&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jointly Learning to Align and Translate with Transformer Models&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Compressive Transformers</title>
      <link>https://shaojiejiang.github.io/post/en/compressive-transformers/</link>
      <pubDate>Tue, 12 May 2020 14:29:44 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/compressive-transformers/</guid>
      <description>&lt;p&gt;Built on top of Transformer-XL, Compressive Transformer&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; condenses old memories (hidden states) and stores them in the compressed memory buffer, before completely discarding them.
This model is suitable for long-range sequence learning but may cause too much computational burden for tasks that only have short sequences.
Compressive Transformers can also be used as memory components in conjunction with other models.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In the beginning, the authors draw the connection between their work and human brains by mentioning that humans memorize things via lossy compression.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We aggressively select, filter, or integrate input stimuli based on factors of surprise, perceived danger, or repetition &amp;ndash; amongst other signals.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It&amp;rsquo;s often, if not always, good to see such insights of how AI works are inspired by humans.
It&amp;rsquo;s also good to see that they relate their work to previous works, i.e. RNNs, transformers and sparse attention.&lt;/p&gt;
&lt;p&gt;An RNN compresses previous memories into a fixed size hidden vector, which is space-efficient, but also results in its temporal nature and hence difficult to parallelize.
Transformers, on the other hand, store all the past memories uncompressed, which can be beneficial for achieving better performances such as precision, BLEU, perplexity, etc, but it costs more and more computation and memory space with the sequence length growing.
Sparse attention can be used to reduce computation, while the spatial cost remains the same.&lt;/p&gt;
&lt;h2 id=&#34;model-design-and-training&#34;&gt;Model design and training&lt;/h2&gt;
&lt;p&gt;The proposed Compressive Transformer uses the same attention mechanism over its set of memories and compressed memories, trained to query both its short-term granular memory and longer-term coarse memory.&lt;/p&gt;
&lt;p&gt;If trained using original task-relevant loss only, it requires backpropagating-through-time (BPTT) over long unrolls for very old memories.
A better solution is to use local auxiliary losses by stopping gradients and reconstructing either the original memory vectors (lossless objective) or attention vectors (lossy objective; reportedly to work better).
The second choice for the auxiliary loss, in other words, means that we don&amp;rsquo;t care whether the original memory can be reconstructed or not, as long as the attention vector can be reconstructed, given the same query (brilliant!).&lt;/p&gt;
&lt;h3 id=&#34;some-practical-concerns&#34;&gt;Some practical concerns&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The auxiliary loss is only used to train the compression module, as it harms the learning when the gradients flow back to the main network.
This might also explain why I couldn&amp;rsquo;t reproduce &lt;a href=&#34;../adaptive-computation-time&#34;&gt;ACT&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;Batch accumulation (4x bigger batch size) is used for better performance.
It is observed in some works that bigger batch sizes lead to better generalization, but some other works found the opposite to be true (discussed in the papers and talks mentioned &lt;a href=&#34;../visualizing-loss&#34;&gt;in my other post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Model optimization is very sensitive to gradient scales, so the gradient norms are clipped to 0.1 for stable results.
This is typical for transformer variants.&lt;/li&gt;
&lt;li&gt;Convolution works best for memory compression.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;further-thoughsquestions&#34;&gt;Further thoughs/questions:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Compressive Transformer improves the modeling of rare words.
But why?&lt;/li&gt;
&lt;li&gt;In the discussion section, the authors pointed out that future directions could include the investigation of adaptive compression rates by layer, the use of long-range shallow memory layers together with deep short-range memory, and even the use of RNNs as compressors.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1911.05507&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compressive Transformers for Long-Range Sequence Modelling&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Visualizing the Loss Landscape of Neural Nets</title>
      <link>https://shaojiejiang.github.io/post/en/visualizing-loss/</link>
      <pubDate>Wed, 06 May 2020 10:13:43 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/visualizing-loss/</guid>
      <description>&lt;p&gt;Here are some notes take while reading the NeurlIPS 2018 paper &lt;a href=&#34;http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt;.
This work helps explain why some models are easier to train/generalize than others.
The above image is a good illustration: with a much smoother loss landscape, DenseNet with 121 layers is much easier to train than a ResNet-110 without skip connections, and generalizes better in the mean time.&lt;/p&gt;
&lt;p&gt;The traditional way of visualizing loss functions of neural models in 2D contour plots is by choosing a center point $\theta^*$ (normally the converged model parameters), two random direction vectors $\delta$ and $\eta$, then plot the function:
$$f(\alpha, \beta) = L(\theta^* + \alpha \delta + \beta \eta)$$
Batch norm parameters are unchanged.&lt;/p&gt;
&lt;p&gt;The above method fails to capture the intrinsic geometry of loss surfaces, and cannot be used to compare the geometry of two different minimizers or two different networks.
This is because of the &lt;em&gt;scale invariance&lt;/em&gt; in network weights (this statement only applies to rectified networks as per the paper).
To tackle this, the authors normalize each filter in a direction vector $d$ ($\delta$ or $\eta$) to have the same norm of the corresponding filter in $\theta$:
$$d_{i, j} \leftarrow \frac{d_{i, j}}{||d_{i, j}||} ||\theta_{i, j} ||.$$
$i$ is the layer number and $j$ the filter number.
With the proposed filter-wise normalized direction vectors, the authors found that the sharpness of local minima correlates well with generalization error, even better than layer-wise normalization (for direction vectors).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why flat minima:&lt;/strong&gt; In a recent talk&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, Tom Goldstein (the last author) pointed out that flat minima correspond to large margin classifiers, which is more tolerant to domain shifts of data, thus having better generalization ability.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Known influential factors:&lt;/strong&gt;
Small-batch training results in flat minima, while large-batch training results in sharp minima.
Increased width prevents chaotic behavior, and skip connections dramatically widen minimizers (see figure in the beginning).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interpreting with precaution:&lt;/strong&gt;
The loss surface is viewed under a dramatic dimensionality reduction.
According to the authors&amp;rsquo; analysis, if non-convexity is present in the dimensionality reduced plot, then non-convexity must be present in the full-dimensional surface as well.
However, apparent convexity in the low-dimensional surface does not mean the high-dimensional function is truly convex. Rather it means that the positive curvatures are dominant.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In a nutshell:&lt;/strong&gt; It&amp;rsquo;s a great work trying to visualize the mystery of what&amp;rsquo;s going well/bad when training a neural model.
Although claiming the study to be empirical, I personally found their experiments and results very convincing.
Appendix B about visualizing optimization paths is also very insightful, and the authors probably also thought so, so they decided to move it as a main section in their latest &lt;a href=&#34;https://arxiv.org/pdf/1712.09913.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Arxiv version&lt;/a&gt; 😄!&lt;/p&gt;
&lt;p&gt;Further thoughts/questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Has it been done for visualizing NLP models?&lt;/li&gt;
&lt;li&gt;Is it more appropriate to visualize loss for NLG or other measures?
This might depend on how to define &amp;ldquo;labels&amp;rdquo; in NLG tasks.&lt;/li&gt;
&lt;li&gt;How big a convolution filter normally is?&lt;/li&gt;
&lt;li&gt;What&amp;rsquo;s similar between RNN and skip connections?&lt;/li&gt;
&lt;li&gt;This work can be used together with automatic neural architecture search, but is there any other more efficient way of getting better models?&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://iclr2020deepdiffeq.rice.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalization in neural nets:  a perspective from science (not math)&lt;/a&gt; Starting at 1:54:00 in the video.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>学习的三要素</title>
      <link>https://shaojiejiang.github.io/post/zh/three-key-elements-of-learning/</link>
      <pubDate>Thu, 30 Apr 2020 09:12:07 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/three-key-elements-of-learning/</guid>
      <description>&lt;p&gt;前注：如无特殊说明，本文的摘录均取自&amp;quot;摄影的艺术 (世界顶级摄影大师)&amp;quot; by Bruce Barnbaum, 樊智毅&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;自我审问要有一个合理的极限。在因为过于自省而感到焦虑之前，你应该通过拍摄一些照片来对外交流。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在读到这段话的时候，我想起了机器学习对我个人学习方式的一些启发，同时也是我博士导师Maarten对我的教导：表述（比如演讲、报告、交流和写作等）是学习的一个重要环节。
孔子说：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;学而不思则罔，思而不学则贻。&amp;rdquo; ——论语·为政&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;但我认为，如果没有一个表述的过程，“学”和“思”很有可能会变成空中楼阁。
只有通过将自己的观点表达出来、放到现实中让它们去经受时间的考验、经受别人的批评，才能真正地将自己的所学所思落到实地。
因此我认为，学习的三要素可以归纳为：学、思和述。&lt;/p&gt;
&lt;h2 id=&#34;向机器学习学习&#34;&gt;向“机器学习”学习&lt;/h2&gt;
&lt;p&gt;近来随着对“机器学习”思考的深入，我逐渐从中得到一些启发，比如我开始重视阅读就是因为认识到了大数据量对训练机器学习算法的重要性：如果算法需要输入大量的数据以及反复地学习才能得到理想的性能，那么我是不是也应该这么做呢？
阅读就是人类学习的一个重要“输入”方式。
它让前人总结出的思想精华，穿越了时空的限制不断地输入到读者的思想中。
当然，其他更加实时实地的输入方式也是不可或缺的，比如参加别人的演讲和报告。
我常常自嘲：机器学习领域的大师们把工作和生活中总结出来的哲理应用到机器学习算法上，而我这样的无名之辈从他们的工作中都能学到受之不尽的哲理。
封神之路任重而道远啊，哈哈！&lt;/p&gt;
&lt;p&gt;最近对机器学习有了更加深入的理解，认识到“输入”、“处理”和“输出检验”这三个环节，缺一不可。
而反思下我自己，输入（阅读）和处理（思考）正在稳步进行，但输出（表述）却还做得远远不够啊。
这也是我最近决定培养记笔记习惯的一个重要原因。
那么在我业余爱好的摄影领域，“输出”就不是记笔记或跟别人语言交流那么简单了，而更多地应该是通过拍摄照片来表述自己：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;成功地表达你的信息，是创意摄影的根本。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;关于表述&#34;&gt;关于表述&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;单纯地向别人汇报你看到的场景，那是逃避责任；把场景演绎出来，才是接受挑战。虽然场景可能不是你创造出来的，但照片却一定是！因此，不要止步于你的所见，加入你的评论、感受和建议，把它们都放到照片中吧，表达你的观点，阐明你的立场，让读者信服你的结论。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这段话是在讲艺术摄影的表达方式：从自己看到的场景（输入）中提炼出自己的观点（处理），并把这些观点表现在自己的作品中（输出）。
同时这段话也阐述了表达方式的三个层次：不加思索拍出的快照是最底层的、简单的记录，融入了自己观点或视角的作品是中层的，能够让观众明白你传达出的观点才是最高层的。
那么写作又何尝不是如此呢？
简单的记叙是最底层的，就算多用些华丽的词藻（对应到摄影上就是套用构图的范式、），也还是一篇没有思想的文字；表达了自己思想的文字是勇敢的，至于观点能不能被读者接受，是一个需要不断反思的过程：是自己的表达方式不够有说服力，还是想法太超前而不为世人接受？&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;了解自己要说什么！ 了解自己要怎么说！ 然后毫不妥协地说出来&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有意思的是，这也正是我自己最近关于写作的心得，尤其是在指导研究生毕业论文的时候，我对他们说出了几乎同样话，只不过我第三部分的观点要更批判性一些：站在读者的客观角度去审视自己有没有很好地传达观点。&lt;/p&gt;
&lt;p&gt;以上一直在说“机器学习”和“阅读”对我学习、摄影创作的影响以及他们之前的共性。
反思机器学习，其实上面提到的表达三层次对机器学习的研究也是具有指导意义的：简单的记述（autoencoder）是最低层次；具有创作性的表达方式（对话、摘要、翻译等等）是中层的；而如何让机器的表达更为人类所接受（是否合理、连贯、有趣），正是现在领域内的研究难点。
这也许对机器学习的多任务训练有些价值？&lt;/p&gt;
&lt;h2 id=&#34;关于摄影的真实&#34;&gt;关于摄影的“真实”&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;我认为大部分艺术家所主要追寻的不是真实，而是一种恰当的方式，以表达他们所理解的真实。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;人都是有思想的，有思想就一定会有主观，那么一个作者基于现实事物的创作，无论是摄影还是文学，都一定是主观的。
就算你能够保证自己作品的绝对客观和真实，你也没有办法保证读者带着主观情绪来审视你的作品，那么在他们眼里，你还是主观的。
是不是很有些相对论的意思，哈哈。&lt;/p&gt;
&lt;p&gt;真正的客观和真实是不存在的，也不应该作为终极追求。
正如我上面所说的对表达方式的反思，应该在自己的表达和别人的接受度上取一个合理的平衡，这才是创作的精髓吧。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>我需要明确自己的摄影主题</title>
      <link>https://shaojiejiang.github.io/post/zh/photography-subject/</link>
      <pubDate>Wed, 29 Apr 2020 14:17:37 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/photography-subject/</guid>
      <description>&lt;p&gt;前注：如无特殊说明，本文的摘录均取自&amp;quot;摄影的艺术 (世界顶级摄影大师)&amp;quot; by Bruce Barnbaum, 樊智毅&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;你的兴趣是什么？只有你自己可以回答。但这个回答是非常重要的，因为如果你要创作有意义的摄影作品，就必须专注于那些你最感兴趣的领域。不仅如此，你还必须专注于那些你具有强烈个人想法的领域。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是一个被大师们多次强调的心得：兴趣是第一导师。
而如果没有兴趣会怎么样？
不得不说，下面这段话里描述的感受似曾相识。。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;在日常对话中，你是否尝试过在你不感兴趣或没什么见解的主题上，说一些具有意义的话？这是不可能的！你无话可说，因为你不感兴趣。不过，这一般不会妨碍你继续讨论。正如人们谈论没有兴趣的话题一样，他们也可以拍摄其不感兴趣的事物，而结果是一样的：枯燥乏味。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个例子也很生动：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;以一个伟大的演说家（例如丘吉尔或者马丁·路德·金）为例，如果我们让他们对缝被子这个主题作一次激情洋溢的演讲，他们是没法做到的！他们无话可说，因为这不是其话题所在、其激情所在。他们需要在自己的主题上展示伟大的演说才华和说服技巧。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;大师们的作品一般都是风格非常一致的，因为他们很专注。
而正是专注，才让他们成为大师。
世界上的沃土那么多，但如果仅仅局限于走马观花似地去游览这些土地，而不是选择一块进行深耕，也是不会有任何收获的，这同时也是最近自己对科研的一些感悟。
现在慢慢认识到，所谓的“鄙视链”，一般都是处在起跑线上犹豫的选手对于前途望而生畏的感叹。
真正在赛道上奔驰的专业运动员，是不会有时间和心情去思考这些的，因为对于他们来说，他们的领域跟“上游”和“下游”领域之间的区别仅仅是赛场的不同，而每个赛场都有足够激烈的比赛来供他们忙碌、有足够大的荣誉来供他们争取。
用Stephen Covey的话来说，这叫富足思维（abundance mentality）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;而伟大的摄影家则知道什么是他们感兴趣的、什么是他们觉得乏味的，也能认识到自己的强项和弱项，并专于自己的兴趣和强项。他们会定期地在其他领域进行一些尝试，来扩大自己的兴趣范围并改进他们的弱项（你也应该这样），但他们不会把尝试性的拍摄和严肃深刻的表达混淆。 韦斯顿不会拍摄瞬间发生的事情，纽曼不会拍摄风景照，尤斯曼不会拍摄不幸的社会成员，阿勃丝不会印制出超现实效果的多重影像。他们的每一位都专注于自己兴趣最大、本领最强的领域。他们或许可以在其他领域创作出不错的作品，但这些作品的永恒性和冲击力会大打折扣。他们，还有其他伟大的摄影家，都睿智地决定在他们最擅长的领域内创作。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;回到正题摄影上。
我从一年半前认真对待摄影以来，还没有真正对什么主题感兴趣过，或者说没有坚持下来。
刚开始是带着18-200mm套机镜头，在大街小巷里漫无目的地穿梭，不知道自己想拍什么。
就算偶尔看到感觉有意思的东西，也不知道该怎么把它们记录下来。
几乎所有的题材都是一时兴起、浅尝辄止，尤其是在买了新设备之后拍一些适合新镜头的主题。
比如当时买了85mm f/1.8镜头和三脚架，也在拍了一两次自拍照之后便再没有尝试过，虽然后来有过一两次的冲动；买了70-300mm长焦镜头也就拍了一两次鸟；买105mm微距只拍了几次蘑菇；买10-20mm广角镜头也只拍了几次建筑。
说来实在惭愧，微距和广角镜头让自己满意的两张照片，还都分别是在刚买这两支镜头那会拍下的：


















&lt;figure  id=&#34;figure-蘑菇105mm微距&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;蘑菇，105mm微距&#34; srcset=&#34;
               /post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_bfad4daf1ffcd6c102def86aa31ae798.webp 400w,
               /post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_70c9005b3313dcca607fb93f69d63b6e.webp 760w,
               /post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/mushroom_hud1f6acd565390c5de0622ffd0d4fee53_421574_bfad4daf1ffcd6c102def86aa31ae798.webp&#34;
               width=&#34;600&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      蘑菇，105mm微距
    &lt;/figcaption&gt;&lt;/figure&gt;



















&lt;figure  id=&#34;figure-箭头10-20mm广角&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;“箭头”，10-20mm广角&#34; srcset=&#34;
               /post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_2055bfaa515325ec6094166f4cc021ff.webp 400w,
               /post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_e25923abca67ba00fd03f88146b59bfd.webp 760w,
               /post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/arrow-head_hu944ed5084f0e8608078ceee70df4f487_448689_2055bfaa515325ec6094166f4cc021ff.webp&#34;
               width=&#34;500&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      “箭头”，10-20mm广角
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;其实很多摄影题材都有吸引我的地方。
拍摄旅游题材可以收获各地的美景照，但是前期需要对目的地的人文、历史甚至天气和交通等都要有充足的了解；拍摄微距可以把渺小的事物以震撼的角度展示出来，前提是我有足够的耐心去做细致入微的观察；长焦摄影可以让我把美妙的野生动物拉近到眼前并展现给观众，但是难度不亚于狙击一个不确定的目标，而且前期对动物习性的了解也是少不了的；风景摄影可以展示人类建筑或是大自然美景的震撼，但是创意角度的选取、怎么避开人群或者让他们很好地融入画面、在什么样的天气拍摄等等也是让我很头大；而人像摄影不仅需要自己的审美观点，还需要建立好跟模特的关系或交流——如果我有模特的话。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;在更深的层次上，除非摄影师和主角之间有着友好的关系，否则任何尝试都不可以为你的肖像摄影增色（即使没有友好的关系，也至少有实质性的沟通或强烈的第一印象）。摄影师应该认识主角，对他感兴趣，对他有一定的看法，并努力把主角的个性以最强烈的方式表达出来。有时，摄影师必须强烈依赖于第一印象，因为他往往很难花上足够的时间来完全了解主角。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;总而言之，任何题材下好的作品似乎都离不开前期做大量的功课。
我明显是摄影态度还不够端正：想要好的照片又总嫌弃过程太辛苦，能静下心来看些摄影教材也是从前不久才开始的。&lt;/p&gt;
&lt;p&gt;没有什么好的照片是靠运气得来的；就算有，也需要提前把基本功练好才能抓住稍纵即逝的机会。
我无法忘记刚买D7200的时候，现实给我的一次教训。
当时拿着刚到手的新相机，兴致冲冲地就上街取景了，然后拍下了这张后来很快意识到问题很多的照片：


















&lt;figure  id=&#34;figure-网红墙下睡觉的猫未调修&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;网红墙下睡觉的猫，未调修&#34; srcset=&#34;
               /post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_c689bab0096ef9110d9bc1bf6d82c0b3.webp 400w,
               /post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_32055c83bc31962ab69502014f5e1042.webp 760w,
               /post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat_hu6bda404798615778eeb98b7747e06ddb_834419_c689bab0096ef9110d9bc1bf6d82c0b3.webp&#34;
               width=&#34;600&#34;
               height=&#34;507&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      网红墙下睡觉的猫，未调修
    &lt;/figcaption&gt;&lt;/figure&gt;

那天我很幸运，因为偶遇了这面网红墙。当时还不知道这面墙小有名气，只是觉得这句话很醒目、很有个性，最关键的是下面长凳上有只睡觉的猫，就像是有人专门安排的一样！
然而那天我也是很不幸的，因为首先我当时拍照不会构图，把画面重点的猫放到了左下角非常边缘的位置；其次没有足够了解相机的设置，所以没有记录下来无损的照片，给后期调整带来了很大的限制。
所以当我把这张照片分享到社交媒体上之后反响平平，甚至很多人都没注意到那只在阴影里的猫，反而被占据画面大部分的、高对比的墙吸引了。
其实即便我现在有这样的机会，也还是很难拍好这个场面的：墙上的涂鸦太大、太醒目；猫的位置不合适，很难作为主体被突出。
当然，要想拍的比我这张好，还是很容易的，毕竟这张照片已经不能再烂了，哈哈！
我当天拍了好多张，其中也有些角度好点的，但无奈当时自己太傻，把那些照片以“锐度不够”删掉了。。。
都是新手学费啊！
这样的事情当然是少有的可惜。
但如果我不练好基本功、不找到自己的摄影风格，我还会继续浪费快门数。&lt;/p&gt;
&lt;p&gt;后来我又多次拜访过这面墙，可惜再也没见到有猫在下面睡觉了。。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;后记：在后续对本书的学习中，看到了作者对如下作品的介绍，让我重新燃对猫那张照片的一点信心。&lt;/p&gt;


















&lt;figure  id=&#34;figure-乞女-by-bruce-barnbaum&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://images.squarespace-cdn.com/content/v1/563fac1de4b07f78f2db1c2c/1447459082594-OAR3ZKKQIR87D1PII2Y9/ke17ZwdGBToddI8pDm48kJcL8RUadGdk4gpl41YTwHNZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PICa0vOBoO_YRM0aI4T8IW9lHW4ggziL-I7oURYsi2vL8KMshLAGzx4R3EDFOm1kBS/TheBeggarWoman.jpg?&#34; alt=&#34;乞女 by Bruce Barnbaum&#34; loading=&#34;lazy&#34; data-zoomable width=&#34;600&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      乞女 by Bruce Barnbaum
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;虽然乞丐是视觉中心，但你不会立刻就能发现她。她太小了，无法马上就看得到。但只要你发现了，影像的性质就完全改变了。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从Barnbaum的这幅作品里，我认识到有时候对作品的一些打破常规的解释是有必要的：既然大师可以把主体放在不显眼的位置，那么我为什么不可以呢？
只要我给观者足够的引导和解释，比如这里标题只提猫，等观者发现猫的时候，依然可以体会到我当是看到这个场景时的感受。&lt;/p&gt;


















&lt;figure  id=&#34;figure-睡觉的猫抢救版&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;睡觉的猫，抢救版&#34; srcset=&#34;
               /post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_9f4f7b33e40b5909e220b1bb7600afee.webp 400w,
               /post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_bfbf5b134aea0fcf9e79bee6963dad0f.webp 760w,
               /post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/zh/photography-subject/images/sleeping-cat-touched_hu6bda404798615778eeb98b7747e06ddb_410272_9f4f7b33e40b5909e220b1bb7600afee.webp&#34;
               width=&#34;400&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      睡觉的猫，抢救版
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;这里对照片进行了裁剪和黑白处理以过滤掉颜色干扰，并做了split toning以使猫和墙的色彩略有分离。
当然，这里叫它“抢救版”，是因为它离一幅好的作品依然还很远。
如果我再有这样的机会，我会把机位左移，以尽量把猫放在更显眼的位置。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Computation Time</title>
      <link>https://shaojiejiang.github.io/post/en/adaptive-computation-time/</link>
      <pubDate>Tue, 28 Apr 2020 10:46:44 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/adaptive-computation-time/</guid>
      <description>&lt;p&gt;My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;additive-vs-multiplicative-halting-probability&#34;&gt;Additive vs multiplicative halting probability&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Multiplicative:&lt;/strong&gt; In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time.
It is acknowledged by the authors that using the logits $h_n^t$ as the halting probability at step $n$ might be more straightforward.
Therefore, the overall halting probability is calculated as $$p_t^n = h_t^n \prod_{u=1}^{n-1} (1 - h_t^u).$$
We use $(1 - h_t^u)$ for previous update steps to indicate that the updating is &lt;em&gt;not&lt;/em&gt; stopped until $n$.&lt;/p&gt;
&lt;p&gt;As each $p_t^n \in (0, 1)$ is relatively independent with each other and $\sum p_t^n$ is not bound to 1, this approach &lt;em&gt;does not&lt;/em&gt; restrict the update depth to grow arbitrarily.
The model can be of course trained to lower the expected ponder time $\rho_t = \sum n p_t^n$, but it is observed in the experiments that the resulting model is not preferable in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$h_t^1$ is usually just below threshold, intermediate $h_t^n = 0$, and final $h_t^N$ is high enough to halt the update.&lt;/li&gt;
&lt;li&gt;as the expectation is low, $p_t^N \ll p_t^1$, but the network learns to have a much higher magnitude of output states at step $N$, so that the final output is still dominated by the final state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Additive:&lt;/strong&gt; In contrast, the additive approach have an constraint of $\sum p_t^n = 1$, so that the probability is decreased monotonically with the number of updates growing larger.
Though being non-differentiable, the total ponder time (total updates at all positions) is penalized to avoid consuming unnecessary computation.
There is still one drawback of this approach, however.
The performance is sensitive to the penalty factor $\tau$, which is not intuitive to choose as a hyperparameter.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.08983&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Computation Time for Recurrent Neural Networks&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>凯恩斯主义——中国以基建为核心的经济策略理论背景</title>
      <link>https://shaojiejiang.github.io/post/zh/keynesianism/</link>
      <pubDate>Mon, 27 Apr 2020 22:18:24 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/keynesianism/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;收入等于人们的需求消费的总量。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;投资——购买厂房、设备等——等于将支出“注入”经济中。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;在这里，水龙头指利息，即借贷的价格。当利息降低——打开水龙头——借贷变得便宜，更多的人会进行贷款。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;世界充满了不确定，人们并不一定要将自己的储蓄与厂房和工厂挂钩。或许你可能只想把钱放在床垫下面以备不时之需。在凯恩斯看来，利息并不能有助于将多余的储蓄转为投资。事实上，储蓄和投资之间并没有关联。&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;凯恩斯认为，当流出量大于流入量时便会发生经济衰退。&amp;rdquo; (from &amp;ldquo;经济学通识课（耶鲁大学出品！耶鲁大学经济学入门课，普通人也能读懂的经济学！理论到现实，搭起用经济学改善现实生活的桥梁 ) (博集经管商务必读系列)&amp;rdquo; by 尼尔·基什特尼, 张缘, 刘婧)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;传统经济学认为，一个国家的收入等于经济产能，但凯恩斯认为收入等于人们的消费总量。消费等于为经济注入活力，而没有得到利用的储蓄意味着活力的流失。如果无法阻止人们把钱都存起来，并且无法有效利用人们的储蓄，那么经济衰退就会发生。&lt;/p&gt;
&lt;p&gt;也许中国的经济学家早就认可了这个理论，所以他们制定了一个由基础建设为核心的经济发展策略。中国的人民变得喜欢存钱，他们存起来的钱一部分被用来投资公共基础设施：公路、铁路、电力、水利、互联网等等，而且这些设施有利于中国全面的尤其是内陆地区的经济发展。此外，由全面发展带的的经济进步，促使人们拥有了更多的储蓄。他们的储蓄并不会永久增加，因为当下的社会风气迫使他们把几乎所有的储蓄都用在了买房、买车、养老、医疗、旅游以及后代教育上。人们挣得越来越多，而且中国的经济学家们总有办法让人们把手里的钱花出去，于是中国的经济才会持续不断地焕发新活力。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>拼多多或许造福了中国的贫下中农以及经济与科技</title>
      <link>https://shaojiejiang.github.io/post/zh/pinduoduo/</link>
      <pubDate>Mon, 27 Apr 2020 22:03:10 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/zh/pinduoduo/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;企业家获得成功的同时也得到了财富。他们的新商品在经济体中传播，人们发现自己想要一个留声机或电视，便出门购买。亨利·福特和安德鲁·卡内基分别靠着生产适用于大众的廉价汽车和在钢铁制造中引入新方法而发财致富。 很快，仿效者们开始仿效最初的企业家，生产出了同样的汽车、熔炉或染料。新商品和技术传播地更远了，这引起了整个行业的变革，并扩大了经济体量。最终，一些企业倒闭，经济开始萎缩，直至新一轮创新出现。资本主义的荣衰与浮沉都源自层出不穷的创新浪潮以及创业和模仿的消长。&amp;rdquo; (from &amp;ldquo;经济学通识课（耶鲁大学出品！耶鲁大学经济学入门课，普通人也能读懂的经济学！理论到现实，搭起用经济学改善现实生活的桥梁 ) (博集经管商务必读系列)&amp;rdquo; by 尼尔·基什特尼, 张缘, 刘婧)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;诚然，拼多多也许在技术上、营销上并没有为同行们带来什么创新，而且还破坏了很多工薪阶层对网购平台的印象，然而也许这些都只是负面作用。整体上拼多多使得方便的网购果实惠及到偏远的乡村家庭里，可以说最大化了当前网购产业的利用率，甚至还可能会带动乡村经济的发展。&lt;/p&gt;
&lt;p&gt;就像引文里提到的福特。也许他刚开始也因为生产廉价的汽车而为贵族所不齿，也许他的汽车也伤过一时贪图便宜的贵族的心，但不可否认的是，正是他的这种努力才使得汽车成为平民大众的交通工具而不是贵族用来炫耀的奢侈品。那么拼多多也是承担了这样一个角色。&lt;/p&gt;
&lt;p&gt;作为生在新时代的我们，亦或是生活在大城市、坐在舒适办公桌前的我们，可能无法理解在网购领域已经有淘宝天猫京东等平台覆盖了从廉价到品质各个价格区间，为什么还会有拼多多来进一步拉低品质与价格的下限。我们之所以会有这样的想法，是因为我们在与网购一起成长，我们接受了网购同时也成就了网购。习惯成自然的我们忽视了有一个群体一直都不在我们与网购形成的共生体中——那些生活在乡村的贫苦农民们。&lt;/p&gt;
&lt;p&gt;他们不能像我们一样很快适应新科技、新潮流的发展，更不能像我们一样在价格与品质之间随性选择。记得之前在一篇时事点评里（『每日人物』关于拼多多的点评《&lt;a href=&#34;https://mp.weixin.qq.com/s/kfj6cAIsajiyumnURNxpeQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;他们，在拼多多上拼运气&lt;/a&gt;》。惊讶得发现文章已经是2018年8月的了，我明明记得是去年看过的啊。。）看到过，我们眼里的假冒伪劣产品，可能是那些穷人眼里的一次消费升级。他们不会用淘宝天猫京东，也没有动力去学习如何使用——毕竟他们需求低，就算有需求，这些平台的商品也可能会超出他们的（心理）承受范围。若不是拼多多依靠低价、亲友帮忙砍价等等策略让这个群体接受这种新的消费方式，网络购物何时才能渗透到社会的神经末梢里去？拼多多是一个模仿者，也是一个推广者、一股深化技术对社会变革不可缺少的力量。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Hub for Transformer Blogs and Papers</title>
      <link>https://shaojiejiang.github.io/post/en/transformer-blog-paper-hub/</link>
      <pubDate>Mon, 02 Mar 2020 14:26:59 +0100</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/transformer-blog-paper-hub/</guid>
      <description>&lt;p&gt;This is a growing list of pointers to useful blog posts and papers related to transformers.&lt;/p&gt;
&lt;h2 id=&#34;transformers-explained&#34;&gt;Transformers explained&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: The Illustrated Transformer&lt;/a&gt; has many intuitive animations of how transformer models work&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mostafadehghani.com/2019/05/05/universal-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Universal Transformers&lt;/a&gt; introduces the idea of &lt;em&gt;recurrence among layers&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformer vs RNN and CNN for Translation Task&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gnns-similarities-and-differences&#34;&gt;GNNs: similarities and differences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://graphdeeplearning.github.io/post/transformers-are-gnns/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformers are Graph Neural Networks&lt;/a&gt; bridges transformer models and Graph Neural Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;transformer-improvements&#34;&gt;Transformer improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/deepmind-releases-a-new-architecture-and-a-new-dataset-to-improve-long-term-memory-in-deep-22f4b098153&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems&lt;/a&gt; Nural Turing Machine + transformer?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TLDR: Token Loss Dynamic Reweighting for Reducing Repetitive Utterance Generation</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2020-tldr/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2020-tldr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What&#39;s New in XLNet?</title>
      <link>https://shaojiejiang.github.io/post/en/xlnet/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/post/en/xlnet/</guid>
      <description>&lt;h2 id=&#34;rip-bert&#34;&gt;R.I.P BERT&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT&lt;/a&gt; got a head shot yesterday, by another guy called &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;XLNet&lt;/a&gt;.
It is reported that XLNet defeated BERT on 20 NLP tasks, and achieved 18 new state-of-the-art results.
Isn&amp;rsquo;t it impressive?
So, farewell, BERT.


















&lt;figure  id=&#34;figure-rip-bert&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;R.I.P BERT&#34; srcset=&#34;
               /post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_ad3b60d050a67b9089255b10065b08b1.webp 400w,
               /post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_910a11ae63908cf22e4e12ec92059faf.webp 760w,
               /post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_ad3b60d050a67b9089255b10065b08b1.webp&#34;
               width=&#34;570&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      R.I.P BERT
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;is-bert-really-dead&#34;&gt;Is BERT really dead?&lt;/h2&gt;
&lt;p&gt;Since I love BERT, I decided to read the paper to find out what killed him.
While reading, I was thinking wait a minute, is BERT really dead?
After finished the paper, I was so glad to know that BERT is still well alive!
He is just wearing another coat named &lt;em&gt;Two-Stream Self-Attention (TSSA)&lt;/em&gt;, with some other gadgets!
Because:&lt;br&gt;
&lt;code&gt;XLNet = BERT + TSSA + bidirectional data input&lt;/code&gt;&lt;br&gt;
Bert you&amp;rsquo;re so tough, buddy!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at what were trying to kill BERT.&lt;/p&gt;
&lt;h3 id=&#34;two-stream-self-attention-tssa&#34;&gt;Two-stream self-attention (TSSA)&lt;/h3&gt;
&lt;p&gt;Why TSSA is needed to kill BERT?
Well, let&amp;rsquo;s first see some weaknesses BERT has.&lt;/p&gt;
&lt;p&gt;BERT is using a masked language model (MLM) training objective, which is essentially why it achieves bidirectional representation.


















&lt;figure  id=&#34;figure-image-sourcehttpsnlpstanfordeduseminardetailsjdevlinpdf&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;[Image source](https://nlp.stanford.edu/seminar/details/jdevlin.pdf)&#34; srcset=&#34;
               /post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_bbb8a61fc9a0697db6e27484ef59e402.webp 400w,
               /post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_a370a21d4f992ecb82ae8e2649177c0d.webp 760w,
               /post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_bbb8a61fc9a0697db6e27484ef59e402.webp&#34;
               width=&#34;760&#34;
               height=&#34;103&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;a href=&#34;https://nlp.stanford.edu/seminar/details/jdevlin.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Image source&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this example, both words &amp;ldquo;store&amp;rdquo; and &amp;ldquo;gallon&amp;rdquo; are intended to be predicted by BERT, and their input word embeddings are replaced by the embedding of a special token &lt;em&gt;[MASK]&lt;/em&gt;.
Usually this isn&amp;rsquo;t a problem, but what if the prediction of &amp;ldquo;store&amp;rdquo; requires knowing the word &amp;ldquo;gallon&amp;rdquo;?
That is exactly where BERT falls short.&lt;/p&gt;
&lt;p&gt;TSSA is what you can use to overcome that downside of MLM:


















&lt;figure  id=&#34;figure-query-stream-sourcehttpsarxivorgabs190608237&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Query stream, [source](https://arxiv.org/abs/1906.08237)&#34; srcset=&#34;
               /post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_a84c83c6b945dba172c71d33c7936aac.webp 400w,
               /post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_0758dcc20abd12daa219dd5d9bddf6da.webp 760w,
               /post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_a84c83c6b945dba172c71d33c7936aac.webp&#34;
               width=&#34;760&#34;
               height=&#34;645&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Query stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this illustration, query stream gives you the &lt;code&gt;query&lt;/code&gt; vector needed for attention calculation, and this stream is designed in such a way that it doesn&amp;rsquo;t leak the info of the word it&amp;rsquo;s going to predict, but guarantees all information from other positions.
Take $x_1$ for example: $x_1$&amp;rsquo;s embedding (and hidden state) is not used at all, but embeddings and hidden states from other positions are used in each layer.&lt;/p&gt;


















&lt;figure  id=&#34;figure-content-stream-sourcehttpsarxivorgabs190608237&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Content stream, [source](https://arxiv.org/abs/1906.08237)&#34; srcset=&#34;
               /post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_f91903c52b2690d818283e5376124698.webp 400w,
               /post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_77f32608974273a37acf00c5c6de89e2.webp 760w,
               /post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://shaojiejiang.github.io/post/en/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_f91903c52b2690d818283e5376124698.webp&#34;
               width=&#34;760&#34;
               height=&#34;561&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Content stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;source&lt;/a&gt;
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Content stream, on the other hand, gives you the &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; vectors needed for context vector calculation.
This stream uses a strategy similar to that in a standard &lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer decoder&lt;/a&gt; by masking future positions.
The only difference is that in content stream, the order of tokens is &lt;em&gt;randomly permuted&lt;/em&gt;.
For example $x_2$ is right after $x_3$, and therefore $h_2^{(1)}$ can only see the embedding of itself and that of $x_3$ (and $mem^{(0)}$), but not that of $x_1$ or $x_4$.&lt;/p&gt;
&lt;h3 id=&#34;mask-a-span&#34;&gt;Mask a span&lt;/h3&gt;
&lt;p&gt;Another difference from BERT is masking a span of consecutive words.
The reason I guess, is that this guarantees the dependence of masked words (as claimed to be what BERT can&amp;rsquo;t model).
This is not a fresh-new idea, though.
Recently there are two ERNIE papers (BERT based) that propose masking named entities (often of multiple words, &lt;a href=&#34;https://arxiv.org/pdf/1905.07129.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;) and/or phrases (&lt;a href=&#34;https://arxiv.org/pdf/1904.09223.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-data-input&#34;&gt;Bidirectional data input&lt;/h3&gt;
&lt;p&gt;Another notably different thing in XLNet is the usage of bidirectional data input.
The idea (I guess) is to decide the factorization direction (either forward or backward), so that the idea of &amp;ldquo;masking future positions&amp;rdquo; used in a standard Transformer decoder can also be easily used together with XLNet.&lt;/p&gt;
&lt;p&gt;Masking a span makes XLNet look like a denoising autoencoder; but by using bidirectional data input (or masking future positions), XLNet performs more like a autoregressive language model in the masked region.&lt;/p&gt;
&lt;h2 id=&#34;closing-remarks&#34;&gt;Closing remarks&lt;/h2&gt;
&lt;p&gt;So now you probably can see the similarities and differences between XLNet and BERT.
If not, here is a quick summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of masking random words, mask a span of words&lt;/li&gt;
&lt;li&gt;Use bidirectional data input to decide which direction you treat as &amp;ldquo;future&amp;rdquo;, and then apply the idea of masking future positions&lt;/li&gt;
&lt;li&gt;To avoid leaking the information of the position to be predicted, use Two-Stream Self-Attention (TSSA)&lt;/li&gt;
&lt;li&gt;Other minor things like segment recurrence, relative positional encoding, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it doesn&amp;rsquo;t seem to be enough changes to make all those improvements.
What if BERT is also trained using the additional data (Giga5, ClueWeb, Common Crawl), will XLNet still be able to defeat BERT?&lt;/p&gt;
&lt;p&gt;EDIT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another model named &lt;a href=&#34;https://arxiv.org/abs/1905.02450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MASS&lt;/a&gt; employs a very similar idea.&lt;/li&gt;
&lt;li&gt;According to Jacob Devlin (author of BERT), relative positional embedding might be of great importance.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://shaojiejiang.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://revealjs.com/pdf-export/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;porridge&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;blueberry&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Eating...&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} One {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} **Two** {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% fragment %}} Three {{% /fragment %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{% speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Only the speaker can read these notes
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;-&lt;/span&gt; Press &lt;span class=&#34;sb&#34;&gt;`S`&lt;/span&gt; key to view
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  {{% /speaker_note %}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-image&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;/media/boards.jpg&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;background-color&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;#0000FF&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{{&lt;span class=&#34;p&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;slide&lt;/span&gt; &lt;span class=&#34;na&#34;&gt;class&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;my-style&amp;#34;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;gt;&lt;/span&gt;}}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;reveal&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;section&lt;/span&gt; &lt;span class=&#34;nt&#34;&gt;h3&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;k&#34;&gt;color&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;navy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://discord.gg/z8wNYzb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/content/slides/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving Neural Response Diversity with Frequency-Aware Cross-Entropy Loss</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2019-improving/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2019-improving/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2018-sequence/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2018-sequence/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Struck tracker via color Haar-like feature and selective updating</title>
      <link>https://shaojiejiang.github.io/publication/jiang-2017-robust/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/jiang-2017-robust/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Project</title>
      <link>https://shaojiejiang.github.io/project/example/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/project/example/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://shaojiejiang.github.io/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Object tracking via dual linear structured SVM and explicit feature map</title>
      <link>https://shaojiejiang.github.io/publication/ning-2016-object/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/publication/ning-2016-object/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://shaojiejiang.github.io/home-zh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/home-zh/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Google Search this site 全站谷歌搜索</title>
      <link>https://shaojiejiang.github.io/search/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/search/</guid>
      <description>&lt;script async src=&#34;https://cse.google.com/cse.js?cx=018409216753371592144:rrbl3bjsbqc&#34;&gt;&lt;/script&gt;
&lt;div class=&#34;gcse-search&#34;&gt;&lt;/div&gt;
&lt;p&gt;Tips: Please change to Light theme 提示：请换成明亮主题&lt;/p&gt;
&lt;p&gt;Note: If the integrated search function fails to return content, try Google search.
说明：如果集成搜索功能失效，请使用谷歌搜索。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Shaojie Jiang</title>
    <link>https://shaojiejiang.github.io/post/</link>
      <atom:link href="https://shaojiejiang.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Shaojie Jiang</copyright>
    <image>
      <url>https://shaojiejiang.github.io/images/icon_hua235b20c26de23a006d4e436991121ab_1829_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://shaojiejiang.github.io/post/</link>
    </image>
    
    <item>
      <title>Adaptive Computation Time</title>
      <link>https://shaojiejiang.github.io/post/adaptive-computation-time/</link>
      <pubDate>Tue, 28 Apr 2020 10:46:44 +0200</pubDate>
      <guid>https://shaojiejiang.github.io/post/adaptive-computation-time/</guid>
      <description>&lt;p&gt;My notes for the paper: Adaptive Computation Time for Recurrent Neural Networks&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;additive-vs-multiplicative-halting-probability&#34;&gt;Additive vs multiplicative halting probability&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Multiplicative:&lt;/strong&gt; In the paper (footnote 1), the authors discuss throughly their considerations for deciding the computation time.
It is acknowledged by the authors that using the logits $h_n^t$ as the halting probability at step $n$ might be more straightforward.
Therefore, the overall halting probability is calculated as $$p_t^n = h_t^n \prod_{u=1}^{n-1} (1 - h_t^u)$$.
We use $(1 - h_t^u)$ for previous update steps to indicate that the updating is *not* stopped until $n$.&lt;/p&gt;
&lt;p&gt;As each $p_t^n \in (0, 1)$ is relatively independent with each other and $\sum p_t^n$ is not bound to 1, this approach &lt;em&gt;does not&lt;/em&gt; restrict the update depth to grow arbitrarily.
The model can be of course trained to lower the expected ponder time $\rho_t = \sum n p_t^n$, but it is observed in the experiments that the resulting model is not preferable in two ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$h_t^1$ is usually just below threshold, intermediate $h_t^n = 0$, and final $h_t^N$ is high enough to halt the update.&lt;/li&gt;
&lt;li&gt;as the expectation is low, $p_t^N \ll p_t^1$, but the network learns to have a much higher magnitude of output states at step $N$, so that the final output is still dominated by the final state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Additive:&lt;/strong&gt; In contrast, the additive approach have an constraint of $\sum p_t^n = 1$, so that the probability is decreased monotonically with the number of updates growing larger.
Though being non-differentiable, the total ponder time (total updates at all positions) is penalized to avoid consuming unnecessary computation.
There is still one drawback of this approach, however.
The performance is sensitive to the penalty factor $\tau$, which is not intuitive to choose as a hyperparameter.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1603.08983&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adaptive Computation Time for Recurrent Neural Networks&lt;/a&gt; &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>A Hub for Transformer Blogs and Papers</title>
      <link>https://shaojiejiang.github.io/post/transformer-blog-paper-hub/</link>
      <pubDate>Mon, 02 Mar 2020 14:26:59 +0100</pubDate>
      <guid>https://shaojiejiang.github.io/post/transformer-blog-paper-hub/</guid>
      <description>&lt;p&gt;This is a growing list of pointers to useful blog posts and papers related to transformers.&lt;/p&gt;
&lt;h2 id=&#34;transformers-explained&#34;&gt;Transformers explained&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: The Illustrated Transformer&lt;/a&gt; has many intuitive animations of how transformer models work&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://mostafadehghani.com/2019/05/05/universal-transformers/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Universal Transformers&lt;/a&gt; introduces the idea of &lt;em&gt;recurrence among layers&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://medium.com/analytics-vidhya/transformer-vs-rnn-and-cnn-18eeefa3602b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformer vs RNN and CNN for Translation Task&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gnns-similarities-and-differences&#34;&gt;GNNs: similarities and differences&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://graphdeeplearning.github.io/post/transformers-are-gnns/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: Transformers are Graph Neural Networks&lt;/a&gt; bridges transformer models and Graph Neural Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;transformer-improvements&#34;&gt;Transformer improvements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://towardsdatascience.com/deepmind-releases-a-new-architecture-and-a-new-dataset-to-improve-long-term-memory-in-deep-22f4b098153&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Blog: DeepMind Releases a New Architecture and a New Dataset to Improve Long-Term Memory in Deep Learning Systems&lt;/a&gt; Nural Turing Machine + transformer?&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What&#39;s New in XLNet?</title>
      <link>https://shaojiejiang.github.io/post/xlnet/</link>
      <pubDate>Thu, 20 Jun 2019 00:00:00 +0000</pubDate>
      <guid>https://shaojiejiang.github.io/post/xlnet/</guid>
      <description>&lt;h2 id=&#34;rip-bert&#34;&gt;R.I.P BERT&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT&lt;/a&gt; got a head shot yesterday, by another guy called 
&lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;XLNet&lt;/a&gt;.
It is reported that XLNet defeated BERT on 20 NLP tasks, and achieved 18 new state-of-the-art results.
Isn&amp;rsquo;t it impressive?
So, farewell, BERT.





  
  











&lt;figure id=&#34;figure-rip-bert&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_2000x2000_fit_q90_lanczos.jpg&#34; data-caption=&#34;R.I.P BERT&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/xlnet/images/bert_dead_hu61e83ca8534a90d5b1ebee93953bac39_29320_2000x2000_fit_q90_lanczos.jpg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;570&#34; height=&#34;570&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    R.I.P BERT
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;is-bert-really-dead&#34;&gt;Is BERT really dead?&lt;/h2&gt;
&lt;p&gt;Since I love BERT, I decided to read the paper to find out what killed him.
While reading, I was thinking wait a minute, is BERT really dead?
After finished the paper, I was so glad to know that BERT is still well alive!
He is just wearing another coat named &lt;em&gt;Two-Stream Self-Attention (TSSA)&lt;/em&gt;, with some other gadgets!
Because:&lt;br&gt;
&lt;code&gt;XLNet = BERT + TSSA + bidirectional data input&lt;/code&gt;&lt;br&gt;
Bert you&amp;rsquo;re so tough, buddy!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a closer look at what were trying to kill BERT.&lt;/p&gt;
&lt;h3 id=&#34;two-stream-self-attention-tssa&#34;&gt;Two-stream self-attention (TSSA)&lt;/h3&gt;
&lt;p&gt;Why TSSA is needed to kill BERT?
Well, let&amp;rsquo;s first see some weaknesses BERT has.&lt;/p&gt;
&lt;p&gt;BERT is using a masked language model (MLM) training objective, which is essentially why it achieves bidirectional representation.





  
  











&lt;figure id=&#34;figure-image-sourcehttpsnlpstanfordeduseminardetailsjdevlinpdf&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Image source&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/xlnet/images/MLM_hub4c01273cdd2a52becfd097515ece19b_34267_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1138&#34; height=&#34;154&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;a href=&#34;https://nlp.stanford.edu/seminar/details/jdevlin.pdf&#34;&gt;Image source&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this example, both words &amp;ldquo;store&amp;rdquo; and &amp;ldquo;gallon&amp;rdquo; are intended to be predicted by BERT, and their input word embeddings are replaced by the embedding of a special token &lt;em&gt;[MASK]&lt;/em&gt;.
Usually this isn&amp;rsquo;t a problem, but what if the prediction of &amp;ldquo;store&amp;rdquo; requires knowing the word &amp;ldquo;gallon&amp;rdquo;?
That is exactly where BERT falls short.&lt;/p&gt;
&lt;p&gt;TSSA is what you can use to overcome that downside of MLM:





  
  











&lt;figure id=&#34;figure-query-stream-sourcehttpsarxivorgabs190608237&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Query stream, source&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/xlnet/images/query_stream_hued03336a8aeea8af3524f5a71c4c5e85_138678_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;936&#34; height=&#34;794&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Query stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;source&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;In this illustration, query stream gives you the &lt;code&gt;query&lt;/code&gt; vector needed for attention calculation, and this stream is designed in such a way that it doesn&amp;rsquo;t leak the info of the word it&amp;rsquo;s going to predict, but guarantees all information from other positions.
Take $x_1$ for example: $x_1$&#39;s embedding (and hidden state) is not used at all, but embeddings and hidden states from other positions are used in each layer.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-content-stream-sourcehttpsarxivorgabs190608237&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shaojiejiang.github.io/post/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_2000x2000_fit_lanczos_2.png&#34; data-caption=&#34;Content stream, source&#34;&gt;


  &lt;img data-src=&#34;https://shaojiejiang.github.io/post/xlnet/images/content_stream_hude0ab6174270e4e71fb20a58d5784b5d_120246_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;894&#34; height=&#34;660&#34;&gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Content stream, &lt;a href=&#34;https://arxiv.org/abs/1906.08237&#34;&gt;source&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Content stream, on the other hand, gives you the &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; vectors needed for context vector calculation.
This stream uses a strategy similar to that in a standard 
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer decoder&lt;/a&gt; by masking future positions.
The only difference is that in content stream, the order of tokens is &lt;em&gt;randomly permuted&lt;/em&gt;.
For example $x_2$ is right after $x_3$, and therefore $h_2^{(1)}$ can only see the embedding of itself and that of $x_3$ (and $mem^{(0)}$), but not that of $x_1$ or $x_4$.&lt;/p&gt;
&lt;h3 id=&#34;mask-a-span&#34;&gt;Mask a span&lt;/h3&gt;
&lt;p&gt;Another difference from BERT is masking a span of consecutive words.
The reason I guess, is that this guarantees the dependence of masked words (as claimed to be what BERT can&amp;rsquo;t model).
This is not a fresh-new idea, though.
Recently there are two ERNIE papers (BERT based) that propose masking named entities (often of multiple words, 
&lt;a href=&#34;https://arxiv.org/pdf/1905.07129.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;) and/or phrases (
&lt;a href=&#34;https://arxiv.org/pdf/1904.09223.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper link&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;bidirectional-data-input&#34;&gt;Bidirectional data input&lt;/h3&gt;
&lt;p&gt;Another notably different thing in XLNet is the usage of bidirectional data input.
The idea (I guess) is to decide the factorization direction (either forward or backward), so that the idea of &amp;ldquo;masking future positions&amp;rdquo; used in a standard Transformer decoder can also be easily used together with XLNet.&lt;/p&gt;
&lt;p&gt;Masking a span makes XLNet look like a denoising autoencoder; but by using bidirectional data input (or masking future positions), XLNet performs more like a autoregressive language model in the masked region.&lt;/p&gt;
&lt;h2 id=&#34;closing-remarks&#34;&gt;Closing remarks&lt;/h2&gt;
&lt;p&gt;So now you probably can see the similarities and differences between XLNet and BERT.
If not, here is a quick summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of masking random words, mask a span of words&lt;/li&gt;
&lt;li&gt;Use bidirectional data input to decide which direction you treat as &amp;ldquo;future&amp;rdquo;, and then apply the idea of masking future positions&lt;/li&gt;
&lt;li&gt;To avoid leaking the information of the position to be predicted, use Two-Stream Self-Attention (TSSA)&lt;/li&gt;
&lt;li&gt;Other minor things like segment recurrence, relative positional encoding, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, it doesn&amp;rsquo;t seem to be enough changes to make all those improvements.
What if BERT is also trained using the additional data (Giga5, ClueWeb, Common Crawl), will XLNet still be able to defeat BERT?&lt;/p&gt;
&lt;p&gt;EDIT:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Another model named 
&lt;a href=&#34;https://arxiv.org/abs/1905.02450&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MASS&lt;/a&gt; employs a very similar idea.&lt;/li&gt;
&lt;li&gt;According to Jacob Devlin (author of BERT), relative positional embedding might be of great importance.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
